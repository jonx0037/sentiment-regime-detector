DOI,Title,Authors,Journal,Year,Abstract,ResearchRabbitId,Cited By,References,PubMedId
(missing DOI),Language Models are Few-Shot Learners,"Tom Brown, Ben Mann, N. C. Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Asha Neelakantan, P Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Thomas Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",Neural Information Processing Systems,2020,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",1488ac47-c196-4d1d-970f-f33b9841429e,61997,224,(missing PubMedId)
(missing DOI),FinBERT: Financial Sentiment Analysis with Pre-trained Language Models,Doƒüu Tan Aracƒ±,arXiv.org,2019,"Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods.",b361f051-554c-4ece-b5ef-497b9dc3c823,865,56,(missing PubMedId)
(missing DOI),Language Models are Few-Shot Learners,"T. B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric J. Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",Neural Information Processing Systems,2020,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",fa349244-6280-4b0c-8d32-2b9852665a69,62125,224,(missing PubMedId)
(missing DOI),FinBERT: Financial Sentiment Analysis with Pre-trained Language Models,"Dogu Araci,  Zulkuf Genc",arXiv: Computation and Language,2019,"Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods.",607e6c4d-368c-4f09-a4ec-2cea75153d0d,866,56,(missing PubMedId)
(missing DOI),LoRA: Low-Rank Adaptation of Large Language Models,"Junfeng Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen",International Conference on Learning Representations,2021,"An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",e9d210ff-2532-4adf-9759-98912b79c488,18656,106,(missing PubMedId)
10.1109/ACCESS.2020.3009626,Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers,"Kostadin Mishev, Ana Gjorgjevikj, Irena Vodenska, Lou Chitkushev, Dimitar Trajanov",IEEE Access,2020,"Financial and economic news is continuously monitored by financial market participants. According to the efficient market hypothesis, all past information is reflected in stock prices and new information is instantaneously absorbed in determining future stock prices. Hence, prompt extraction of positive or negative sentiments from news is very important for investment decision-making by traders, portfolio managers and investors. Sentiment analysis models can provide an efficient method for extracting actionable signals from the news. However, financial sentiment analysis is challenging due to domain-specific language and unavailability of large labeled datasets. General sentiment analysis models are ineffective when applied to specific domains such as finance. To overcome these challenges, we design an evaluation platform which we use to assess the effectiveness and performance of various sentiment analysis approaches, based on combinations of text representation methods and machine-learning classifiers. We perform more than one hundred experiments using publicly available datasets, labeled by financial experts. We start the evaluation with specific lexicons for sentiment analysis in finance and gradually build the study to include word and sentence encoders, up to the latest available NLP transformers. The results show improved efficiency of contextual embeddings in sentiment analysis compared to lexicons and fixed word and sentence encoders, even when large datasets are not available. Furthermore, distilled versions of NLP transformers produce comparable results to their larger teacher models, which makes them suitable for use in production environments.",310c9b67-c587-4293-a3a0-a5d3a22e5fa0,270,138,(missing PubMedId)
10.48550/ARXIV.2312.08725,A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis,"Sorouralsadat Fatemi, Yuheng Hu",arXiv.org,2023,"Financial sentiment analysis plays a crucial role in uncovering latent patterns and detecting emerging trends, enabling individuals to make well-informed decisions that may yield substantial advantages within the constantly changing realm of finance. Recently, Large Language Models (LLMs) have demonstrated their effectiveness in diverse domains, showcasing remarkable capabilities even in zero-shot and few-shot in-context learning for various Natural Language Processing (NLP) tasks. Nevertheless, their potential and applicability in the context of financial sentiment analysis have not been thoroughly explored yet. To bridge this gap, we employ two approaches: in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs on a finance-domain dataset. Given the computational costs associated with fine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs, spanning from 250M to 3B parameters for fine-tuning. We then compare the performances with state-of-the-art results to evaluate their effectiveness in the finance-domain. Our results demonstrate that fine-tuned smaller LLMs can achieve comparable performance to state-of-the-art fine-tuned LLMs, even with models having fewer parameters and a smaller training dataset. Additionally, the zero-shot and one-shot performance of LLMs produces comparable results with fine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our analysis demonstrates that there is no observed enhancement in performance for finance-domain sentiment analysis when the number of shots for in-context learning is increased.",3fbe0944-cd8b-41c3-bf8e-1ab205df76e8,18,22,(missing PubMedId)
10.1109/ACCESS.2024.3445413,"Large Language Models and Sentiment Analysis in Financial Markets: A Review, Datasets and Case Study","Chenghao Liu, Arunkumar Arulappan, Ranesh Kumar Naha, Aniket Mahanti, Joarder Kamruzzaman, In-Ho Ra",IEEE Access,2023,(missing abstract),5aefa634-1293-4c60-adcf-50a8bf711807,15,0,(missing PubMedId)
10.1016/J.NLP.2025.100148,Financial sentiment analysis for pre-trained language models incorporating dictionary knowledge and neutral features,"Yongyong Sun,  He Yuan,  Fei Xu",Natural Language Processing Journal,2025,(missing abstract),d9fd77a6-d411-4084-a34b-19155c7acd25,1,35,(missing PubMedId)
10.48550/ARXIV.2310.12406,FinEntity: Entity-level Sentiment Classification for Financial Texts,"Yixuan Tang, Yi Yang, Allen Huang, Andy Tam, Justin Z Tang",arXiv.org,2023,"In the financial domain, conducting entity-level sentiment analysis is crucial for accurately assessing the sentiment directed toward a specific financial entity. To our knowledge, no publicly available dataset currently exists for this purpose. In this work, we introduce an entity-level sentiment classification dataset, called \textbf{FinEntity}, that annotates financial entity spans and their sentiment (positive, neutral, and negative) in financial news. We document the dataset construction process in the paper. Additionally, we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on entity-level sentiment classification. In a case study, we demonstrate the practical utility of using FinEntity in monitoring cryptocurrency markets. The data and code of FinEntity is available at \url{https://github.com/yixuantt/FinEntity}",2d120b7f-c5e8-49a4-b3b1-948961e15a15,19,32,(missing PubMedId)
10.3390/BDCC8080087,FinSoSent: Advancing Financial Market Sentiment Analysis through Pretrained Large Language Models,"Josiel Delgadillo, Johnson Kinyua, Charles Mutigwe",Big Data and Cognitive Computing,2024,"Predicting the directions of financial markets has been performed using a variety of approaches, and the large volume of unstructured data generated by traders and other stakeholders on social media microblog platforms provides unique opportunities for analyzing financial markets using additional perspectives. Pretrained large language models (LLMs) have demonstrated very good performance on a variety of sentiment analysis tasks in different domains. However, it is known that sentiment analysis is a very domain-dependent NLP task that requires knowledge of the domain ontology, and this is particularly the case with the financial domain, which uses its own unique vocabulary. Recent developments in NLP and deep learning including LLMs have made it possible to generate actionable financial sentiments using multiple sources including financial news, company fundamentals, technical indicators, as well social media microblogs posted on platforms such as StockTwits and X (formerly Twitter). We developed a financial social media sentiment analyzer (FinSoSent), which is a domain-specific large language model for the financial domain that was pretrained on financial news articles and fine-tuned and tested using several financial social media corpora. We conducted a large number of experiments using different learning rates, epochs, and batch sizes to yield the best performing model. Our model outperforms current state-of-the-art FSA models based on over 860 experiments, demonstrating the efficacy and effectiveness of FinSoSent. We also conducted experiments using ensemble models comprising FinSoSent and the other current state-of-the-art FSA models used in this research, and a slight performance improvement was obtained based on majority voting. Based on the results obtained across all models in these experiments, the significance of this study is that it highlights the fact that, despite the recent advances of LLMs, sentiment analysis even in domain-specific contexts remains a difficult research problem.",bbab7d0c-73bd-484e-ac70-06e589aff8d8,17,0,(missing PubMedId)
10.48550/ARXIV.2403.12285,FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications,"Thanos D. Konstantinidis, Giorgos Iacovides, Mingxue Xu, Tony G. Constantinides, Danilo P. Mandic",arXiv.org,2024,"There are multiple sources of financial news online which influence market movements and trader's decisions. This highlights the need for accurate sentiment analysis, in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions. Standard lexicon based sentiment approaches have demonstrated their power in aiding financial decisions. However, they are known to suffer from issues related to context sensitivity and word ordering. Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources. To facilitate a finance specific LLM framework, we introduce a novel approach based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. This is achieved by fine-tuning the Llama2 7B model on a small portion of supervised financial sentiment analysis data, so as to jointly handle the complexities of financial lexicon and context, and further equipping it with a neural network based decision mechanism. Such a generator-classifier scheme, referred to as FinLlama, is trained not only to classify the sentiment valence but also quantify its strength, thus offering traders a nuanced insight into financial news articles. Complementing this, the implementation of parameter-efficient fine-tuning through LoRA optimises trainable parameters, thus minimising computational and memory requirements, without sacrificing accuracy. Simulation results demonstrate the ability of the proposed FinLlama to provide a framework for enhanced portfolio management decisions and increased market returns. These results underpin the ability of FinLlama to construct high-return portfolios which exhibit enhanced resilience, even during volatile periods and unpredictable market events.",9768b516-f23f-4aef-97d1-6756ac86292f,28,26,(missing PubMedId)
10.3390/BDCC8060063,LLMs and NLP Models in Cryptocurrency Sentiment Analysis: A Comparative Classification Study,"Konstantinos I. Roumeliotis, Nikolaos D. Tselikas, Dimitrios Œö. Nasiopoulos",Big Data and Cognitive Computing,2024,"Cryptocurrencies are becoming increasingly prominent in financial investments, with more investors diversifying their portfolios and individuals drawn to their ease of use and decentralized financial opportunities. However, this accessibility also brings significant risks and rewards, often influenced by news and the sentiments of crypto investors, known as crypto signals. This paper explores the capabilities of large language models (LLMs) and natural language processing (NLP) models in analyzing sentiment from cryptocurrency-related news articles. We fine-tune state-of-the-art models such as GPT-4, BERT, and FinBERT for this specific task, evaluating their performance and comparing their effectiveness in sentiment classification. By leveraging these advanced techniques, we aim to enhance the understanding of sentiment dynamics in the cryptocurrency market, providing insights that can inform investment decisions and risk management strategies. The outcomes of this comparative study contribute to the broader discourse on applying advanced NLP models to cryptocurrency sentiment analysis, with implications for both academic research and practical applications in financial markets.",f618651b-8f5e-4df0-9e03-e679626bc882,30,58,(missing PubMedId)
10.1145/3604237.3626843,FinBERT-FOMC: Fine-Tuned FinBERT Model with Sentiment Focus Method for Enhancing Sentiment Analysis of FOMC Minutes,"Sandro G√∂ssi, Ziwei Chen, Wonseong Kim, Bernhard Bermeitinger, Siegfried Handschuh",International Conference on AI in Finance,2023,"In this research project, we used the financial texts published by the Federal Open Market Committee (FOMC), known as the FOMC Minutes, for sentiment analysis. The pre-trained FinBERT model, a state-of-the-art transformer-based model trained for NLP tasks in finance, was utilized for that. The focus of this research has been on improving the predictive performance of complex financial sentences, as our problem analysis has shown that such sentences pose a significant challenge to existing models. To accomplish this objective the original FinBERT model was fine-tuned for domain-specific sentiment analysis. A strategy, referred to as Sentiment Focus (SF) was utilized to reduce the complexity of sentences, making them more amenable to accurate sentiment predictions. To evaluate the efficacy of our method, we curated a manually labeled test dataset comprising 1375 entries. The results demonstrated an overall improvement of in accuracy when using SF-enhanced fine-tuned FinBERT over the original FinBERT model. In cases of complex sentences containing conjunctions like but, while, and though with contradicting sentiments, our fine-tuned model outperformed the original FinBERT by a margin of .",1e78da0e-1df7-41b4-af96-dae0fc829187,31,12,(missing PubMedId)
10.3390/BDCC8110143,"Innovative Sentiment Analysis and Prediction of Stock Price Using FinBERT, GPT-4 and Logistic Regression: A Data-Driven Approach","Olamilekan Shobayo, Sidikat Adeyemi-longe, Olusogo Popoola, Bayode Ogunleye",Big Data and Cognitive Computing,2024,"This study explores the comparative performance of cutting-edge AI models, i.e., Finaance Bidirectional Encoder representations from Transsformers (FinBERT), Generatice Pre-trained Transformer GPT-4, and Logistic Regression, for sentiment analysis and stock index prediction using financial news and the NGX All-Share Index data label. By leveraging advanced natural language processing models like GPT-4 and FinBERT, alongside a traditional machine learning model, Logistic Regression, we aim to classify market sentiment, generate sentiment scores, and predict market price movements. This research highlights global AI advancements in stock markets, showcasing how state-of-the-art language models can contribute to understanding complex financial data. The models were assessed using metrics such as accuracy, precision, recall, F1 score, and ROC AUC. Results indicate that Logistic Regression outperformed the more computationally intensive FinBERT and predefined approach of versatile GPT-4, with an accuracy of 81.83% and a ROC AUC of 89.76%. The GPT-4 predefined approach exhibited a lower accuracy of 54.19% but demonstrated strong potential in handling complex data. FinBERT, while offering more sophisticated analysis, was resource-demanding and yielded a moderate performance. Hyperparameter optimization using Optuna and cross-validation techniques ensured the robustness of the models. This study highlights the strengths and limitations of the practical applications of AI approaches in stock market prediction and presents Logistic Regression as the most efficient model for this task, with FinBERT and GPT-4 representing emerging tools with potential for future exploration and innovation in AI-driven financial analytics.",e85cd072-a59e-4ce3-bd55-127b068ec65e,27,20,(missing PubMedId)
10.3390/AXIOMS12090835,Forecasting the S&P 500 Index Using Mathematical-Based Sentiment Analysis and Deep Learning Models: A FinBERT Transformer Model and LSTM,"Jihwan Kim, Hui-Sang Kim, Sun‚ÄêYong Choi",Axioms,2023,"Stock price prediction has been a subject of significant interest in the financial mathematics field. Recently, interest in natural language processing models has increased, and among them, transformer models, such as BERT and FinBERT, are attracting attention. This study uses a mathematical framework to investigate the effects of human sentiment on stock movements, especially in text data. In particular, FinBERT, a domain-specific language model based on BERT tailored for financial language, was employed for the sentiment analysis on the financial texts to extract sentiment information. In this study, we use ‚Äúsummary‚Äù text data extracted from The New York Times, representing concise summaries of news articles. Accordingly, we apply FinBERT to the summary text data to calculate sentiment scores. In addition, we employ the LSTM (Long short-term memory) methodology, one of the machine learning models, for stock price prediction using sentiment scores. Furthermore, the LSTM model was trained by stock price data and the estimated sentiment scores. We compared the predictive power of LSTM models with and without sentiment analysis based on error measures such as MSE, RMSE, and MAE. The empirical results demonstrated that including sentiment scores through the LSTM model led to improved prediction accuracy for all three measures. These findings indicate the significance of incorporating news sentiment into stock price predictions, shedding light on the potential impact of psychological factors on financial markets. By using the FinBERT transformer model, this study aimed to investigate the interplay between sentiment and stock price predictions, contributing to a deeper understanding of mathematical-based sentiment analysis in finance and its role in enhancing forecasting in financial mathematics. Furthermore, we show that using summary data instead of entire news articles is a useful strategy for mathematical-based sentiment analysis.",522e7ca0-f058-4403-bb33-e087d02b3301,46,74,(missing PubMedId)
10.1109/ICPICS62053.2024.10796670,Financial Sentiment Analysis on News and Reports Using Large Language Models and FinBERT,"Yaoxin Shen, PeiWen Zhang","2022 IEEE 4th International Conference on Power, Intelligent Computing and Systems (ICPICS)",2024,(missing abstract),4da153e7-c99b-4ea1-9a98-80299f369b5c,7,0,(missing PubMedId)
10.1109/IDCIOT64235.2025.10915080,Advanced Financial Sentiment Analysis Using FinBERT to Explore Sentiment Dynamics,"S Baghavathi Priya,  Manish Kumar,  Nitheesh Prakash J D,  N. Krithika",2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),2025,"This research investigates the incorporation of sophisticated Natural Language Processing (NLP) methods in the intricate field of finance. In particular, it utilizes FinBERT, a tailored language model created to understand the complex subtleties of financial text data, in conjunction with assessments of DistilBERT and Bidirectional Encoder Representations from Transformers (BERT). The main goal is to create a strong preprocessing framework that improves the interpretative ability of models for sentiment classification designed for financial uses while providing an in-depth evaluation of the comparative performance of the models. Through extensive empirical validation, our approach demonstrates that FinBERT outperforms the other models in accurately capturing subtle sentiment variations and financial terminology. The comparative study underlines the compromises between computational efficiency and precision, with DistilBERT providing a streamlined option. This research emphasizes FinBERT's transformative capabilities in analyzing financial sentiment and simultaneously delivering essential insights into the comparative strengths and weaknesses of other approaches, presenting a comprehensive viewpoint to inform decision-making within the financial sector. Upon comprehensive analysis of the methods used in this paper, it is shown that FinBERT is best at capturing hidden features within the data and able to work well with unseen data also with an average accuracy of 89.6% over all the 3 classes that are present in the dataset. The suggested framework has the potential to greatly improve predictive analytics and risk management approaches, allowing for more informed, precise, and prompt financial decisions in intricate market conditions.",c334548a-e7bf-4292-94e2-7daa86dc3f3a,3,24,(missing PubMedId)
10.2139/SSRN.4028072,FinEAS: Financial Embedding Analysis of Sentiment,"Asier Guti√©rrez-Fandi√±o, Petter N. Kolm, Miquel Noguer i Alonso, Jordi Armengol-Estap√©",SSRN Electronic Journal,2021,"In this article, the authors introduce a new language representation model for sentiment analysis of financial text called financial embedding analysis of sentiment (FinEAS). The new approach is based on transformer language models that are explicitly developed for sentence-level analysis. By building upon Sentence-BERT, a sentence-level extension of vanilla BERT, the authors argue that the new approach produces sentence embeddings of higher quality that significantly improve sentence/document-level tasks such as financial sentiment analysis. Using a large-scale financial news dataset from RavenPack, they demonstrate that for financial sentiment analysis the new model outperforms several state-of-the-art models such as BERT, a bidirectional LSTM, and FinBERT, a financial-domain-specific BERT. The authors make the model code publicly available.",e12ce433-ea06-48ba-bf65-2659c99d27ae,12,25,(missing PubMedId)
10.1002/ISAF.70015,FinSentiment: Predicting Financial Sentiment Through Transfer Learning,"Zehra Erva Ergun,  Emre Sefer",Intelligent Systems in Accounting Finance & Management,2025,"ABSTRACT There is an increasing interest in financial text mining tasks. Significant progress has been made by using deep learning‚Äêbased models on a generic corpus, which also shows reasonable results on financial text mining tasks such as financial sentiment analysis. However, financial sentiment analysis is still demanding work because of the insufficiency of labeled data for the financial domain and its specialized language. General‚Äêpurpose deep learning methods are not as effective mainly due to specialized language used in the financial context. In this study, we focus on enhancing the performance of financial text mining tasks by improving the existing pretrained language models via NLP transfer learning. Pretrained language models demand a small quantity of labeled samples, and they could be enhanced to a greater extent by training them on domain‚Äêspecific corpora instead. We propose an enhanced model FinSentiment, which incorporates enhanced versions of a number of recently proposed pretrained models, such as BERT, XLNet, RoBERTa, GPT, Llama, and T5, to better perform across NLP tasks in financial domain by training these models on financial domain corpora. The corresponding finance‚Äêspecific models in FinSentiment are called Fin‚ÄêBERT, Fin‚ÄêXLNet, Fin‚ÄêRoBERTa, Fin‚ÄêGPT, Fin‚ÄêLlama, and Fin‚ÄêT5, respectively. We also propose variants of these models jointly trained over financial domain and general corpora. Our finance‚Äêspecific FinSentiment models, in general, show the best performance across three financial sentiment analysis datasets, even when only a subpart of these models is fine‚Äêtuned with a smaller training set. Our results exhibit enhancement for each tested performance criteria on the existing results for these datasets. Extensive experimental results demonstrate the effectiveness and robustness of especially RoBERTa pretrained on financial corpora. Overall, we show that NLP transfer learning techniques are favorable solutions to financial sentiment analysis tasks. Our source code has been deposited at https://github.com/seferlab/finsentiment .",ac89af7c-d4e9-40ca-abaf-ccf6db716235,4,105,(missing PubMedId)
10.1007/S10614-025-10901-8,Enhancing Sentiment Analysis in Stock Market Tweets Through BERT-Based Knowledge Transfer,"Emre Cicekyurt,  Gokhan Bakal",Computational Economics,2025,"
 
 One of the widely studied text classification efforts is sentiment analysis. It is a specific examination involving natural language processing and machine learning methods to understand semantic orientation from textual data. Working social media posts, such as tweets, for sentiment analysis, is quite common among researchers due to the speed of information dissemination. In this regard, forecasting stock market tweets is a widely studied research topic. Some studies have revealed a strong connection between sentiment and stock market performance, while others have not found any notable associations. The proposed work shows two distinct approaches to sentiment analysis over the stock market tweets. The first approach employs traditional machine learning algorithms, including logistic regression, random forest, and XGBoost. The second approach constructs deep learning (as a subfield of machine learning) models using LSTM and CNN algorithms to classify the test instances into positive, negative, or neutral classes through ten randomly shuffled data splits. In this study, the labeled data size is gradually increased utilizing a pre-trained model, FinBERT. It is exclusively employed to label unlabeled data instances to integrate them into the experiments. The goal is to monitor the effect of the additional newly-labeled examples on the sentiment analysis performance. The experiments showed that the average F1-score improved by 20% for the deep learning models and 17% for the machine learning models. In the end, the paper reveals a strong positive correlation between training data size and the classification performance of the experimental approaches.
",551b87f3-99f7-4ff3-8baf-6d830628182b,8,50,(missing PubMedId)
10.3390/IJFS13020075,Financial Sentiment Analysis and Classification: A Comparative Study of Fine-Tuned Deep Learning Models,"Dimitrios Œö. Nasiopoulos,  Konstantinos I. Roumeliotis,  ŒîŒ±ŒºŒπŒ±ŒΩœåœÇ Œ†. Œ£Œ±Œ∫Œ¨œÇ,  Kanellos Toudas,  Panagiotis Reklitis",International Journal of Financial Studies,2025,"Financial sentiment analysis is crucial for making informed decisions in the financial markets, as it helps predict trends, guide investments, and assess economic conditions. Traditional methods for financial sentiment classification, such as Support Vector Machines (SVM), Random Forests, and Logistic Regression, served as our baseline models. While somewhat effective, these conventional approaches often struggled to capture the complexity and nuance of financial language. Recent advancements in deep learning, particularly transformer-based models like GPT and BERT, have significantly enhanced sentiment analysis by capturing intricate linguistic patterns. In this study, we explore the application of deep learning for financial sentiment analysis, focusing on fine-tuning GPT-4o, GPT-4o-mini, BERT, and FinBERT, alongside comparisons with traditional models. To ensure optimal configurations, we performed hyperparameter tuning using Bayesian optimization across 100 trials. Using a combined dataset of FiQA and Financial PhraseBank, we first apply zero-shot classification and then fine tune each model to improve performance. The results demonstrate substantial improvements in sentiment prediction accuracy post-fine-tuning, with GPT-4o-mini showing strong efficiency and performance. Our findings highlight the potential of deep learning models, particularly GPT models, in advancing financial sentiment classification, offering valuable insights for investors and financial analysts seeking to understand market sentiment and make data-driven decisions.",81e63d89-0558-46c4-88e4-74f6c9602107,3,37,(missing PubMedId)
10.48550/ARXIV.2401.05215,Pre-trained Large Language Models for Financial Sentiment Analysis,"Wei Luo, Dihong Gong",arXiv.org,2024,"Financial sentiment analysis refers to classifying financial text contents into sentiment categories (e.g. positive, negative, and neutral). In this paper, we focus on the classification of financial news title, which is a challenging task due to a lack of large amount of training samples. To overcome this difficulty, we propose to adapt the pretrained large language models (LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge amount of text corpora,have an advantage in text understanding and can be effectively adapted to domain-specific task while requiring very few amount of training samples. In particular, we adapt the open-source Llama2-7B model (2023) with the supervised fine-tuning (SFT) technique [4]. Experimental evaluation shows that even with the 7B model (which is relatively small for LLMs), our approach significantly outperforms the previous state-of-the-art algorithms.",c2e7b55d-5d0e-4da4-bd8c-18f0ce5b559d,7,27,(missing PubMedId)
10.1109/IDCIOT64235.2025.10914764,"Comparative Advances in Financial Sentiment Analysis:A Review of BERT,FinBert, and Large Language Models","M. Mahendran,  Akilesh Gokul,  P Sree Lakshmi,  S. Pavithra",2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),2025,"Capturing market sentiments and supporting well-informed financial decision-making depend on the developing field of Financial Sentiment Analysis (FSA). Natural language processing (NLP) has made significant strides in comprehending and categorizing sentiment in intricate financial texts, especially with the use of Large Language Models (LLMs). The application of various LLMs, such as Bidirectional Encoder Representations form Transformers(BERT) and Financial BERT (FinBERT), as well as distilled models, such as DistilBERT and DistilRoBERTa, on a variety of financial datasets, including Financial Phrase Bank and LexisNexis news articles, was the main focus of this review article. highlighting different approaches such as model fine-tuning, zero-shot and few-shot learning, and prompt engineering. The study focuses on practical model predictions through a case study of sentimental analysis of cryptocurrencies. While FinBERT, a financial variant of BERT, exhibits high accuracy and robustness, other LLMs exhibit varying degrees of success based on the dataset and domain requirements. The analysis concentrates on the difficulties, compromises, and potential paths for improving LLMs for financial sentiment analysis.",e3f82bc4-9db7-4d34-b4a1-5cb9b989b7e0,2,30,(missing PubMedId)
10.48550/ARXIV.2409.18999,Enhancing TinyBERT for Financial Sentiment Analysis Using GPT-Augmented FinBERT Distillation,Geoffrey Thomas,arXiv.org,2024,"In the rapidly evolving field of financial sentiment analysis, the efficiency and accuracy of predictive models are critical due to their significant impact on financial markets. Transformer based models like BERT and large language models (LLMs) like GPT-4, have advanced NLP tasks considerably. Despite their advantages, BERT-based models face challenges with computational intensity in edge computing environments, and the substantial size and compute requirements of LLMs limit their practical deployment. This study proposes leveraging the generative capabilities of LLMs, such as GPT-4 Omni, to create synthetic, domain-specific training data. This approach addresses the challenge of data scarcity and enhances the performance of smaller models by making them competitive with their larger counterparts. The research specifically aims to enhance FinBERT, a BERT model fine-tuned for financial sentiment analysis, and develop TinyFinBERT, a compact transformer model, through a structured, two-tiered knowledge distillation strategy. Using data augmented by GPT-4 Omni, which involves generating new training examples and transforming existing data, we significantly improved the accuracy of FinBERT, preparing it to serve as a teacher model. This enhanced FinBERT then distilled knowledge to TinyFinBERT, employing both GPT-4 Omni and GPT-3.5 Turbo augmented data. The distillation strategy incorporated both logit and intermediate layer distillation. The training and evaluation of TinyFinBERT utilized the PhraseBank dataset and the FiQA 2018 Task1 dataset, achieving performance comparable to FinBERT while being substantially smaller and more efficient. This research demonstrates how LLMs can effectively contribute to the advancement of financial sentiment analysis by enhancing the capabilities of smaller, more efficient models through innovative data augmentation and distillation techniques.",24136ceb-c004-4998-810c-da7196bc534b,7,0,(missing PubMedId)
10.48550/ARXIV.2412.09859,Financial Sentiment Analysis: Leveraging Actual and Synthetic Data for Supervised Fine-tuning,Abraham Atsiwo,arXiv.org,2024,"The Efficient Market Hypothesis (EMH) highlights the essence of financial news in stock price movement. Financial news comes in the form of corporate announcements, news titles, and other forms of digital text. The generation of insights from financial news can be done with sentiment analysis. General-purpose language models are too general for sentiment analysis in finance. Curated labeled data for fine-tuning general-purpose language models are scare, and existing fine-tuned models for sentiment analysis in finance do not capture the maximum context width. We hypothesize that using actual and synthetic data can improve performance. We introduce BertNSP-finance to concatenate shorter financial sentences into longer financial sentences, and finbert-lc to determine sentiment from digital text. The results show improved performance on the accuracy and the f1 score for the financial phrasebank data with $50\%$ and $100\%$ agreement levels.",18cf056c-4625-48ab-9789-1fc38745e197,1,0,(missing PubMedId)