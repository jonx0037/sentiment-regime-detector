[[Liu_et+al_LargeLanguageModelsSentimentAnalysis_2024]]

# [Large Language Models and Sentiment Analysis in Financial Markets: A Review, Datasets, and Case Study](https://doi.org/10.1109/access.2024.3445413)

## [[Chenghao Liu]]; [[Arunkumar Arulappan]]; [[Ranesh Kumar Naha]] et al.

## Abstract
==This paper comprehensively examines Large Language Models (LLMs) in sentiment analysis, specifically focusing on financial markets and exploring the correlation between news sentiment and Bitcoin prices==. We systematically categorize various LLMs used in financial sentiment analysis, highlighting their unique applications and features. ==We also investigate the methodologies for effective data collection and categorization, underscoring the need for diverse and comprehensive datasets==. Our research features a case study investigating the correlation between news sentiment and Bitcoin prices, utilizing advanced sentiment analysis and financial analysis methods to demonstrate the practical application of LLMs. ==The findings reveal a modest but discernible correlation between news sentiment and Bitcoin price fluctuations, with historical news patterns showing a more substantial impact on Bitcoin's longer-term price than immediate news events==. This highlights LLMs' potential in market trend prediction and informed investment decision-making. © 2013 IEEE.

## Key concepts
#behavioral_economics; #sentiment_analysis; #finding/BERT; #BERT; #financial_sentiment_analysis; #ChatGPT; #claim/bitcoin; #bitcoin; #financial_markets; #natural_language_processing; #machine_learning; #claim/large_language_models; #large_language_models

## Quote
> This paper examines the use of Large Language Models (LLMs) in sentiment analysis for financial markets, specifically focusing on the correlation between news sentiment and Bitcoin prices, and highlights their potential in market trend prediction and informed investment decision-making.

## Key points
- Sentiment analysis (SA) in financial markets has emerged as a critical study area, given its widespread application in specific sectors like the stock market [^1], [^2], [^3], [^4]
- In this comprehensive literature review, we adeptly examine the intersection of Large Language Models (LLMs) and sentiment analysis within financial markets, providing a detailed exploration of LLMs’ evolution, application, and future opportunities in this domain
- The review navigates through the intricacies of sentiment analysis, underlining its significance in understanding market dynamics and investor behavior
- The review methodically dissects the role of LLMs in various financial contexts, from cryptocurrency market prediction to stock price forecasting, showcasing their capability to extract and interpret complex economic sentiments
- The case study on Bitcoin price and news sentiment further exemplifies the practical application of LLMs, reinforcing that sentiment analysis, powered by advanced language models, is pivotal in deciphering market trends
- The review is open to addressing the challenges and limitations inherent in the current state of LLMs


## Summary

### Introduction To LLMs
The paper examines Large Language Models (LLMs) in sentiment analysis, focusing on financial markets and the correlation between news sentiment and Bitcoin prices.
LLMs have shown remarkable proficiency in mimicking human language skills, resulting in significant transformations across various fields, including the financial domain.
The study categorizes various LLMs used in financial sentiment analysis, highlighting their unique applications and features.

### Sentiment Analysis
Sentiment analysis in financial markets has emerged as a critical study area, particularly given its widespread application in specific sectors like the stock market.
The study investigates the correlation between news sentiment, as analyzed by LLMs, and Bitcoin price movements, utilizing advanced sentiment analysis and financial analysis methods.
The findings reveal a modest but discernible correlation between news sentiment and Bitcoin price fluctuations.
Sentiment analysis has been identified as a critical component in predicting stock prices and cryptocurrency market trends.
LLMs have been used to analyze social sentiment data from sources such as GitHub and Reddit, synthesizing emotional and sentiment indicators from social media commentary into hourly and daily series datasets.
The findings have indicated that incorporating these social sentiment metrics markedly enhances the predictive accuracy for the daily pricing of Bitcoin and Ethereum.
For instance, Ortu et al investigated cryptocurrency price prediction by analyzing social sentiment data, employing a pre-trained BERT-based model to synthesize emotional and sentiment indicators.

### LLM Applications
The study features a case study investigating the correlation between news sentiment and Bitcoin prices, utilizing advanced sentiment analysis and financial analysis methods to demonstrate the practical application of LLMs.
The paper makes a significant contribution to the field of financial sentiment analysis by integrating the advanced capabilities of LLMs with the dynamic realm of Bitcoin and cryptocurrency markets.
The focus on the unique features and applications of these LLMs in the financial domain reveals new insights into their transformative role in market trend prediction and investment decision-making.

### LLMs
LLMs are categorized into three types based on their architecture: encoder-only, encoder-decoder, and decoder-only.
Encoder-only LLMs, such as BERT, process input text into a hidden representation.
Encoder-decoder LLMs, like FINMEM, integrate both encoder and decoder components to produce output text.
Decoder-only LLMs, including the GPT series, use the decoder module to generate output text.
These models have shown promising performance in financial sentiment analysis, with applications in predicting market trends and optimizing trading strategies.

### Data Acquisition
Data is a crucial component in training LLMs, and its quality and diversity significantly influence model performance.
Datasets can be categorized into four groups: open-source, collected, constructed, and industrial.
Open-source datasets, such as FiQA and Financial PhraseBank, are publicly available and reliable.
Collected datasets are compiled from various sources, while constructed datasets are modified or enhanced to align with specific research goals.
Industrial datasets contain proprietary data and are essential for real-world business contexts.

### Applications
LLMs have diverse applications in financial sentiment analysis, including predicting market trends, optimizing trading strategies, and analyzing investor sentiments.
They can be used to automate financial report summaries, predict stock prices, and identify potential investment opportunities.
The integration of LLMs in the financial sector has marked a significant evolution in how financial data is analyzed and interpreted, with potential to transform the industry.

### Predictive Analytics
The application of Large Language Models (LLMs) for predicting cryptocurrency market trends has shown promising results, particularly when integrating sentiment analysis into these predictions.
Studies have demonstrated the potential of LLMs to distill sentiment from vast datasets, offering a novel dimension to forecasting models.
For example, Zou and Herremans introduced a pioneering multimodal model, PreBit, to anticipate significant Bitcoin price movements.
Other studies have highlighted the advantages of refining FinBERT with weakly labeled data, illustrating how even imprecisely labeled datasets can significantly improve text-based feature prediction and forecasting accuracy for cryptocurrency returns.

### Stock Market Forecasting
LLMs have been applied to stock market forecasting, showcasing their versatility across various financial applications.
Studies have demonstrated the utility of LLMs in economic text mining, suggesting further application of FinBERT across different financial NLP tasks.
For example, Araci introduced FinBERT, a model tailored for the financial sector, demonstrating superior capabilities in economic text mining.
Other studies have provided evidence that contextual embeddings substantially improve efficiency for sentiment analysis over traditional lexicons and static word encoders, pointing to the potential of LLMs to revolutionize sentiment analysis with a more profound understanding of contextual nuances in financial texts.

### Models
The DCC model allows for variation in correlations over time, adding flexibility and realism to the analysis.
It directly accounts for heteroscedasticity by calculating Dynamic Conditional Correlations (DCCs) from standardized residuals.
The DCC model involves a two-step process to ascertain conditional correlations, initially estimating a univariate GARCH model for each return series.
Transfer entropy offers distinct advantages over traditional methods, facilitating a non-parametric analysis of time-series data and minimizing the need for extensive presumptions about stochastic processes.

### Methodology
The DCC model is delineated using a two-step process, with a univariate GARCH model estimated for each return series.
Transfer entropy is calculated using the formula for transfer entropy from J to I, which measures the net information flow.
The Markov process is introduced to estimate the likelihood of transitioning from one state to another during information transfer.
The escort distribution is used to facilitate the prediction of potential transition matrix scenarios.

### Results
The results of the case study analyzing the volatility of Bitcoin prices and news sentiment show a low long-term correlation between Bitcoin prices and news sentiment.
The DCC-GARCH model reveals dynamic adjustments in conditional correlation within a multivariate framework.
Transfer entropy values highlight significant spillover effects within the cryptocurrency markets, with smaller market capitalization cryptocurrencies reacting more sensitively to changes.
News events that substantially affect Bitcoin's valuation often initiate a domino effect, impacting the valuations of other cryptocurrencies.

### Challenges
The study highlights several challenges associated with using Large Language Models (LLMs) in sentiment analysis, including technical difficulties, generalizability, and interpretability.
The increasing size of LLMs, such as GPT-1 to GPT-3, poses significant computational and storage demands, raising concerns about accessibility in resource-limited contexts.
Additionally, LLMs often struggle to maintain consistent performance across diverse domains and tasks, and their opaque nature can hinder trust and reliability.

### Future Opportunities
The future of LLMs in sentiment analysis holds promise, with opportunities for optimization, expansion of natural language processing capabilities, and enhancement of performance in existing sentiment analysis tasks.
The development of more efficient deployment strategies, improved generalizability, and enhanced interpretability is crucial for the advancement of LLMs.
The integration of more diverse data types, such as spoken language, diagrams, and multimodal data, could significantly expand the capabilities of LLMs in capturing and interpreting varied forms of user sentiment.

### Limitations
The study acknowledges the limitations of the current state of LLMs, including the need for more extensive datasets, uniform research methods, and a universal evaluation framework.
The lack of transparency in many LLMs, concerns over data quality and ownership, and potential vulnerability to adversarial attacks are also highlighted as significant challenges.
Addressing these limitations is essential for the continued innovation and development of LLMs in sentiment analysis.


## Study subjects

### 46143 documents
- TRC2-financial is a specialized subset of the TRC244 collection from Reuters, which encompasses 1.8 million news articles released between 2008 and 2010. ==This subset specifically contains 46,143 documents, totaling nearly 29 million words and close to 400,000 sentences [^12]==. SemEval 2017 Task 5 focuses on fine-grained sentiment analysis (FSA) of news headlines and microblogs [^59]

### 1694 microblog posts
- SemEval 2017 Task 5 focuses on fine-grained sentiment analysis (FSA) of news headlines and microblogs [^59]. ==The training set for this task includes 1,142 financial news headlines and 1,694 microblog posts, each annotated with target entities and their corresponding sentiment scores==. The test set comprises 491 financial news headlines and 794 posts [^11]

## Data analysis
- #method/gpt_model
- #method/adf_test
- #method/large_language_models
- #method/adf_test_statistic
- #method/finbert_model
- #method/dcc_model
- #method/garch_model

## Findings
- Furthermore, LLaMA2 has proven effective, reaching an accuracy of 84.03% through supervised learning and aligning financial texts [^54]
- Our dataset represents over 80% of the cryptocurrency market’s total market capitalization, ensuring a comprehensive analysis scope
- FinBERT’s performance in <a class="keyword" href="https://en.wikipedia.org/wiki/Sentiment_analysis#Financial_sentiment_analysis" title="financial sentiment analysis">financial sentiment analysis</a> tasks showed a notable 15% improvement over generic <a class="keyword" href="https://en.wikipedia.org/wiki/BERT_(language_model)" title="BERT">BERT</a> models [^12]

##  Builds on previous research
- However, it is important to mention that there is no widely agreed-upon standard in the literature for the minimum parameter size for an LLM, as its efficiency is linked to the dataset’s size and the total computing power used. ==In our study, we follow the classification and taxonomy of LLMs introduced by Pan et al== [^38], dividing mainstream LLMs into three categories based on their architecture: encoder-only, encoder-decoder, and decoder-only.

##  Confirmation of earlier findings
- Nonetheless, Bitcoin exerts a more profound influence on these cryptocurrencies. Given that Bitcoin accounts for approximately 50% of the total market capitalization of all cryptocurrencies, ==our observations align with those reported in the study by Zhang et al== [^83].

## Counterpoint to earlier claims
- Table 6 details the outcomes of unit root tests conducted on the variables utilized in this study, explicitly presenting the Augmented Dickey-Fuller (ADF) test results for each factor. ==Despite the fact that stationarity is not a prerequisite for utilizing the transfer entropy approach, which can handle probability density functions from a single realization as highlighted by Wollstadt et al [^82], we nevertheless proceeded to perform a stationarity test==.

## Contributions
- In this comprehensive literature review, <mark class="fact">we adeptly examine the intersection of LLMs</mark> and sentiment analysis within financial markets, providing a detailed exploration of LLMs’ evolution, application, and future opportunities in this domain. <mark class="fact">The review navigates through the intricacies of sentiment analysis</mark>, underlining its significance in understanding market dynamics and investor behavior. Our meticulous analysis of LLMs, mainly their development from BERT [^14] to more sophisticated models like FinBERT [^12] and ChatGPT, reveals these models’ substantial impact on financial sentiment analysis.<mark class="fact">The review methodically dissects the role of LLMs in various financial contexts</mark>, from cryptocurrency market prediction to stock price forecasting, showcasing their capability to extract and interpret complex economic sentiments. <mark class="fact">The case study on Bitcoin price and news sentiment further exemplifies the practical application of LLMs</mark>, reinforcing that sentiment analysis, powered by advanced language models, is pivotal in deciphering market trends.

## Future work
- The future work includes overcoming the limitations of LLMs, such as improving their generalizability and interpretability. The study also suggests the potential expansion of LLM capabilities to include multimodal data inputs and the implementation of a standard evaluation framework.
- No information is provided about future work related to the study.


## References
[^1]: M. Baker and J. Wurgler, ‘‘Investor sentiment in the stock market,’’ J. Econ. Perspect., vol. 21, no. 2, pp. 129–152, 2007.  [OA](https://engine.scholarcy.com/oa_version?query=Baker%2C%20M.%20Wurgler%2C%20J.%20%E2%80%98Investor%20sentiment%20in%20the%20stock%20market%2C%E2%80%99%202007&author=Baker&title=%E2%80%98Investor%20sentiment%20in%20the%20stock%20market%2C%E2%80%99&year=2007) [GScholar](https://scholar.google.co.uk/scholar?q=Baker%2C%20M.%20Wurgler%2C%20J.%20%E2%80%98Investor%20sentiment%20in%20the%20stock%20market%2C%E2%80%99%202007) [Scite](/scite_tallies?query=author%3ABaker%2Ctitle%3A%E2%80%98Investor%20sentiment%20in%20the%20stock%20market%2C%E2%80%99%2Cyear%3A2007)

[^2]: P. C. Tetlock, ‘‘Giving content to investor sentiment: The role of media in the stock market,’’ J. Finance, vol. 62, pp. 1139–1168, Jun. 2007. [Online]. Available: https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.2007.01232.x  [OA](https://doi.org/10.1111/j.1540-6261.2007.01232.x)  [Scite](/scite_tallies?query=https://doi.org/10.1111/j.1540-6261.2007.01232.x)

[^3]: L. A. Smales, ‘‘The importance of fear: Investor sentiment and stock market returns,’’ Appl. Econ., vol. 49, no. 34, pp. 3395–3421, Jul. 2017. [Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/00036846.2016.1259754  [OA](https://doi.org/10.1080/00036846.2016.1259754)  [Scite](/scite_tallies?query=https://doi.org/10.1080/00036846.2016.1259754)

[^4]: T. Rao and S. Srivastava. (2012). Analyzing Stock Market Movements Using Twitter Sentiment Analysis. [Online]. Available: http://dx.doi.org/10.1109/ASONAM.2012.30 and https://repository.lincoln.ac.uk/articles/conference_contribution/Analyzing_stock_market_movements_using_T witter_sentiment_analysis/25165223/2?file=44450105  [OA](https://doi.org/10.1109/ASONAM.2012.30)  [Scite](/scite_tallies?query=https://doi.org/10.1109/ASONAM.2012.30)

[^5]: E. Cambria and B. White, ‘‘Jumping NLP curves: A review of natural language processing research,’’ IEEE Comput. Intell. Mag., vol. 9, no. 2, pp. 48–57, May 2014.  [OA](https://engine.scholarcy.com/oa_version?query=Cambria%2C%20E.%20White%2C%20B.%20%E2%80%98Jumping%20NLP%20curves%3A%20A%20review%20of%20natural%20language%20processing%20research%2C%E2%80%99%202014-05&author=Cambria&title=%E2%80%98Jumping%20NLP%20curves%3A%20A%20review%20of%20natural%20language%20processing%20research%2C%E2%80%99&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Cambria%2C%20E.%20White%2C%20B.%20%E2%80%98Jumping%20NLP%20curves%3A%20A%20review%20of%20natural%20language%20processing%20research%2C%E2%80%99%202014-05) [Scite](/scite_tallies?query=author%3ACambria%2Ctitle%3A%E2%80%98Jumping%20NLP%20curves%3A%20A%20review%20of%20natural%20language%20processing%20research%2C%E2%80%99%2Cyear%3A2014)

[^6]: V. Ramiah, X. Xu, and I. A. Moosa, ‘‘Neoclassical finance, behavioral finance and noise traders: A review and assessment of the literature,’’ Int. Rev. Financial Anal., vol. 41, pp. 89–100, Oct. 2015.  [OA](https://engine.scholarcy.com/oa_version?query=Ramiah%2C%20V.%20Xu%2C%20X.%20Moosa%2C%20I.A.%20%E2%80%98Neoclassical%20finance%2C%20behavioral%20finance%20and%20noise%20traders%3A%20A%20review%20and%20assessment%20of%20the%20literature%2C%E2%80%99%202015-10&author=Ramiah&title=%E2%80%98Neoclassical%20finance%2C%20behavioral%20finance%20and%20noise%20traders%3A%20A%20review%20and%20assessment%20of%20the%20literature%2C%E2%80%99&year=2015) [GScholar](https://scholar.google.co.uk/scholar?q=Ramiah%2C%20V.%20Xu%2C%20X.%20Moosa%2C%20I.A.%20%E2%80%98Neoclassical%20finance%2C%20behavioral%20finance%20and%20noise%20traders%3A%20A%20review%20and%20assessment%20of%20the%20literature%2C%E2%80%99%202015-10) [Scite](/scite_tallies?query=author%3ARamiah%2Ctitle%3A%E2%80%98Neoclassical%20finance%2C%20behavioral%20finance%20and%20noise%20traders%3A%20A%20review%20and%20assessment%20of%20the%20literature%2C%E2%80%99%2Cyear%3A2015)

[^7]: F. Wu, Y. Huang, and Y. Song, ‘‘Structured microblog sentiment classification via social context regularization,’’ Neurocomputing, vol. 175, pp. 599–609, Jan. 2016.  [OA](https://engine.scholarcy.com/oa_version?query=Wu%2C%20F.%20Huang%2C%20Y.%20Song%2C%20Y.%20%E2%80%98Structured%20microblog%20sentiment%20classification%20via%20social%20context%20regularization%2C%E2%80%99%202016-01&author=Wu&title=%E2%80%98Structured%20microblog%20sentiment%20classification%20via%20social%20context%20regularization%2C%E2%80%99&year=2016) [GScholar](https://scholar.google.co.uk/scholar?q=Wu%2C%20F.%20Huang%2C%20Y.%20Song%2C%20Y.%20%E2%80%98Structured%20microblog%20sentiment%20classification%20via%20social%20context%20regularization%2C%E2%80%99%202016-01) [Scite](/scite_tallies?query=author%3AWu%2Ctitle%3A%E2%80%98Structured%20microblog%20sentiment%20classification%20via%20social%20context%20regularization%2C%E2%80%99%2Cyear%3A2016)

[^8]: T. Al-Moslmi, S. Gaber, M. Albared, and N. Omar. (2016). Feature Selection Methods Effects on Machine Learning Approaches in Malay Sentiment Analysis. [Online]. Available: https://www.researchgate.net/publication/308968243  [OA](https://www.researchgate.net/publication/308968243)  

[^9]: R. C. Moore and W. Lewis, ‘‘Intelligent selection of language model training data,’’ in Proc. ACL Conf. Short Papers, 2010, pp. 220–224.  [OA](https://scholar.google.co.uk/scholar?q=Moore%2C%20R.C.%20Lewis%2C%20W.%20%E2%80%98Intelligent%20selection%20of%20language%20model%20training%20data%2C%E2%80%99%202010) [GScholar](https://scholar.google.co.uk/scholar?q=Moore%2C%20R.C.%20Lewis%2C%20W.%20%E2%80%98Intelligent%20selection%20of%20language%20model%20training%20data%2C%E2%80%99%202010) 

[^10]: S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann, ‘‘BloombergGPT: A large language model for finance,’’ 2023, arXiv:2303.17564.  [OA](https://arxiv.org/abs/2303.17564)  

[^11]: K. Mishev, A. Gjorgjevikj, I. Vodenska, L. T. Chitkushev, and D. Trajanov, ‘‘Evaluation of sentiment analysis in finance: From lexicons to transformers,’’ IEEE Access, vol. 8, pp. 131662–131682, 2020.  [OA](https://engine.scholarcy.com/oa_version?query=Mishev%2C%20K.%20Gjorgjevikj%2C%20A.%20Vodenska%2C%20I.%20Chitkushev%2C%20L.T.%20%E2%80%98Evaluation%20of%20sentiment%20analysis%20in%20finance%3A%20From%20lexicons%20to%20transformers%2C%E2%80%99%202020&author=Mishev&title=%E2%80%98Evaluation%20of%20sentiment%20analysis%20in%20finance%3A%20From%20lexicons%20to%20transformers%2C%E2%80%99&year=2020) [GScholar](https://scholar.google.co.uk/scholar?q=Mishev%2C%20K.%20Gjorgjevikj%2C%20A.%20Vodenska%2C%20I.%20Chitkushev%2C%20L.T.%20%E2%80%98Evaluation%20of%20sentiment%20analysis%20in%20finance%3A%20From%20lexicons%20to%20transformers%2C%E2%80%99%202020) [Scite](/scite_tallies?query=author%3AMishev%2Ctitle%3A%E2%80%98Evaluation%20of%20sentiment%20analysis%20in%20finance%3A%20From%20lexicons%20to%20transformers%2C%E2%80%99%2Cyear%3A2020)

[^12]: D. Araci, ‘‘FinBERT: Financial sentiment analysis with pre-trained language models,’’ 2019, arXiv:1908.10063.  [OA](https://arxiv.org/abs/1908.10063)  

[^13]: P. Seroyizhko, Z. Zhexenova, M. Z. Shafiq, F. Merizzi, A. Galassi, and F. Ruggeri, ‘‘A sentiment and emotion annotated dataset for Bitcoin price forecasting based on Reddit posts,’’ in Proc. 4th Workshop Financial Technol. Natural Lang. Process. (FinNLP), 2022, pp. 203–210. [Online]. Available: https://aclanthology.org/2022.finnlp-1.27  [OA](https://aclanthology.org/2022.finnlp-1.27)  

[^14]: J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training of deep bidirectional transformers for language understanding,’’ in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol., vol. 1, Oct. 2018, pp. 4171–4186.  [OA](https://engine.scholarcy.com/oa_version?query=Devlin%2C%20J.%20Chang%2C%20M.-W.%20Lee%2C%20K.%20Toutanova%2C%20K.%20%E2%80%98BERT%3A%20Pre-training%20of%20deep%20bidirectional%20transformers%20for%20language%20understanding%2C%E2%80%99%202018-10&author=Devlin&title=%E2%80%98BERT%3A%20Pre-training%20of%20deep%20bidirectional%20transformers%20for%20language%20understanding%2C%E2%80%99&year=2018) [GScholar](https://scholar.google.co.uk/scholar?q=Devlin%2C%20J.%20Chang%2C%20M.-W.%20Lee%2C%20K.%20Toutanova%2C%20K.%20%E2%80%98BERT%3A%20Pre-training%20of%20deep%20bidirectional%20transformers%20for%20language%20understanding%2C%E2%80%99%202018-10) [Scite](/scite_tallies?query=author%3ADevlin%2Ctitle%3A%E2%80%98BERT%3A%20Pre-training%20of%20deep%20bidirectional%20transformers%20for%20language%20understanding%2C%E2%80%99%2Cyear%3A2018)

[^15]: J. Kocon, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szydlo, J. Baran, J. Bielaniewicz, M. Gruza, A. Janz, K. Kanclerz, A. Kocon, B. Koptyra, W. Mieleszczenko-Kowszewicz, P. Milkowski, M. Oleksy, M. Piasecki, L. Radlinski, K. Wojtasik, S. Wozniak, and P. Kazienko, ‘‘ChatGPT: Jack of all trades, master of none,’’ Inf. Fusion, vol. 99, Nov. 2023, Art. no. 101861.  [OA](https://engine.scholarcy.com/oa_version?query=Kocon%2C%20J.%20Cichecki%2C%20I.%20Kaszyca%2C%20O.%20Kochanek%2C%20M.%20%E2%80%98ChatGPT%3A%20Jack%20of%20all%20trades%2C%20master%20of%20none%2C%E2%80%99%202023-11&author=Kocon&title=%E2%80%98ChatGPT%3A%20Jack%20of%20all%20trades%2C%20master%20of%20none%2C%E2%80%99&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Kocon%2C%20J.%20Cichecki%2C%20I.%20Kaszyca%2C%20O.%20Kochanek%2C%20M.%20%E2%80%98ChatGPT%3A%20Jack%20of%20all%20trades%2C%20master%20of%20none%2C%E2%80%99%202023-11) [Scite](/scite_tallies?query=author%3AKocon%2Ctitle%3A%E2%80%98ChatGPT%3A%20Jack%20of%20all%20trades%2C%20master%20of%20none%2C%E2%80%99%2Cyear%3A2023)

[^16]: M. Chakraborty and S. Subramaniam, ‘‘Does sentiment impact cryptocurrency?’’ J. Behav. Finance, vol. 24, no. 2, pp. 202–218, Apr. 2023. [Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/15427560.2021.1950723  [OA](https://doi.org/10.1080/15427560.2021.1950723)  [Scite](/scite_tallies?query=https://doi.org/10.1080/15427560.2021.1950723)

[^17]: A. H. Huang, H. Wang, and Y. Yang, ‘‘FinBERT: A large language model for extracting information from financial text,’’ Contemp. Accounting Res., vol. 40, no. 2, pp. 806–841, May 2023. [Online]. Available: https://onlinelibrary.wiley.com/doi/full/10.1111/1911-3846.12832  [OA](https://doi.org/10.1111/1911-3846.12832)  [Scite](/scite_tallies?query=https://doi.org/10.1111/1911-3846.12832)

[^18]: H. Tong, J. Li, N. Wu, M. Gong, D. Zhang, and Q. Zhang, ‘‘Ploutos: Towards interpretable stock movement prediction with financial large language model,’’ 2024, arXiv:2403.00782.  [OA](https://arxiv.org/abs/2403.00782)  

[^19]: A. S. George and A. H. George, ‘‘A review of ChatGPT AIs impact on several business sectors,’’ Partners Universal Int. Innov. J., vol. 1, no. 1, pp. 9–23, 2023.  [OA](https://engine.scholarcy.com/oa_version?query=George%2C%20A.S.%20George%2C%20A.H.%20%E2%80%98A%20review%20of%20ChatGPT%20AIs%20impact%20on%20several%20business%20sectors%2C%E2%80%99%202023&author=George&title=%E2%80%98A%20review%20of%20ChatGPT%20AIs%20impact%20on%20several%20business%20sectors%2C%E2%80%99&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=George%2C%20A.S.%20George%2C%20A.H.%20%E2%80%98A%20review%20of%20ChatGPT%20AIs%20impact%20on%20several%20business%20sectors%2C%E2%80%99%202023) [Scite](/scite_tallies?query=author%3AGeorge%2Ctitle%3A%E2%80%98A%20review%20of%20ChatGPT%20AIs%20impact%20on%20several%20business%20sectors%2C%E2%80%99%2Cyear%3A2023)

[^20]: N. A. Sharma, A. B. M. S. Ali, and M. A. Kabir, ‘‘A review of sentiment analysis: Tasks, applications, and deep learning techniques,’’ Int. J. Data Sci. Anal., pp. 1–38, Jul. 2024, doi:10.1007/s41060-024-00594-x.  [OA](https://doi.org/10.1007/s41060-024-00594-x)  [Scite](/scite_tallies?query=https://doi.org/10.1007/s41060-024-00594-x)

[^21]: M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib, M. M. J. Mim, J. Ahmad, M. E. Ali, and S. Azam, ‘‘A review on large language models: Architectures, applications, taxonomies, open issues and challenges,’’ IEEE Access, vol. 12, pp. 26839–26874, 2024.  [OA](https://engine.scholarcy.com/oa_version?query=Raiaan%2C%20M.A.K.%20Mukta%2C%20M.S.H.%20Fatema%2C%20K.%20Fahad%2C%20N.M.%20%E2%80%98A%20review%20on%20large%20language%20models%3A%20Architectures%2C%20applications%2C%20taxonomies%2C%20open%20issues%20and%20challenges%2C%E2%80%99%202024&author=Raiaan&title=%E2%80%98A%20review%20on%20large%20language%20models%3A%20Architectures%2C%20applications%2C%20taxonomies%2C%20open%20issues%20and%20challenges%2C%E2%80%99&year=2024) [GScholar](https://scholar.google.co.uk/scholar?q=Raiaan%2C%20M.A.K.%20Mukta%2C%20M.S.H.%20Fatema%2C%20K.%20Fahad%2C%20N.M.%20%E2%80%98A%20review%20on%20large%20language%20models%3A%20Architectures%2C%20applications%2C%20taxonomies%2C%20open%20issues%20and%20challenges%2C%E2%80%99%202024) [Scite](/scite_tallies?query=author%3ARaiaan%2Ctitle%3A%E2%80%98A%20review%20on%20large%20language%20models%3A%20Architectures%2C%20applications%2C%20taxonomies%2C%20open%20issues%20and%20challenges%2C%E2%80%99%2Cyear%3A2024)

[^22]: B. Chen, Z. Wu, and R. Zhao, ‘‘From fiction to fact: The growing role of generative AI in business and finance,’’ J. Chin. Econ. Bus. Stud., vol. 21, no. 4, pp. 471–496, Oct. 2023.  [OA](https://engine.scholarcy.com/oa_version?query=Chen%2C%20B.%20Wu%2C%20Z.%20Zhao%2C%20R.%20%E2%80%98From%20fiction%20to%20fact%3A%20The%20growing%20role%20of%20generative%20AI%20in%20business%20and%20finance%2C%E2%80%99%202023-10&author=Chen&title=%E2%80%98From%20fiction%20to%20fact%3A%20The%20growing%20role%20of%20generative%20AI%20in%20business%20and%20finance%2C%E2%80%99&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Chen%2C%20B.%20Wu%2C%20Z.%20Zhao%2C%20R.%20%E2%80%98From%20fiction%20to%20fact%3A%20The%20growing%20role%20of%20generative%20AI%20in%20business%20and%20finance%2C%E2%80%99%202023-10) [Scite](/scite_tallies?query=author%3AChen%2Ctitle%3A%E2%80%98From%20fiction%20to%20fact%3A%20The%20growing%20role%20of%20generative%20AI%20in%20business%20and%20finance%2C%E2%80%99%2Cyear%3A2023)

[^23]: M. M. Dong, T. C. Stratopoulos, and V. X. Wang, A Scoping Review of ChatGPT Research in Accounting and Finance, T. C. Wang and V. Xiaoqi, Eds., Dec. 2023. [Online]. Available: https://ssrn.com/abstract=4680203 and http://dx.doi.org/10.2139/ssrn.4680203  [OA](https://doi.org/10.2139/ssrn.4680203)  [Scite](/scite_tallies?query=https://doi.org/10.2139/ssrn.4680203)

[^24]: S. A. Farimani, M. V. Jahan, and A. M. Fard, ‘‘From text representation to financial market prediction: A literature review,’’ Information, vol. 13, no. 10, p. 466, Sep. 2022.  [OA](https://engine.scholarcy.com/oa_version?query=Farimani%2C%20S.A.%20Jahan%2C%20M.V.%20Fard%2C%20A.M.%20%E2%80%98From%20text%20representation%20to%20financial%20market%20prediction%3A%20A%20literature%20review%2C%E2%80%99%202022-09&author=Farimani&title=%E2%80%98From%20text%20representation%20to%20financial%20market%20prediction%3A%20A%20literature%20review%2C%E2%80%99&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Farimani%2C%20S.A.%20Jahan%2C%20M.V.%20Fard%2C%20A.M.%20%E2%80%98From%20text%20representation%20to%20financial%20market%20prediction%3A%20A%20literature%20review%2C%E2%80%99%202022-09) [Scite](/scite_tallies?query=author%3AFarimani%2Ctitle%3A%E2%80%98From%20text%20representation%20to%20financial%20market%20prediction%3A%20A%20literature%20review%2C%E2%80%99%2Cyear%3A2022)

[^25]: A. Koshiyama, N. Firoozye, and P. Treleaven, ‘‘Algorithms in future capital markets: A survey on AI, ML and associated algorithms in capital markets,’’ in Proc. 1st ACM Int. Conf. AI Finance, 2020, pp. 1–8.  [OA](https://scholar.google.co.uk/scholar?q=Koshiyama%2C%20A.%20Firoozye%2C%20N.%20Treleaven%2C%20P.%20%E2%80%98Algorithms%20in%20future%20capital%20markets%3A%20A%20survey%20on%20AI%2C%20ML%20and%20associated%20algorithms%20in%20capital%20markets%2C%E2%80%99%202020) [GScholar](https://scholar.google.co.uk/scholar?q=Koshiyama%2C%20A.%20Firoozye%2C%20N.%20Treleaven%2C%20P.%20%E2%80%98Algorithms%20in%20future%20capital%20markets%3A%20A%20survey%20on%20AI%2C%20ML%20and%20associated%20algorithms%20in%20capital%20markets%2C%E2%80%99%202020) 

[^26]: O. Bashchenko, ‘‘Bitcoin price factors: Natural language processing approach,’’ SSRN Electron. J., vol. 13, pp. 22–48, Mar. 2022. [Online]. Available: https://papers.ssrn.com/abstract=4079091  [OA](https://papers.ssrn.com/abstract=4079091)  [Scite](/scite_tallies?query=author%3ABashchenko%2Ctitle%3A%E2%80%98Bitcoin%20price%20factors%3A%20Natural%20language%20processing%20approach%2C%E2%80%99%2Cyear%3A2022)

[^27]: B. N. Thanh, A. T. Nguyen, T. T. Chu, and S. Ha. (2023). ChatGPT, Twitter Sentiment and Bitcoin Return. [Online]. Available: https://papers.ssrn.com/abstract=4628097  [OA](https://papers.ssrn.com/abstract=4628097)  

[^28]: B. Kitchenham. (2007). Guidelines for Performing Systematic Literature Reviews in Software Engineering. [Online]. Available: https://www.researchgate.net/publication/302924724  [OA](https://www.researchgate.net/publication/302924724)  

[^29]: B. Kitchenham, L. Madeyski, and D. Budgen, ‘‘SEGRESS: Software engineering guidelines for REporting secondary studies,’’ IEEE Trans. Softw. Eng., vol. 49, no. 3, pp. 1273–1298, Mar. 2023.  [OA](https://engine.scholarcy.com/oa_version?query=Kitchenham%2C%20B.%20Madeyski%2C%20L.%20Budgen%2C%20D.%20%E2%80%98SEGRESS%3A%20Software%20engineering%20guidelines%20for%20REporting%20secondary%20studies%2C%E2%80%99%202023-03&author=Kitchenham&title=%E2%80%98SEGRESS%3A%20Software%20engineering%20guidelines%20for%20REporting%20secondary%20studies%2C%E2%80%99&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Kitchenham%2C%20B.%20Madeyski%2C%20L.%20Budgen%2C%20D.%20%E2%80%98SEGRESS%3A%20Software%20engineering%20guidelines%20for%20REporting%20secondary%20studies%2C%E2%80%99%202023-03) [Scite](/scite_tallies?query=author%3AKitchenham%2Ctitle%3A%E2%80%98SEGRESS%3A%20Software%20engineering%20guidelines%20for%20REporting%20secondary%20studies%2C%E2%80%99%2Cyear%3A2023)

[^30]: H. Zhao, Z. Liu, Z. Wu, Y. Li, T. Yang, P. Shu, S. Xu, H. Dai, L. Zhao, G. Mai, N. Liu, and T. Liu, ‘‘Revolutionizing finance with LLMs: An overview of applications and insights,’’ 2024, arXiv:2401.11641.  [OA](https://arxiv.org/abs/2401.11641)  

[^31]: K. Du, F. Xing, R. Mao, and E. Cambria, ‘‘Financial sentiment analysis: Techniques and applications,’’ ACM Comput. Surv., vol. 56, no. 9, pp. 1–42, Oct. 2024.  [OA](https://engine.scholarcy.com/oa_version?query=Du%2C%20K.%20Xing%2C%20F.%20Mao%2C%20R.%20Cambria%2C%20E.%20%E2%80%98Financial%20sentiment%20analysis%3A%20Techniques%20and%20applications%2C%E2%80%99%202024-10&author=Du&title=%E2%80%98Financial%20sentiment%20analysis%3A%20Techniques%20and%20applications%2C%E2%80%99&year=2024) [GScholar](https://scholar.google.co.uk/scholar?q=Du%2C%20K.%20Xing%2C%20F.%20Mao%2C%20R.%20Cambria%2C%20E.%20%E2%80%98Financial%20sentiment%20analysis%3A%20Techniques%20and%20applications%2C%E2%80%99%202024-10) [Scite](/scite_tallies?query=author%3ADu%2Ctitle%3A%E2%80%98Financial%20sentiment%20analysis%3A%20Techniques%20and%20applications%2C%E2%80%99%2Cyear%3A2024)

[^32]: M. N. Ashtiani and B. Raahemi, ‘‘News-based intelligent prediction of financial markets using text mining and machine learning: A systematic literature review,’’ Expert Syst. Appl., vol. 217, May 2023, Art. no. 119509.  [OA](https://engine.scholarcy.com/oa_version?query=Ashtiani%2C%20M.N.%20Raahemi%2C%20B.%20%E2%80%98News-based%20intelligent%20prediction%20of%20financial%20markets%20using%20text%20mining%20and%20machine%20learning%3A%20A%20systematic%20literature%20review%2C%E2%80%99%202023-05&author=Ashtiani&title=%E2%80%98News-based%20intelligent%20prediction%20of%20financial%20markets%20using%20text%20mining%20and%20machine%20learning%3A%20A%20systematic%20literature%20review%2C%E2%80%99&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Ashtiani%2C%20M.N.%20Raahemi%2C%20B.%20%E2%80%98News-based%20intelligent%20prediction%20of%20financial%20markets%20using%20text%20mining%20and%20machine%20learning%3A%20A%20systematic%20literature%20review%2C%E2%80%99%202023-05) [Scite](/scite_tallies?query=author%3AAshtiani%2Ctitle%3A%E2%80%98News-based%20intelligent%20prediction%20of%20financial%20markets%20using%20text%20mining%20and%20machine%20learning%3A%20A%20systematic%20literature%20review%2C%E2%80%99%2Cyear%3A2023)

[^33]: M. Shanahan, ‘‘Talking about large language models,’’ 2022, arXiv:2212.03551.  [OA](https://arxiv.org/abs/2212.03551)  

[^34]: J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou, ‘‘Chain-of-thought prompting elicits reasoning in large language models,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 35, 2022, pp. 24824–24837.  [OA](https://scholar.google.co.uk/scholar?q=Wei%2C%20J.%20Wang%2C%20X.%20Schuurmans%2C%20D.%20Bosma%2C%20M.%20%E2%80%98Chain-of-thought%20prompting%20elicits%20reasoning%20in%20large%20language%20models%2C%E2%80%99%202022) [GScholar](https://scholar.google.co.uk/scholar?q=Wei%2C%20J.%20Wang%2C%20X.%20Schuurmans%2C%20D.%20Bosma%2C%20M.%20%E2%80%98Chain-of-thought%20prompting%20elicits%20reasoning%20in%20large%20language%20models%2C%E2%80%99%202022) 

[^35]: R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic, ‘‘Galactica: A large language model for science,’’ 2022, arXiv:2211.09085.  [OA](https://arxiv.org/abs/2211.09085)  

[^36]: J. Hoffmann et al., ‘‘Training compute-optimal large language models,’’ 2022, arXiv:2203.15556.  [OA](https://arxiv.org/abs/2203.15556)  

[^37]: J. Xu Zhao, Y. Xie, K. Kawaguchi, J. He, and M. Q. Xie, ‘‘Automatic model selection with large language models for reasoning,’’ 2023, arXiv:2305.14333.  [OA](https://arxiv.org/abs/2305.14333)  

[^38]: S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, ‘‘Unifying large language models and knowledge graphs: A roadmap,’’ 2023, arXiv:2306.08302.  [OA](https://arxiv.org/abs/2306.08302)  

[^39]: Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, ‘‘ALBERT: A lite BERT for self-supervised learning of language representations,’’ in Proc. 8th Int. Conf. Learn. Represent. (ICLR), 2020, pp. 1–17.  [OA](https://scholar.google.co.uk/scholar?q=Lan%2C%20Z.%20Chen%2C%20M.%20Goodman%2C%20S.%20Gimpel%2C%20K.%20%E2%80%98ALBERT%3A%20A%20lite%20BERT%20for%20self-supervised%20learning%20of%20language%20representations%2C%E2%80%99%202020) [GScholar](https://scholar.google.co.uk/scholar?q=Lan%2C%20Z.%20Chen%2C%20M.%20Goodman%2C%20S.%20Gimpel%2C%20K.%20%E2%80%98ALBERT%3A%20A%20lite%20BERT%20for%20self-supervised%20learning%20of%20language%20representations%2C%E2%80%99%202020) 

[^40]: Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, ‘‘RoBERTa: A robustly optimized BERT pretraining approach,’’ 2019, arXiv:1907.11692.  [OA](https://arxiv.org/abs/1907.11692)  

[^41]: A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 30, 2017, pp. 1–11.  [OA](https://engine.scholarcy.com/oa_version?query=Vaswani%2C%20A.%20Shazeer%2C%20N.%20Parmar%2C%20N.%20Uszkoreit%2C%20J.%20%E2%80%98Attention%20is%20all%20you%20need%2C%E2%80%99%202017&author=Vaswani&title=%E2%80%98Attention%20is%20all%20you%20need%2C%E2%80%99&year=2017) [GScholar](https://scholar.google.co.uk/scholar?q=Vaswani%2C%20A.%20Shazeer%2C%20N.%20Parmar%2C%20N.%20Uszkoreit%2C%20J.%20%E2%80%98Attention%20is%20all%20you%20need%2C%E2%80%99%202017) [Scite](/scite_tallies?query=author%3AVaswani%2Ctitle%3A%E2%80%98Attention%20is%20all%20you%20need%2C%E2%80%99%2Cyear%3A2017)

[^42]: M. Kulakowski and F. Frasincar, ‘‘Sentiment classification of cryptocurrency-related social media posts,’’ IEEE Intell. Syst., vol. 38, no. 4, pp. 5–9, Jul. 2023.  [OA](https://engine.scholarcy.com/oa_version?query=Kulakowski%2C%20M.%20Frasincar%2C%20F.%20%E2%80%98Sentiment%20classification%20of%20cryptocurrency-related%20social%20media%20posts%2C%E2%80%99%202023-07&author=Kulakowski&title=%E2%80%98Sentiment%20classification%20of%20cryptocurrency-related%20social%20media%20posts%2C%E2%80%99&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Kulakowski%2C%20M.%20Frasincar%2C%20F.%20%E2%80%98Sentiment%20classification%20of%20cryptocurrency-related%20social%20media%20posts%2C%E2%80%99%202023-07) [Scite](/scite_tallies?query=author%3AKulakowski%2Ctitle%3A%E2%80%98Sentiment%20classification%20of%20cryptocurrency-related%20social%20media%20posts%2C%E2%80%99%2Cyear%3A2023)

[^43]: Y. Yu, H. Li, Z. Chen, Y. Jiang, Y. Li, D. Zhang, R. Liu, J. W. Suchow, and K. Khashanah, ‘‘FinMem: A performance-enhanced LLM trading agent with layered memory and character design,’’ 2023, arXiv:2311.13743.  [OA](https://arxiv.org/abs/2311.13743)  

[^44]: Y. Li, Y. Yu, H. Li, Z. Chen, and K. Khashanah, ‘‘TradingGPT: Multiagent system with layered memory and distinct characters for enhanced financial trading performance,’’ 2023, arXiv:2309.03736.  [OA](https://arxiv.org/abs/2309.03736)  

[^45]: A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, ‘‘Language models are unsupervised multitask learners,’’ OpenAI Blog, vol. 1, no. 8, p. 9, 2019. [Online]. Available: https://github.com/codelucas/newspaper  [OA](https://github.com/codelucas/newspaper)  [Scite](/scite_tallies?query=author%3ARadford%2Ctitle%3A%E2%80%98Language%20models%20are%20unsupervised%20multitask%20learners%2C%E2%80%99%2Cyear%3A2019)

[^46]: A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, Improving Language Understanding by Generative Pre-training. Accessed: Feb. 3, 2024. [Online]. Available: https://gluebenchmark.com/leaderboard  [OA](https://gluebenchmark.com/leaderboard)  

[^47]: T. B. Brown et al., ‘‘Language models are few-shot learners,’’ in Proc. NIPS, 2020, pp. 1877–1901. [Online]. Available: https://commoncrawl.org/the-data/  [OA](https://commoncrawl.org/the-data/)  

[^48]: J. Achiam et al., ‘‘GPT-4 technical report,’’ 2023, arXiv:2303.08774.  [OA](https://arxiv.org/abs/2303.08774)  

[^49]: G. Fatouros, J. Soldatos, K. Kouroumali, G. Makridis, and D. Kyriazis, ‘‘Transforming sentiment analysis in the financial domain with ChatGPT,’’ Mach. Learn. Appl., vol. 14, Dec. 2023, Art. no. 100508.  [OA](https://engine.scholarcy.com/oa_version?query=Fatouros%2C%20G.%20Soldatos%2C%20J.%20Kouroumali%2C%20K.%20Makridis%2C%20G.%20%E2%80%98Transforming%20sentiment%20analysis%20in%20the%20financial%20domain%20with%20ChatGPT%2C%E2%80%99%202023-12&author=Fatouros&title=%E2%80%98Transforming%20sentiment%20analysis%20in%20the%20financial%20domain%20with%20ChatGPT%2C%E2%80%99&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Fatouros%2C%20G.%20Soldatos%2C%20J.%20Kouroumali%2C%20K.%20Makridis%2C%20G.%20%E2%80%98Transforming%20sentiment%20analysis%20in%20the%20financial%20domain%20with%20ChatGPT%2C%E2%80%99%202023-12) [Scite](/scite_tallies?query=author%3AFatouros%2Ctitle%3A%E2%80%98Transforming%20sentiment%20analysis%20in%20the%20financial%20domain%20with%20ChatGPT%2C%E2%80%99%2Cyear%3A2023)

[^50]: B. Zhang, H. Yang, and X.-Y. Liu, ‘‘Instruct-FinGPT: Financial sentiment analysis by instruction tuning of general-purpose large language models,’’ 2023, arXiv:2306.12659.  [OA](https://arxiv.org/abs/2306.12659)  

[^51]: H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, ‘‘LLaMA: Open and efficient foundation language models,’’ 2023, arXiv:2302.13971.  [OA](https://arxiv.org/abs/2302.13971)  

[^52]: H. Touvron et al., ‘‘Llama 2: Open foundation and fine-tuned chat models,’’ 2023, arXiv:2307.09288.  [OA](https://arxiv.org/abs/2307.09288)  

[^53]: Q. Xie, W. Han, X. Zhang, Y. Lai, M. Peng, A. Lopez-Lira, and J. Huang, ‘‘PIXIU: A large language model, instruction data and evaluation benchmark for finance,’’ 2023, arXiv:2306.05443.  [OA](https://arxiv.org/abs/2306.05443)  

[^54]: B. Peng, E. Chersoni, Y.-Y. Hsu, L. Qiu, and C.-R. Huang, ‘‘Supervised cross-momentum contrast: Aligning representations with prototypical examples to enhance financial sentiment analysis,’’ Knowl.-Based Syst., vol. 295, Jul. 2024, Art. no. 111683.  [OA](https://engine.scholarcy.com/oa_version?query=Peng%2C%20B.%20Chersoni%2C%20E.%20Hsu%2C%20Y.-Y.%20Qiu%2C%20L.%20%E2%80%98Supervised%20cross-momentum%20contrast%3A%20Aligning%20representations%20with%20prototypical%20examples%20to%20enhance%20financial%20sentiment%20analysis%2C%E2%80%99%202024-07&author=Peng&title=%E2%80%98Supervised%20cross-momentum%20contrast%3A%20Aligning%20representations%20with%20prototypical%20examples%20to%20enhance%20financial%20sentiment%20analysis%2C%E2%80%99&year=2024) [GScholar](https://scholar.google.co.uk/scholar?q=Peng%2C%20B.%20Chersoni%2C%20E.%20Hsu%2C%20Y.-Y.%20Qiu%2C%20L.%20%E2%80%98Supervised%20cross-momentum%20contrast%3A%20Aligning%20representations%20with%20prototypical%20examples%20to%20enhance%20financial%20sentiment%20analysis%2C%E2%80%99%202024-07) [Scite](/scite_tallies?query=author%3APeng%2Ctitle%3A%E2%80%98Supervised%20cross-momentum%20contrast%3A%20Aligning%20representations%20with%20prototypical%20examples%20to%20enhance%20financial%20sentiment%20analysis%2C%E2%80%99%2Cyear%3A2024)

[^55]: C. He, C. Li, T. Han, and L. Shen, ‘‘Assessing and enhancing LLMs: A physics and history dataset and one-more-check pipeline method,’’ in Proc. Int. Conf. Neural Inf. Process., 2024, pp. 504–517.  [OA](https://scholar.google.co.uk/scholar?q=He%2C%20C.%20Li%2C%20C.%20Han%2C%20T.%20Shen%2C%20L.%20%E2%80%98Assessing%20and%20enhancing%20LLMs%3A%20A%20physics%20and%20history%20dataset%20and%20one-more-check%20pipeline%20method%2C%E2%80%99%202024) [GScholar](https://scholar.google.co.uk/scholar?q=He%2C%20C.%20Li%2C%20C.%20Han%2C%20T.%20Shen%2C%20L.%20%E2%80%98Assessing%20and%20enhancing%20LLMs%3A%20A%20physics%20and%20history%20dataset%20and%20one-more-check%20pipeline%20method%2C%E2%80%99%202024) 

[^56]: Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. Moussa, M. Beane, T.-H. Huang, B. Routledge, and W. Y. Wang, ‘‘FinQA: A dataset of numerical reasoning over financial data,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., 2021, pp. 3697–3711.  [OA](https://scholar.google.co.uk/scholar?q=Chen%2C%20Z.%20Chen%2C%20W.%20Smiley%2C%20C.%20Shah%2C%20S.%20%E2%80%98FinQA%3A%20A%20dataset%20of%20numerical%20reasoning%20over%20financial%20data%2C%E2%80%99%202021) [GScholar](https://scholar.google.co.uk/scholar?q=Chen%2C%20Z.%20Chen%2C%20W.%20Smiley%2C%20C.%20Shah%2C%20S.%20%E2%80%98FinQA%3A%20A%20dataset%20of%20numerical%20reasoning%20over%20financial%20data%2C%E2%80%99%202021) 

[^57]: Z. Liu, D. Huang, K. Huang, Z. Li, and J. Zhao, ‘‘FinBERT: A pre-trained financial language representation model for financial text mining,’’ in Proc. 29th Int. Joint Conf. Artif. Intell., Jul. 2020, pp. 4513–4519. [Online]. Available: http://commoncrawl.org/  [OA](http://commoncrawl.org/)  

[^58]: P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala, ‘‘Good debt or bad debt: Detecting semantic orientations in economic texts,’’ J. Assoc. Inf. Sci. Technol., vol. 65, no. 4, pp. 782–796, Apr. 2014.  [OA](https://engine.scholarcy.com/oa_version?query=Malo%2C%20P.%20Sinha%2C%20A.%20Korhonen%2C%20P.%20Wallenius%2C%20J.%20%E2%80%98Good%20debt%20or%20bad%20debt%3A%20Detecting%20semantic%20orientations%20in%20economic%20texts%2C%E2%80%99%202014-04&author=Malo&title=%E2%80%98Good%20debt%20or%20bad%20debt%3A%20Detecting%20semantic%20orientations%20in%20economic%20texts%2C%E2%80%99&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Malo%2C%20P.%20Sinha%2C%20A.%20Korhonen%2C%20P.%20Wallenius%2C%20J.%20%E2%80%98Good%20debt%20or%20bad%20debt%3A%20Detecting%20semantic%20orientations%20in%20economic%20texts%2C%E2%80%99%202014-04) [Scite](/scite_tallies?query=author%3AMalo%2Ctitle%3A%E2%80%98Good%20debt%20or%20bad%20debt%3A%20Detecting%20semantic%20orientations%20in%20economic%20texts%2C%E2%80%99%2Cyear%3A2014)

[^59]: K. Cortis, A. Freitas, T. Daudert, M. Huerlimann, M. Zarrouk, S. Handschuh, and B. Davis, ‘‘SemEval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news,’’ in Proc. 11th Int. Workshop Semantic Eval. (SemEval), 2017, pp. 519–535.  [OA](https://scholar.google.co.uk/scholar?q=Cortis%2C%20K.%20Freitas%2C%20A.%20Daudert%2C%20T.%20Huerlimann%2C%20M.%20%E2%80%98SemEval-2017%20task%205%3A%20Fine-grained%20sentiment%20analysis%20on%20financial%20microblogs%20and%20news%2C%E2%80%99%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Cortis%2C%20K.%20Freitas%2C%20A.%20Daudert%2C%20T.%20Huerlimann%2C%20M.%20%E2%80%98SemEval-2017%20task%205%3A%20Fine-grained%20sentiment%20analysis%20on%20financial%20microblogs%20and%20news%2C%E2%80%99%202017) 

[^60]: G. Kim, M. Kim, B. Kim, and H. Lim, ‘‘CBITS: Crypto BERT incorporated trading system,’’ IEEE Access, vol. 11, pp. 6912–6921, 2023.  [OA](https://engine.scholarcy.com/oa_version?query=Kim%2C%20G.%20Kim%2C%20M.%20Kim%2C%20B.%20Lim%2C%20H.%20%E2%80%98CBITS%3A%20Crypto%20BERT%20incorporated%20trading%20system%2C%E2%80%99%202023&author=Kim&title=%E2%80%98CBITS%3A%20Crypto%20BERT%20incorporated%20trading%20system%2C%E2%80%99&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Kim%2C%20G.%20Kim%2C%20M.%20Kim%2C%20B.%20Lim%2C%20H.%20%E2%80%98CBITS%3A%20Crypto%20BERT%20incorporated%20trading%20system%2C%E2%80%99%202023) [Scite](/scite_tallies?query=author%3AKim%2Ctitle%3A%E2%80%98CBITS%3A%20Crypto%20BERT%20incorporated%20trading%20system%2C%E2%80%99%2Cyear%3A2023)

[^61]: Y. Zou and D. Herremans, ‘‘PreBit—A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin,’’ Expert Syst. Appl., vol. 233, Dec. 2023, Art. no. 120838.  [OA](https://engine.scholarcy.com/oa_version?query=Zou%2C%20Y.%20Herremans%2C%20D.%20%E2%80%98PreBit%E2%80%94A%20multimodal%20model%20with%20Twitter%20FinBERT%20embeddings%20for%20extreme%20price%20movement%20prediction%20of%20Bitcoin%2C%E2%80%99%202023-12&author=Zou&title=%E2%80%98PreBit%E2%80%94A%20multimodal%20model%20with%20Twitter%20FinBERT%20embeddings%20for%20extreme%20price%20movement%20prediction%20of%20Bitcoin%2C%E2%80%99&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Zou%2C%20Y.%20Herremans%2C%20D.%20%E2%80%98PreBit%E2%80%94A%20multimodal%20model%20with%20Twitter%20FinBERT%20embeddings%20for%20extreme%20price%20movement%20prediction%20of%20Bitcoin%2C%E2%80%99%202023-12) [Scite](/scite_tallies?query=author%3AZou%2Ctitle%3A%E2%80%98PreBit%E2%80%94A%20multimodal%20model%20with%20Twitter%20FinBERT%20embeddings%20for%20extreme%20price%20movement%20prediction%20of%20Bitcoin%2C%E2%80%99%2Cyear%3A2023)

[^62]: A. Raheman, A. Kolonin, I. Fridkins, I. Ansari, and M. Vishwas, ‘‘Social media sentiment analysis for cryptocurrency market prediction,’’ 2022, arXiv:2204.10185.  [OA](https://arxiv.org/abs/2204.10185)  

[^63]: M. Ortu, N. Uras, C. Conversano, S. Bartolucci, and G. Destefanis, ‘‘On technical trading and social media indicators for cryptocurrency price classification through deep learning,’’ Expert Syst. Appl., vol. 198, Jul. 2022, Art. no. 116804.  [OA](https://engine.scholarcy.com/oa_version?query=Ortu%2C%20M.%20Uras%2C%20N.%20Conversano%2C%20C.%20Bartolucci%2C%20S.%20%E2%80%98On%20technical%20trading%20and%20social%20media%20indicators%20for%20cryptocurrency%20price%20classification%20through%20deep%20learning%2C%E2%80%99%202022-07&author=Ortu&title=%E2%80%98On%20technical%20trading%20and%20social%20media%20indicators%20for%20cryptocurrency%20price%20classification%20through%20deep%20learning%2C%E2%80%99&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Ortu%2C%20M.%20Uras%2C%20N.%20Conversano%2C%20C.%20Bartolucci%2C%20S.%20%E2%80%98On%20technical%20trading%20and%20social%20media%20indicators%20for%20cryptocurrency%20price%20classification%20through%20deep%20learning%2C%E2%80%99%202022-07) [Scite](/scite_tallies?query=author%3AOrtu%2Ctitle%3A%E2%80%98On%20technical%20trading%20and%20social%20media%20indicators%20for%20cryptocurrency%20price%20classification%20through%20deep%20learning%2C%E2%80%99%2Cyear%3A2022)

[^64]: B. Fazlija and P. Harder, ‘‘Using financial news sentiment for stock price direction prediction,’’ Mathematics, vol. 10, no. 13, p. 2156, Jun. 2022. [Online]. Available: https://www.mdpi.com/2227-7390/10/13/2156/htm  [OA](https://www.mdpi.com/2227-7390/10/13/2156/htm)  [Scite](/scite_tallies?query=author%3AFazlija%2Ctitle%3A%E2%80%98Using%20financial%20news%20sentiment%20for%20stock%20price%20direction%20prediction%2C%E2%80%99%2Cyear%3A2022)

[^65]: U. Gupta, ‘‘GPT-InvestAR: Enhancing stock investment strategies through annual report analysis with large language models,’’ 2023, arXiv:2309.03079.  [OA](https://arxiv.org/abs/2309.03079)  

[^66]: X. Deng, V. Bashlovkina, F. Han, S. Baumgartner, and M. Bendersky, ‘‘What do LLMs know about financial markets? A case study on Reddit market sentiment analysis,’’ 2022, arXiv:2212.11311.  [OA](https://arxiv.org/abs/2212.11311)  

[^67]: H. Q. Abonizio, E. C. Paraiso, and S. Barbon, ‘‘Toward text data augmentation for sentiment analysis,’’ IEEE Trans. Artif. Intell., vol. 3, no. 5, pp. 657–668, Oct. 2022.  [OA](https://engine.scholarcy.com/oa_version?query=Abonizio%2C%20H.Q.%20Paraiso%2C%20E.C.%20Barbon%2C%20S.%20%E2%80%98Toward%20text%20data%20augmentation%20for%20sentiment%20analysis%2C%E2%80%99%202022-10&author=Abonizio&title=%E2%80%98Toward%20text%20data%20augmentation%20for%20sentiment%20analysis%2C%E2%80%99&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Abonizio%2C%20H.Q.%20Paraiso%2C%20E.C.%20Barbon%2C%20S.%20%E2%80%98Toward%20text%20data%20augmentation%20for%20sentiment%20analysis%2C%E2%80%99%202022-10) [Scite](/scite_tallies?query=author%3AAbonizio%2Ctitle%3A%E2%80%98Toward%20text%20data%20augmentation%20for%20sentiment%20analysis%2C%E2%80%99%2Cyear%3A2022)

[^68]: D. Ider and S. Lessmann, ‘‘Forecasting cryptocurrency returns from sentiment signals: An analysis of BERT classifiers and weak supervision,’’ 2022, arXiv:2204.05781.  [OA](https://arxiv.org/abs/2204.05781)  

[^69]: J. de Curtò, I. de Zarzà, G. Roig, J. C. Cano, P. Manzoni, and C. T. Calafate, ‘‘LLM-informed multi-armed bandit strategies for nonstationary environments,’’ Electronics, vol. 12, no. 13, p. 2814, Jun. 2023. [Online]. Available: https://www.mdpi.com/2079-9292/12/13/2814/htm  [OA](https://www.mdpi.com/2079-9292/12/13/2814/htm)  [Scite](/scite_tallies?query=author%3ACurt%C3%B2%2Ctitle%3A%E2%80%98LLM-informed%20multi-armed%20bandit%20strategies%20for%20nonstationary%20environments%2C%E2%80%99%2Cyear%3A2023)

[^70]: M. Fernandes, S. Khanna, L. Monteiro, A. Thomas, and G. Tripathi, ‘‘Bitcoin price prediction,’’ in Proc. Int. Conf. Adv. Comput., Commun., Control (ICAC3), Dec. 2021, pp. 1–4.  [OA](https://scholar.google.co.uk/scholar?q=Fernandes%2C%20M.%20Khanna%2C%20S.%20Monteiro%2C%20L.%20Thomas%2C%20A.%20%E2%80%98Bitcoin%20price%20prediction%2C%E2%80%99%202021-12) [GScholar](https://scholar.google.co.uk/scholar?q=Fernandes%2C%20M.%20Khanna%2C%20S.%20Monteiro%2C%20L.%20Thomas%2C%20A.%20%E2%80%98Bitcoin%20price%20prediction%2C%E2%80%99%202021-12) 

[^71]: P. Malo, A. Sinha, P. Takala, P. Korhonen, and J. Wallenius, ‘‘Good debt or bad debt: Detecting semantic orientations in economic texts,’’ 2013, arXiv:1307.5336.  [OA](https://arxiv.org/abs/1307.5336)  

[^72]: S. A. Farimani, M. V. Jahan, A. M. Fard, and S. R. K. Tabbakh, ‘‘Investigating the informativeness of technical indicators and news sentiment in financial market price prediction,’’ Knowl.-Based Syst., vol. 247, Jul. 2022, Art. no. 108742.  [OA](https://engine.scholarcy.com/oa_version?query=Farimani%2C%20S.A.%20Jahan%2C%20M.V.%20Fard%2C%20A.M.%20Tabbakh%2C%20S.R.K.%20%E2%80%98Investigating%20the%20informativeness%20of%20technical%20indicators%20and%20news%20sentiment%20in%20financial%20market%20price%20prediction%2C%E2%80%99%202022-07&author=Farimani&title=%E2%80%98Investigating%20the%20informativeness%20of%20technical%20indicators%20and%20news%20sentiment%20in%20financial%20market%20price%20prediction%2C%E2%80%99&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Farimani%2C%20S.A.%20Jahan%2C%20M.V.%20Fard%2C%20A.M.%20Tabbakh%2C%20S.R.K.%20%E2%80%98Investigating%20the%20informativeness%20of%20technical%20indicators%20and%20news%20sentiment%20in%20financial%20market%20price%20prediction%2C%E2%80%99%202022-07) [Scite](/scite_tallies?query=author%3AFarimani%2Ctitle%3A%E2%80%98Investigating%20the%20informativeness%20of%20technical%20indicators%20and%20news%20sentiment%20in%20financial%20market%20price%20prediction%2C%E2%80%99%2Cyear%3A2022)

[^73]: T. C. Chiang, B. N. Jeon, and H. Li, ‘‘Dynamic correlation analysis of financial contagion: Evidence from Asian markets,’’ J. Int. Money Finance, vol. 26, no. 7, pp. 1206–1228, Nov. 2007.  [OA](https://engine.scholarcy.com/oa_version?query=Chiang%2C%20T.C.%20Jeon%2C%20B.N.%20Li%2C%20H.%20%E2%80%98Dynamic%20correlation%20analysis%20of%20financial%20contagion%3A%20Evidence%20from%20Asian%20markets%2C%E2%80%99%202007-11&author=Chiang&title=%E2%80%98Dynamic%20correlation%20analysis%20of%20financial%20contagion%3A%20Evidence%20from%20Asian%20markets%2C%E2%80%99&year=2007) [GScholar](https://scholar.google.co.uk/scholar?q=Chiang%2C%20T.C.%20Jeon%2C%20B.N.%20Li%2C%20H.%20%E2%80%98Dynamic%20correlation%20analysis%20of%20financial%20contagion%3A%20Evidence%20from%20Asian%20markets%2C%E2%80%99%202007-11) [Scite](/scite_tallies?query=author%3AChiang%2Ctitle%3A%E2%80%98Dynamic%20correlation%20analysis%20of%20financial%20contagion%3A%20Evidence%20from%20Asian%20markets%2C%E2%80%99%2Cyear%3A2007)

[^74]: K. J. Forbes and R. Rigobon, ‘‘No contagion, only interdependence: Measuring stock market comovements,’’ J. Finance, vol. 57, no. 5, pp. 2223–2261, Oct. 2002. [Online]. Available: https://onlinelibrary.wiley.com/doi/full/10.1111/0022-1082.00494  [OA](https://doi.org/10.1111/0022-1082.00494)  [Scite](/scite_tallies?query=https://doi.org/10.1111/0022-1082.00494)

[^75]: L. Barnett, A. B. Barrett, and A. K. Seth, ‘‘Granger causality and transfer entropy are equivalent for Gaussian variables,’’ Phys. Rev. Lett., vol. 103, no. 23, Dec. 2009, Art. no. 238701.  [OA](https://engine.scholarcy.com/oa_version?query=Barnett%2C%20L.%20Barrett%2C%20A.B.%20Seth%2C%20A.K.%20%E2%80%98Granger%20causality%20and%20transfer%20entropy%20are%20equivalent%20for%20Gaussian%20variables%2C%E2%80%99%202009-12&author=Barnett&title=%E2%80%98Granger%20causality%20and%20transfer%20entropy%20are%20equivalent%20for%20Gaussian%20variables%2C%E2%80%99&year=2009) [GScholar](https://scholar.google.co.uk/scholar?q=Barnett%2C%20L.%20Barrett%2C%20A.B.%20Seth%2C%20A.K.%20%E2%80%98Granger%20causality%20and%20transfer%20entropy%20are%20equivalent%20for%20Gaussian%20variables%2C%E2%80%99%202009-12) [Scite](/scite_tallies?query=author%3ABarnett%2Ctitle%3A%E2%80%98Granger%20causality%20and%20transfer%20entropy%20are%20equivalent%20for%20Gaussian%20variables%2C%E2%80%99%2Cyear%3A2009)

[^76]: C. E. Shannon, ‘‘A mathematical theory of communication,’’ Bell Syst. Tech. J., vol. 27, no. 3, pp. 379–423, Jul. 1948.  [OA](https://engine.scholarcy.com/oa_version?query=Shannon%2C%20C.E.%20%E2%80%98A%20mathematical%20theory%20of%20communication%2C%E2%80%99%201948-07&author=Shannon&title=%E2%80%98A%20mathematical%20theory%20of%20communication%2C%E2%80%99&year=1948) [GScholar](https://scholar.google.co.uk/scholar?q=Shannon%2C%20C.E.%20%E2%80%98A%20mathematical%20theory%20of%20communication%2C%E2%80%99%201948-07) [Scite](/scite_tallies?query=author%3AShannon%2Ctitle%3A%E2%80%98A%20mathematical%20theory%20of%20communication%2C%E2%80%99%2Cyear%3A1948)

[^77]: S. Kullback and R. A. Leibler, ‘‘On information and sufficiency,’’ Ann. Math. Statist., vol. 22, no. 1, pp. 79–86, 1951.  [OA](https://engine.scholarcy.com/oa_version?query=Kullback%2C%20S.%20Leibler%2C%20R.A.%20%E2%80%98On%20information%20and%20sufficiency%2C%E2%80%99%201951&author=Kullback&title=%E2%80%98On%20information%20and%20sufficiency%2C%E2%80%99&year=1951) [GScholar](https://scholar.google.co.uk/scholar?q=Kullback%2C%20S.%20Leibler%2C%20R.A.%20%E2%80%98On%20information%20and%20sufficiency%2C%E2%80%99%201951) [Scite](/scite_tallies?query=author%3AKullback%2Ctitle%3A%E2%80%98On%20information%20and%20sufficiency%2C%E2%80%99%2Cyear%3A1951)

[^78]: T. Schreiber, ‘‘Measuring information transfer,’’ Phys. Rev. Lett., vol. 85, no. 2, pp. 461–464, Jul. 2000.  [OA](https://engine.scholarcy.com/oa_version?query=Schreiber%2C%20T.%20%E2%80%98Measuring%20information%20transfer%2C%E2%80%99%202000-07&author=Schreiber&title=%E2%80%98Measuring%20information%20transfer%2C%E2%80%99&year=2000) [GScholar](https://scholar.google.co.uk/scholar?q=Schreiber%2C%20T.%20%E2%80%98Measuring%20information%20transfer%2C%E2%80%99%202000-07) [Scite](/scite_tallies?query=author%3ASchreiber%2Ctitle%3A%E2%80%98Measuring%20information%20transfer%2C%E2%80%99%2Cyear%3A2000)

[^79]: T. Dimpfl and F. J. Peter, ‘‘Using transfer entropy to measure information flows between financial markets,’’ Stud. Nonlinear Dyn. Econometrics, vol. 17, no. 1, pp. 85–102, 2013.  [OA](https://engine.scholarcy.com/oa_version?query=Dimpfl%2C%20T.%20Peter%2C%20F.J.%20%E2%80%98Using%20transfer%20entropy%20to%20measure%20information%20flows%20between%20financial%20markets%2C%E2%80%99%202013&author=Dimpfl&title=%E2%80%98Using%20transfer%20entropy%20to%20measure%20information%20flows%20between%20financial%20markets%2C%E2%80%99&year=2013) [GScholar](https://scholar.google.co.uk/scholar?q=Dimpfl%2C%20T.%20Peter%2C%20F.J.%20%E2%80%98Using%20transfer%20entropy%20to%20measure%20information%20flows%20between%20financial%20markets%2C%E2%80%99%202013) [Scite](/scite_tallies?query=author%3ADimpfl%2Ctitle%3A%E2%80%98Using%20transfer%20entropy%20to%20measure%20information%20flows%20between%20financial%20markets%2C%E2%80%99%2Cyear%3A2013)

[^80]: S. Bekiros, D. K. Nguyen, L. S. Junior, and G. S. Uddin, ‘‘Information diffusion, cluster formation and entropy-based network dynamics in equity and commodity markets,’’ Eur. J. Oper. Res., vol. 256, no. 3, pp. 945–961, Feb. 2017.  [OA](https://engine.scholarcy.com/oa_version?query=Bekiros%2C%20S.%20Nguyen%2C%20D.K.%20Junior%2C%20L.S.%20Uddin%2C%20G.S.%20%E2%80%98Information%20diffusion%2C%20cluster%20formation%20and%20entropy-based%20network%20dynamics%20in%20equity%20and%20commodity%20markets%2C%E2%80%99%202017-02&author=Bekiros&title=%E2%80%98Information%20diffusion%2C%20cluster%20formation%20and%20entropy-based%20network%20dynamics%20in%20equity%20and%20commodity%20markets%2C%E2%80%99&year=2017) [GScholar](https://scholar.google.co.uk/scholar?q=Bekiros%2C%20S.%20Nguyen%2C%20D.K.%20Junior%2C%20L.S.%20Uddin%2C%20G.S.%20%E2%80%98Information%20diffusion%2C%20cluster%20formation%20and%20entropy-based%20network%20dynamics%20in%20equity%20and%20commodity%20markets%2C%E2%80%99%202017-02) [Scite](/scite_tallies?query=author%3ABekiros%2Ctitle%3A%E2%80%98Information%20diffusion%2C%20cluster%20formation%20and%20entropy-based%20network%20dynamics%20in%20equity%20and%20commodity%20markets%2C%E2%80%99%2Cyear%3A2017)

[^81]: T. A. Brown, Confirmatory Factor Analysis for Applied Research. NY, USA: Guilford publications, 2015.  [OA](https://scholar.google.co.uk/scholar?q=Brown%2C%20T.A.%20Confirmatory%20Factor%20Analysis%20for%20Applied%20Research%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Brown%2C%20T.A.%20Confirmatory%20Factor%20Analysis%20for%20Applied%20Research%202015) 

[^82]: P. Wollstadt, M. Martínez-Zarzuela, R. Vicente, F. J. Díaz-Pernas, and M. Wibral, ‘‘Efficient transfer entropy analysis of non-stationary neural time series,’’ PLoS ONE, vol. 9, no. 7, Jul. 2014, Art. no. e102833.  [OA](https://engine.scholarcy.com/oa_version?query=Wollstadt%2C%20P.%20Mart%C3%ADnez-Zarzuela%2C%20M.%20Vicente%2C%20R.%20D%C3%ADaz-Pernas%2C%20F.J.%20%E2%80%98Efficient%20transfer%20entropy%20analysis%20of%20non-stationary%20neural%20time%20series%2C%E2%80%99%202014-07&author=Wollstadt&title=%E2%80%98Efficient%20transfer%20entropy%20analysis%20of%20non-stationary%20neural%20time%20series%2C%E2%80%99&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Wollstadt%2C%20P.%20Mart%C3%ADnez-Zarzuela%2C%20M.%20Vicente%2C%20R.%20D%C3%ADaz-Pernas%2C%20F.J.%20%E2%80%98Efficient%20transfer%20entropy%20analysis%20of%20non-stationary%20neural%20time%20series%2C%E2%80%99%202014-07) [Scite](/scite_tallies?query=author%3AWollstadt%2Ctitle%3A%E2%80%98Efficient%20transfer%20entropy%20analysis%20of%20non-stationary%20neural%20time%20series%2C%E2%80%99%2Cyear%3A2014)

[^83]: H. Zhang, H. Hong, Y. Guo, and C. Yang, ‘‘Information spillover effects from media coverage to the crude oil, gold, and Bitcoin markets during the COVID-19 pandemic: Evidence from the time and frequency domains,’’ Int. Rev. Econ. Finance, vol. 78, pp. 267–285, Mar. 2022.  [OA](https://engine.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Hong%2C%20H.%20Guo%2C%20Y.%20Yang%2C%20C.%20%E2%80%98Information%20spillover%20effects%20from%20media%20coverage%20to%20the%20crude%20oil%2C%20gold%2C%20and%20Bitcoin%20markets%20during%20the%20COVID-19%20pandemic%3A%20Evidence%20from%20the%20time%20and%20frequency%20domains%2C%E2%80%99%202022-03&author=Zhang&title=%E2%80%98Information%20spillover%20effects%20from%20media%20coverage%20to%20the%20crude%20oil%2C%20gold%2C%20and%20Bitcoin%20markets%20during%20the%20COVID-19%20pandemic%3A%20Evidence%20from%20the%20time%20and%20frequency%20domains%2C%E2%80%99&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Hong%2C%20H.%20Guo%2C%20Y.%20Yang%2C%20C.%20%E2%80%98Information%20spillover%20effects%20from%20media%20coverage%20to%20the%20crude%20oil%2C%20gold%2C%20and%20Bitcoin%20markets%20during%20the%20COVID-19%20pandemic%3A%20Evidence%20from%20the%20time%20and%20frequency%20domains%2C%E2%80%99%202022-03) [Scite](/scite_tallies?query=author%3AZhang%2Ctitle%3A%E2%80%98Information%20spillover%20effects%20from%20media%20coverage%20to%20the%20crude%20oil%2C%20gold%2C%20and%20Bitcoin%20markets%20during%20the%20COVID-19%20pandemic%3A%20Evidence%20from%20the%20time%20and%20frequency%20domains%2C%E2%80%99%2Cyear%3A2022)

[^84]: S. Moss, ‘‘Google brain unveils trillion-parameter AI language model, the largest yet,’’ Tech. Rep., 2021.  [OA](https://engine.scholarcy.com/oa_version?query=Moss%2C%20S.%20%E2%80%98Google%20brain%20unveils%20trillion-parameter%20AI%20language%20model%2C%20the%20largest%20yet%2C%E2%80%99%202021&author=Moss&title=%E2%80%98Google%20brain%20unveils%20trillion-parameter%20AI%20language%20model%2C%20the%20largest%20yet%2C%E2%80%99&year=2021) [GScholar](https://scholar.google.co.uk/scholar?q=Moss%2C%20S.%20%E2%80%98Google%20brain%20unveils%20trillion-parameter%20AI%20language%20model%2C%20the%20largest%20yet%2C%E2%80%99%202021) [Scite](/scite_tallies?query=author%3AMoss%2Ctitle%3A%E2%80%98Google%20brain%20unveils%20trillion-parameter%20AI%20language%20model%2C%20the%20largest%20yet%2C%E2%80%99%2Cyear%3A2021)

[^85]: S. Bekman, ‘‘The technology behind Bloom training,’’ Tech. Rep., 2022.  [OA](https://engine.scholarcy.com/oa_version?query=Bekman%2C%20S.%20%E2%80%98The%20technology%20behind%20Bloom%20training%2C%E2%80%99%202022&author=Bekman&title=%E2%80%98The%20technology%20behind%20Bloom%20training%2C%E2%80%99&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Bekman%2C%20S.%20%E2%80%98The%20technology%20behind%20Bloom%20training%2C%E2%80%99%202022) [Scite](/scite_tallies?query=author%3ABekman%2Ctitle%3A%E2%80%98The%20technology%20behind%20Bloom%20training%2C%E2%80%99%2Cyear%3A2022)

[^86]: T. L. Scao et al., ‘‘BLOOM: A 176B-parameter open-access multilingual language model,’’ 2022, arXiv:2211.05100.  [OA](https://arxiv.org/abs/2211.05100)  

[^87]: S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach, ‘‘GPT-NeoX-20B: An open-source autoregressive language model,’’ 2022, arXiv:2204.06745.  [OA](https://arxiv.org/abs/2204.06745)  

[^88]: L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy, ‘‘The Pile: An 800 GB dataset of diverse text for language modeling,’’ 2021, arXiv:2101.00027. M. C. Rillig, M. Ågerstrand, M. Bi, K. A. Gould, and U. Sauerland, ‘‘Risks and benefits of large language models for the environment,’’ Environ. Sci. Technol., vol. 57, no. 9, pp. 3464–3466, Mar. 2023. [Online]. Available: https://pubs.acs.org/doi/full/10.1021/acs.est.3c01106 W.  [OA](https://arxiv.org/abs/2101.00027)  [Scite](/scite_tallies?query=https://doi.org/10.1021/acs.est.3c01106)

[^89]: Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, ‘‘MiniLM: Deep self-attention distillation for task-agnostic compression of pretrained transformers,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 2020, 2020, pp. 5776–5788.  [OA](https://engine.scholarcy.com/oa_version?query=Wang%2C%20F.Wei%20Dong%2C%20L.%20Bao%2C%20H.%20Yang%2C%20N.%20%E2%80%98MiniLM%3A%20Deep%20self-attention%20distillation%20for%20task-agnostic%20compression%20of%20pretrained%20transformers%2C%E2%80%99%202020&author=Wang&title=%E2%80%98MiniLM%3A%20Deep%20self-attention%20distillation%20for%20task-agnostic%20compression%20of%20pretrained%20transformers%2C%E2%80%99&year=2020) [GScholar](https://scholar.google.co.uk/scholar?q=Wang%2C%20F.Wei%20Dong%2C%20L.%20Bao%2C%20H.%20Yang%2C%20N.%20%E2%80%98MiniLM%3A%20Deep%20self-attention%20distillation%20for%20task-agnostic%20compression%20of%20pretrained%20transformers%2C%E2%80%99%202020) [Scite](/scite_tallies?query=author%3AWang%2Ctitle%3A%E2%80%98MiniLM%3A%20Deep%20self-attention%20distillation%20for%20task-agnostic%20compression%20of%20pretrained%20transformers%2C%E2%80%99%2Cyear%3A2020)

[^90]: A. Albalak, A. Shrivastava, C. Sankar, A. Sagar, and M. Ross, ‘‘Dataefficiency with a single GPU: An exploration of transfer methods for small language models,’’ 2022, arXiv:2210.03871. X. Deng, V. Bashlovkina, F. Han, S. Baumgartner, and M. Bendersky, ‘‘What do LLMs Know about financial markets? A case study on Reddit market sentiment analysis,’’ in Proc. ACM Web Conf., 2022, pp. 107–110. [Online]. Available: https://dl.acm.org/doi/10.1145/3543873.3587324 S.  [OA](https://arxiv.org/abs/2210.03871)  [Scite](/scite_tallies?query=https://doi.org/10.1145/3543873.3587324)

[^91]: Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, ‘‘ReAct: Synergizing reasoning and acting in language models,’’ 2022, arXiv:2210.03629. X. Li, H. Xiong, X. Li, X. Wu, X. Zhang, J. Liu, J. Bian, and D. Dou, ‘‘Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond,’’ Knowl. Inf. Syst., vol. 64, no. 12, pp. 3197–3234, Dec. 2022. [Online]. Available: https://link.springer.com/article/10.1007/s10115-022-01756-8 S.  [OA](https://arxiv.org/abs/2210.03629)  [Scite](/scite_tallies?query=https://doi.org/10.1007/s10115-022-01756-8)

[^92]: Sinha, H. Chen, A. Sekhon, Y. Ji, and Y. Qi, ‘‘Perturbing inputs for fragile interpretations in deep natural language processing,’’ in Proc. 4th BlackboxNLP Workshop Analyzing Interpreting Neural Netw. NLP, 2021, pp. 420–434. [Online]. Available: https://aclanthology.org/2021.blackboxnlp-1.33 M. T. R.  [OA](https://aclanthology.org/2021.blackboxnlp-1.33)  

[^93]: Laskar, M. S. Bari, M. Rahman, M. A. H. Bhuiyan, S. Joty, and J. X. Huang, ‘‘A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets,’’ in Proc. Annu. Meeting Assoc. Comput. Linguistics, 2023, pp. 431–469.  [OA](https://scholar.google.co.uk/scholar?q=Laskar%2C%20M.S.Bari%20Rahman%2C%20M.%20Bhuiyan%2C%20M.A.H.%20Joty%2C%20S.%20%E2%80%98A%20systematic%20study%20and%20comprehensive%20evaluation%20of%20ChatGPT%20on%20benchmark%20datasets%2C%E2%80%99%202023) [GScholar](https://scholar.google.co.uk/scholar?q=Laskar%2C%20M.S.Bari%20Rahman%2C%20M.%20Bhuiyan%2C%20M.A.H.%20Joty%2C%20S.%20%E2%80%98A%20systematic%20study%20and%20comprehensive%20evaluation%20of%20ChatGPT%20on%20benchmark%20datasets%2C%E2%80%99%202023) 

[^94]: M. U. Haque, I. Dharmadasa, Z. T. Sworna, R. N. Rajapakse, and H. Ahmad, ‘‘‘I think this is the most disruptive technology’: Exploring sentiments of ChatGPT early adopters using Twitter data,’’ 2022, arXiv:2212.05856. I. Gur, O. Nachum, Y. Miao, M. Safdari, A. Huang, A. Chowdhery, S. Narang, N. Fiedel, and A. Faust, ‘‘Understanding HTML with large language models,’’ 2022, arXiv:2210.03945. X. Wang, J. He, Z. Jin, M. Yang, Y. Wang, and H. Qu, ‘‘M2Lens: Visualizing and explaining multimodal models for sentiment analysis,’’ IEEE Trans. Vis. Comput. Graphics, vol. 28, no. 1, pp. 802–812, Jan. 2022.  [OA](https://arxiv.org/abs/2212.05856)  

[^95]: H. Song, J. Li, Z. Xia, Z. Yang, and X. Du, ‘‘Multimodal sentiment analysis based on pre-LN transformer interaction,’’ in Proc. IEEE 6th Inf. Technol. Mechatronics Eng. Conf. (ITOEC), vol. 6, Mar. 2022, pp. 1609–1613.  [OA](https://scholar.google.co.uk/scholar?q=Song%2C%20H.%20Li%2C%20J.%20Xia%2C%20Z.%20Yang%2C%20Z.%20%E2%80%98Multimodal%20sentiment%20analysis%20based%20on%20pre-LN%20transformer%20interaction%2C%E2%80%99%202022-03) [GScholar](https://scholar.google.co.uk/scholar?q=Song%2C%20H.%20Li%2C%20J.%20Xia%2C%20Z.%20Yang%2C%20Z.%20%E2%80%98Multimodal%20sentiment%20analysis%20based%20on%20pre-LN%20transformer%20interaction%2C%E2%80%99%202022-03) 

[^96]: K. Dashtipour, M. Gogate, E. Cambria, and A. Hussain, ‘‘A novel context-aware multimodal framework for Persian sentiment analysis,’’ Neurocomputing, vol. 457, pp. 377–388, Oct. 2021.  [OA](https://engine.scholarcy.com/oa_version?query=Dashtipour%2C%20K.%20Gogate%2C%20M.%20Cambria%2C%20E.%20Hussain%2C%20A.%20%E2%80%98A%20novel%20context-aware%20multimodal%20framework%20for%20Persian%20sentiment%20analysis%2C%E2%80%99%202021-10&author=Dashtipour&title=%E2%80%98A%20novel%20context-aware%20multimodal%20framework%20for%20Persian%20sentiment%20analysis%2C%E2%80%99&year=2021) [GScholar](https://scholar.google.co.uk/scholar?q=Dashtipour%2C%20K.%20Gogate%2C%20M.%20Cambria%2C%20E.%20Hussain%2C%20A.%20%E2%80%98A%20novel%20context-aware%20multimodal%20framework%20for%20Persian%20sentiment%20analysis%2C%E2%80%99%202021-10) [Scite](/scite_tallies?query=author%3ADashtipour%2Ctitle%3A%E2%80%98A%20novel%20context-aware%20multimodal%20framework%20for%20Persian%20sentiment%20analysis%2C%E2%80%99%2Cyear%3A2021)

[^97]: U. Sehar, S. Kanwal, K. Dashtipur, U. Mir, U. Abbasi, and F. Khan, ‘‘Urdu sentiment analysis via multimodal data mining based on deep learning algorithms,’’ IEEE Access, vol. 9, pp. 153072–153082, 2021.  [OA](https://engine.scholarcy.com/oa_version?query=Sehar%2C%20U.%20Kanwal%2C%20S.%20Dashtipur%2C%20K.%20Mir%2C%20U.%20%E2%80%98Urdu%20sentiment%20analysis%20via%20multimodal%20data%20mining%20based%20on%20deep%20learning%20algorithms%2C%E2%80%99%202021&author=Sehar&title=%E2%80%98Urdu%20sentiment%20analysis%20via%20multimodal%20data%20mining%20based%20on%20deep%20learning%20algorithms%2C%E2%80%99&year=2021) [GScholar](https://scholar.google.co.uk/scholar?q=Sehar%2C%20U.%20Kanwal%2C%20S.%20Dashtipur%2C%20K.%20Mir%2C%20U.%20%E2%80%98Urdu%20sentiment%20analysis%20via%20multimodal%20data%20mining%20based%20on%20deep%20learning%20algorithms%2C%E2%80%99%202021) [Scite](/scite_tallies?query=author%3ASehar%2Ctitle%3A%E2%80%98Urdu%20sentiment%20analysis%20via%20multimodal%20data%20mining%20based%20on%20deep%20learning%20algorithms%2C%E2%80%99%2Cyear%3A2021)

[^98]: A. Oussous, F.-Z. Benjelloun, A. A. Lahcen, and S. Belfkih, ‘‘ASA: A framework for Arabic sentiment analysis,’’ J. Inf. Sci., vol. 46, no. 4, pp. 544–559, Aug. 2020. [Online]. Available: https://journals.sagepub.com/doi/10.1177/0165551519849516 Z.  [OA](https://doi.org/10.1177/0165551519849516)  [Scite](/scite_tallies?query=https://doi.org/10.1177/0165551519849516)

[^99]: Ke, J. Sheng, Z. Li, W. Silamu, and Q. Guo, ‘‘Knowledge-guided sentiment analysis via learning from natural language explanations,’’ IEEE Access, vol. 9, pp. 3570–3578, 2021.  [OA](https://engine.scholarcy.com/oa_version?query=Ke%2C%20J.Sheng%20Li%2C%20Z.%20Silamu%2C%20W.%20Guo%2C%20Q.%20%E2%80%98Knowledge-guided%20sentiment%20analysis%20via%20learning%20from%20natural%20language%20explanations%2C%E2%80%99%202021&author=Ke&title=%E2%80%98Knowledge-guided%20sentiment%20analysis%20via%20learning%20from%20natural%20language%20explanations%2C%E2%80%99&year=2021) [GScholar](https://scholar.google.co.uk/scholar?q=Ke%2C%20J.Sheng%20Li%2C%20Z.%20Silamu%2C%20W.%20Guo%2C%20Q.%20%E2%80%98Knowledge-guided%20sentiment%20analysis%20via%20learning%20from%20natural%20language%20explanations%2C%E2%80%99%202021) [Scite](/scite_tallies?query=author%3AKe%2Ctitle%3A%E2%80%98Knowledge-guided%20sentiment%20analysis%20via%20learning%20from%20natural%20language%20explanations%2C%E2%80%99%2Cyear%3A2021)

[^100]: Q. Zhang, J. Zhou, Q. Chen, Q. Bai, J. Xiao, and L. He, ‘‘A knowledge-enhanced adversarial model for cross-lingual structured sentiment analysis,’’ in Proc. Int. Joint Conf. Neural Netw., Jul. 2022, pp. 1–8. G. F. N. Mvondo, B. Niu, and S. Eivazinezhad, ‘‘Generative conversational AI and academic integrity: A mixed method investigation to understand the ethical use of LLM chatbots in higher education,’’ SSRN Electron. J., 2023. [Online]. Available: https://ssrn.com/abstract=4548263 and http://dx.doi.org/10.2139/ssrn.4548263  [OA](https://doi.org/10.2139/ssrn.4548263)  [Scite](/scite_tallies?query=https://doi.org/10.2139/ssrn.4548263)

[^1]: CHENGHAO LIU received the B.Sc. degree in software engineering from Jiangxi University of Finance and Economics, in 2020. He is currently pursuing the master’s degree with The University of Auckland. His research interests include machine learning and large language model to solve financial problems.  [OA](https://scholar.google.co.uk/scholar?q=CHENGHAO%20LIU%20received%20the%20BSc%20degree%20in%20software%20engineering%20from%20Jiangxi%20University%20of%20Finance%20and%20Economics%20in%202020%20He%20is%20currently%20pursuing%20the%20masters%20degree%20with%20The%20University%20of%20Auckland%20His%20research%20interests%20include%20machine%20learning%20and%20large%20language%20model%20to%20solve%20financial%20problems) [GScholar](https://scholar.google.co.uk/scholar?q=CHENGHAO%20LIU%20received%20the%20BSc%20degree%20in%20software%20engineering%20from%20Jiangxi%20University%20of%20Finance%20and%20Economics%20in%202020%20He%20is%20currently%20pursuing%20the%20masters%20degree%20with%20The%20University%20of%20Auckland%20His%20research%20interests%20include%20machine%20learning%20and%20large%20language%20model%20to%20solve%20financial%20problems) 

[^2]: ARUNKUMAR ARULAPPAN (Member, IEEE) received the B.Tech. degree in information technology from Anna University, Chennai, India, the M.Tech. degree in computer science and engineering from Vellore Institute of Technology (VIT), Vellore, India, and the Ph.D. degree from the Faculty of Information and Communication Engineering, Anna University, in 2023. He is an Assistant Professor with the School of Computer Science Engineering and Information Systems (SCORE), VIT University. He is proficient with simulator tools MATLAB, ns-3, Mininet, OpenNet VM, and P4 programming. He is exposed to open source tools, such as OpenStack, Cloudify, OPNFV, and Cloud-Native Computing Foundation (CNCF). His research interests include the cloudnative deployment, SDN, NFV, 5G/6G networks, AI/ML based networking, the Internet of Vehicles, and UAV communications.  [OA](https://scholar.google.co.uk/scholar?q=ARUNKUMAR%20ARULAPPAN%20Member%20IEEE%20received%20the%20BTech%20degree%20in%20information%20technology%20from%20Anna%20University%20Chennai%20India%20the%20MTech%20degree%20in%20computer%20science%20and%20engineering%20from%20Vellore%20Institute%20of%20Technology%20VIT%20Vellore%20India%20and%20the%20PhD%20degree%20from%20the%20Faculty%20of%20Information%20and%20Communication%20Engineering%20Anna%20University%20in%202023%20He%20is%20an%20Assistant%20Professor%20with%20the%20School%20of%20Computer%20Science%20Engineering%20and%20Information%20Systems%20SCORE%20VIT%20University%20He%20is%20proficient%20with%20simulator%20tools%20MATLAB%20ns3%20Mininet%20OpenNet%20VM%20and%20P4%20programming%20He%20is%20exposed%20to%20open%20source%20tools%20such%20as%20OpenStack%20Cloudify%20OPNFV%20and%20CloudNative%20Computing%20Foundation%20CNCF%20His%20research%20interests%20include%20the%20cloudnative%20deployment%20SDN%20NFV%205G6G%20networks%20AIML%20based%20networking%20the%20Internet%20of%20Vehicles%20and%20UAV%20communications) [GScholar](https://scholar.google.co.uk/scholar?q=ARUNKUMAR%20ARULAPPAN%20Member%20IEEE%20received%20the%20BTech%20degree%20in%20information%20technology%20from%20Anna%20University%20Chennai%20India%20the%20MTech%20degree%20in%20computer%20science%20and%20engineering%20from%20Vellore%20Institute%20of%20Technology%20VIT%20Vellore%20India%20and%20the%20PhD%20degree%20from%20the%20Faculty%20of%20Information%20and%20Communication%20Engineering%20Anna%20University%20in%202023%20He%20is%20an%20Assistant%20Professor%20with%20the%20School%20of%20Computer%20Science%20Engineering%20and%20Information%20Systems%20SCORE%20VIT%20University%20He%20is%20proficient%20with%20simulator%20tools%20MATLAB%20ns3%20Mininet%20OpenNet%20VM%20and%20P4%20programming%20He%20is%20exposed%20to%20open%20source%20tools%20such%20as%20OpenStack%20Cloudify%20OPNFV%20and%20CloudNative%20Computing%20Foundation%20CNCF%20His%20research%20interests%20include%20the%20cloudnative%20deployment%20SDN%20NFV%205G6G%20networks%20AIML%20based%20networking%20the%20Internet%20of%20Vehicles%20and%20UAV%20communications) 

[^3]: IN-HO RA (Member, IEEE) received the Ph.D. degree in computer engineering from Chung-Ang University, Seoul, South Korea, in 1995. From February 2007 to August 2008, he was a Visiting Scholar with the University of South Florida, Tampa, FL, USA. He has been with the School of Computer, Information and Communication Engineering, Kunsan National University, where he is currently a Professor. His research interests include wireless ad hoc and sensor networks, blockchain, the IoT, PS-LTE, and microgrids.  [OA](https://scholar.google.co.uk/scholar?q=INHO%20RA%20Member%20IEEE%20received%20the%20PhD%20degree%20in%20computer%20engineering%20from%20ChungAng%20University%20Seoul%20South%20Korea%20in%201995%20From%20February%202007%20to%20August%202008%20he%20was%20a%20Visiting%20Scholar%20with%20the%20University%20of%20South%20Florida%20Tampa%20FL%20USA%20He%20has%20been%20with%20the%20School%20of%20Computer%20Information%20and%20Communication%20Engineering%20Kunsan%20National%20University%20where%20he%20is%20currently%20a%20Professor%20His%20research%20interests%20include%20wireless%20ad%20hoc%20and%20sensor%20networks%20blockchain%20the%20IoT%20PSLTE%20and%20microgrids) [GScholar](https://scholar.google.co.uk/scholar?q=INHO%20RA%20Member%20IEEE%20received%20the%20PhD%20degree%20in%20computer%20engineering%20from%20ChungAng%20University%20Seoul%20South%20Korea%20in%201995%20From%20February%202007%20to%20August%202008%20he%20was%20a%20Visiting%20Scholar%20with%20the%20University%20of%20South%20Florida%20Tampa%20FL%20USA%20He%20has%20been%20with%20the%20School%20of%20Computer%20Information%20and%20Communication%20Engineering%20Kunsan%20National%20University%20where%20he%20is%20currently%20a%20Professor%20His%20research%20interests%20include%20wireless%20ad%20hoc%20and%20sensor%20networks%20blockchain%20the%20IoT%20PSLTE%20and%20microgrids) 

