[[Mishev_et+al_EvaluationSentimentAnalysisFinanceFrom_2020]]

# [Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers](https://doi.org/10.1109/access.2020.3009626)

## [[Kostadin Mishev]]; [[Ana Gjorgjevikj]]; [[Irena Vodenska]] et al.

## Abstract
Financial and economic news is continuously monitored by financial market participants. According to the efficient market hypothesis, all past information is reflected in stock prices and new information is instantaneously absorbed in determining future stock prices. Hence, prompt extraction of positive or negative sentiments from news is very important for investment decision-making by traders, portfolio managers and investors. Sentiment analysis models can provide an efficient method for extracting actionable signals from the news. However, financial sentiment analysis is challenging due to domain-specific language and unavailability of large labeled datasets. General sentiment analysis models are ineffective when applied to specific domains such as finance. To overcome these challenges, we design an evaluation platform which we use to assess the effectiveness and performance of various sentiment analysis approaches, based on combinations of text representation methods and machine-learning classifiers. We perform more than one hundred experiments using publicly available datasets, labeled by financial experts. We start the evaluation with specific lexicons for sentiment analysis in finance and gradually build the study to include word and sentence encoders, up to the latest available NLP transformers. ==The results show improved efficiency of contextual embeddings in sentiment analysis compared to lexicons and fixed word and sentence encoders, even when large datasets are not available==. Furthermore, distilled versions of NLP transformers produce comparable results to their larger teacher models, which makes them suitable for use in production environments.

## Key concepts
#efficient_market_hypothesis; #lexicon; #text_classification; #natural_language_processing; #deep_learning; #transformers; #claim/language_model; #language_model; #sentiment_analysis; #finding/BERT; #BERT; #machine_learning

## Quote
> This paper presents a comprehensive study of natural language processing (NLP) methods for sentiment analysis in finance, evaluating the performance of various text representation methods and machine learning classifiers.

## Key points
- The latest advances in Natural Language Processing (NLP) have received significant attention due to their efficiency in language modeling
- The main contribution of this paper is the development of an evaluation platform, which we use to assess the performance of NLP methodologies for text feature extraction in finance
- We show that recent advances in deep-learning and transfer-learning methods in NLP increase the accuracy of sentiment analysis based on financial headlines
- The study begins with the lexicon-based approach, includes word and sentence encoders and concludes with recent NLP transformers
- The main progress in sentiment analysis accuracy is driven by the text representation methods, which feed the semantic meaning of the words and sentences into the models
- This approach was constructed for sentiment analysis in the finance domain, it can be extended to other areas such as healthcare, legal and business analytics


## Summary

### Sentiment Analysis
Sentiment analysis is a crucial tool for extracting actionable signals from financial news, allowing traders, portfolio managers, and investors to make informed decisions.
The efficient market hypothesis states that all past information is reflected in stock prices, and new information is instantaneously absorbed in determining future stock prices.
However, financial sentiment analysis is challenging due to domain-specific language and the unavailability of large labeled datasets.
General sentiment analysis models are ineffective when applied to specific domains such as finance.

### Text Representation
Text representation methods, including lexicon-based and statistical approaches, are used to extract features from financial texts.
Lexicon-based methods rely on domain-specific knowledge represented as a lexicon or dictionary, such as the Loughran-McDonald lexicon and the Harvard IV-4 dictionary.
Statistical methods, including Count Vectorizer (CV) and TF-IDF, convert text documents into matrices of token counts or weighted term frequencies.
However, these methods have limitations, such as losing ordering information and contextual information.

### Deep Learning
Deep learning methods, including word and sentence encoders, and NLP transformers, have significantly improved sentiment extraction from financial news and texts.
Recent advances in deep learning and transfer learning methods have increased the accuracy of sentiment analysis based on financial headlines.
The use of distilled versions of NLP transformers, such as BERT and RoBERTa, has produced comparable results to their larger teacher models, making them suitable for use in production environments.

### Word Encoders
Word encoders convert discrete words into high-dimensional vectors, providing semantic knowledge.
Popular word encoders include Word2Vec, GloVe, and FastText.
Word2Vec uses Continuous Bag-of-Words and Continuous Skip-gram Model architectures, while GloVe emphasizes co-occurrence probabilities between words.
FastText builds word embeddings at a deeper level by harnessing sub-words and characters, enabling training on smaller datasets and generalization to unknown words.

### Sentence Encoders
Sentence encoders learn fixed-length feature vectors that encode syntax and semantic properties of variable-length sentences.
Recent sentence encoders include Doc2Vec, Skip-Thought Vectors, InferSent, Universal Sentence Encoder (USE), and Language-Agnostic Sentence Representations (LASER).
Doc2Vec represents variable-length fragments of texts as fixed-size dense vectors, while Skip-Thought Vectors use encoder-decoder architecture for sequence modeling.
InferSent learns sentence embeddings using natural language inference data, and USE converts variable-length sentences into 512-dimensional vectors using transfer-learning.

### Evaluation
The performance of these word and sentence encoders is evaluated in various NLP tasks, including sentiment analysis and text classification.
Pre-trained models, such as Word2Vec, GloVe, and FastText, are used to assess their performance on financial texts.
The evaluation of sentence encoders, including Doc2Vec, Skip-Thought Vectors, InferSent, USE, and LASER, is also conducted to assess their ability to extract important features in sentence representation of financial headlines.
The study evaluates the models using various metrics, including Matthews Correlation Coefficient (MCC), which is widely used for assessing binary classification performance.
The results show that the NLP transformers outperform the other models, with BART achieving the best results in sentiment analysis tasks.
The study also highlights the importance of using domain-specific dictionaries for feature extraction in sentiment analysis tasks.

### NLP Models
The study evaluates LASER on English texts, but the same model can be used for sentiment analysis in 92 other languages supported by LASER.
NLP transformers, such as BERT, XLNet, and RoBERTa, have shown state-of-the-art performance in various NLP tasks, including text classification and sentiment analysis.
These models use a transformer architecture, which transforms one sequence into another using encoder and decoder models.

### Transformer Architecture
The transformer architecture is based on multi-headed self-attention mechanisms, which allow for parallelization and learning long-range dependencies between words in a sequence.
The architecture consists of encoder and decoder modules, with multi-head attention and feed-forward layers as the main building blocks.
The scaled dot-product attention mechanism is used to calculate attention weights, which represent the influence of each word in the sequence on other words.

### Language Representation
Language representation models, such as BERT, XLNet, and XLM, use pre-training objectives to learn contextualized embeddings of words in a sentence.
These models overcome the limitations of traditional word embeddings, such as Word2Vec and GloVe, which use fixed embeddings for each word.
The pre-trained models can be fine-tuned for specific tasks, such as sentiment analysis, to achieve better results.
Other models, such as ALBERT, RoBERTa, and DistilBERT, offer optimized versions of BERT, with reduced parameters, improved training methodology, and better performance in various NLP tasks.

### Models
The study evaluates the performance of various models for sentiment analysis in finance, including lexicon-based models, word encoders, sentence encoders, and NLP transformers such as BERT, RoBERTa, XLM-RoBERTa, and BART.
The models are fine-tuned and compared using different machine-learning and deep-learning classifiers.

### Datasets
The study uses two publicly available datasets: the Financial Phrase-Bank dataset and the SemEval 2017 dataset.
The datasets are pre-processed and balanced to address the imbalance between positive and negative sentences.
The pre-processing steps include tokenization, stop-word removal, stemming, and named entity recognition.

### Methods
The study evaluates various NLP-based methods for sentiment analysis in finance, including lexicon-based approaches, word and sentence encoders, and recent NLP transformers.
The methods are compared based on their performance, with the NLP transformers showing superior results.
The study also investigates the use of different word embeddings, such as GloVe and FastText, and the impact of attention layers and bidirectional context on the results.

### Results
The results show that the NLP transformers, particularly BART and ALBERT-xxlarge, achieve the best performance with MCC scores of 0.895 and 0.881, respectively.
The study also finds that contextualized embeddings, such as ELMo and BERT, outperform non-contextualized embeddings.
The results are presented in various tables, including Tables 4-8, which provide a detailed comparison of the different methods and their performance.

### Applications
The study highlights the potential applications of the proposed approach in finance, including forecasting stock market trends and corporate earnings, decision-making in securities trading and portfolio management, brand reputation management, and fraud detection and regulation.
The findings also suggest that the approach can be extended to other areas, such as healthcare, legal, and business analytics, where sentiment analysis can provide valuable insights.


## Study subjects

### 1748 samples
- Additionally, we shuffle the datasets and we set aside stratified 80% of all sentences as a training and stratified 20% of the remaining sentences as a validation set. ==At the end, our balanced training set includes 1748 samples, and a balanced validation set consisting of 438 samples==. C

## Data analysis
- #method/roberta_model
- #method/bert_algorithm
- #method/bigru_method
- #method/correlation_coefficient

## Findings
- Compared to other pre-trained versions of <a class="keyword" href="https://en.wikipedia.org/wiki/BERT_(language_model)" title="BERT">BERT</a>, FinBERT model has achieved a 15% improvement in accuracy in <a class="keyword" href="https://en.wikipedia.org/wiki/Text_classification" title="text classification">text classification</a> tasks specifically applied to financial texts
- <mark class="fact">DistilBERT retains more than 95% of the accuracy</mark> while having 40% fewer parameters

##  Builds on previous research
- The results of this study can be applied in areas such as finance, where decision-making is based on sentiment extraction from massive textual datasets. ==The find==ings imply that selected models can be successfully used for forecasting stock market trends and corporate earnings, decision-making in securities trading and portfolio management, brand reputation management as well as fraud detection and regulation [^87]–[^89].

## Differs from previous work
- If the model has not encountered a word before, it will be unable to interpret it or build a vector for it. ==Additionally, Word2Vec does not support shared representations at sub-word level, which means that it will create two completely different vector representations for words which are morphologically similar, like agree/agreement or worth/worthwhile== [^29].
- Recent research studies have proposed methods that produce ==different embeddings for the same word, taking into consideration specific contexts [^3], [^55], [^58]. As an illustration of context importance, we analyze the following two sentences that contain the word ‘‘Apple’’==: ‘‘Apple Inc performed well this year.’’ and ‘‘Apple fruits are exported to various countries.’’ In the first sentence, Apple refers to the technology company Apple, headquartered in the US, while in the second sentence, apple refers to the fruit, with a completely different meaning.

## Contributions
- This paper presents a comprehensive chronological study of NLP-based methods for sentiment analysis in finance. <mark class="fact">The study begins with the lexicon-based approach</mark>, includes word and sentence encoders and concludes with recent NLP transformers. <mark class="fact">The NLP transformers show superior performances compared to the other evaluated approaches</mark>. <mark class="fact">The main progress in sentiment analysis accuracy is driven by the text representation methods</mark>, <mark class="fact">which feed the semantic meaning of the words and sentences into the models</mark>. <mark class="fact">The results achieved by the best models are comparable to expert</mark>’s opinion. The evaluations were performed on a relatively small dataset of approximately 2000 sentences. Even though the dataset is not large, we obtained good results, suggesting <mark class="fact">that this approach is appropriate for domains where large annotated data is</mark> not available.

## Limitations
- The study is limited by the relatively small dataset of approximately 2000 sentences. However, the study suggests that this approach is appropriate for domains where large annotated data is not available.

## Future work
- The study suggests that the approach can be extended to other areas such as healthcare, legal, and business analytics. The study also suggests that the results can be applied in areas such as finance, where decision-making is based on sentiment extraction from massive textual datasets.


## References
[^1]: A. Yenter and A. Verma, ‘‘Deep CNN-LSTM with combined kernels from multiple branches for IMDb review sentiment analysis,’’ in Proc. IEEE 8th Annu. Ubiquitous Comput., Electron. Mobile Commun. Conf. (UEMCON), Oct. 2017, pp. 540–546.  [OA](https://scholar.google.co.uk/scholar?q=Yenter%2C%20A.%20Verma%2C%20A.%20%E2%80%98Deep%20CNN-LSTM%20with%20combined%20kernels%20from%20multiple%20branches%20for%20IMDb%20review%20sentiment%20analysis%2C%E2%80%99%202017-10) [GScholar](https://scholar.google.co.uk/scholar?q=Yenter%2C%20A.%20Verma%2C%20A.%20%E2%80%98Deep%20CNN-LSTM%20with%20combined%20kernels%20from%20multiple%20branches%20for%20IMDb%20review%20sentiment%20analysis%2C%E2%80%99%202017-10) 

[^2]: D. Cer, Y. Yang, S.-Y. Kong, N. Hua, N. Limtiaco, R. St. John, N. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar, Y.-H. Sung, B. Strope, and R. Kurzweil, ‘‘Universal sentence encoder,’’ 2018, arXiv:1803.11175. [Online]. Available: http://arxiv.org/abs/1803.11175  [OA](http://arxiv.org/abs/1803.11175)  

[^3]: J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training of deep bidirectional transformers for language understanding,’’ 2018, arXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805  [OA](http://arxiv.org/abs/1810.04805)  

[^4]: Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, ‘‘RoBERTa: A robustly optimized BERT pretraining approach,’’ 2019, arXiv:1907.11692. [Online]. Available: http://arxiv.org/abs/1907.11692  [OA](http://arxiv.org/abs/1907.11692)  

[^5]: B. G. Malkiel, ‘‘The efficient market hypothesis and its critics,’’ J. Econ. Perspect., vol. 17, no. 1, pp. 59–82, Feb. 2003.  [OA](https://engine.scholarcy.com/oa_version?query=Malkiel%2C%20B.G.%20%E2%80%98The%20efficient%20market%20hypothesis%20and%20its%20critics%2C%E2%80%99%202003-02&author=Malkiel&title=%E2%80%98The%20efficient%20market%20hypothesis%20and%20its%20critics%2C%E2%80%99&year=2003) [GScholar](https://scholar.google.co.uk/scholar?q=Malkiel%2C%20B.G.%20%E2%80%98The%20efficient%20market%20hypothesis%20and%20its%20critics%2C%E2%80%99%202003-02) [Scite](/scite_tallies?query=author%3AMalkiel%2Ctitle%3A%E2%80%98The%20efficient%20market%20hypothesis%20and%20its%20critics%2C%E2%80%99%2Cyear%3A2003)

[^6]: M.-Y. Day and C.-C. Lee, ‘‘Deep learning for financial sentiment analysis on finance news providers,’’ in Proc. IEEE/ACM Int. Conf. Adv. Social Netw. Anal. Mining (ASONAM), Aug. 2016, pp. 1127–1134.  [OA](https://engine.scholarcy.com/oa_version?query=Day%2C%20M.-Y.%20Lee%2C%20C.-C.%20%E2%80%98Deep%20learning%20for%20financial%20sentiment%20analysis%20on%20finance%20news%20providers%2C%E2%80%99%202016-08&author=Day&title=%E2%80%98Deep%20learning%20for%20financial%20sentiment%20analysis%20on%20finance%20news%20providers%2C%E2%80%99&year=2016) [GScholar](https://scholar.google.co.uk/scholar?q=Day%2C%20M.-Y.%20Lee%2C%20C.-C.%20%E2%80%98Deep%20learning%20for%20financial%20sentiment%20analysis%20on%20finance%20news%20providers%2C%E2%80%99%202016-08) [Scite](/scite_tallies?query=author%3ADay%2Ctitle%3A%E2%80%98Deep%20learning%20for%20financial%20sentiment%20analysis%20on%20finance%20news%20providers%2C%E2%80%99%2Cyear%3A2016)

[^7]: L. Dodevska, V. Petreski, K. Mishev, A. Gjorgjevikj, I. Vodenska, L. Chitkushev, and D. Trajanov, ‘‘Predicting companies stock price direction by using sentiment analysis of news articles,’’ in Proc. 15th Annu. Int. Conf. Comput. Sci. Educ. Comput. Sci., Fulda, Germany, Jul. 2019, pp. 37–42.  [OA](https://engine.scholarcy.com/oa_version?query=Dodevska%2C%20L.%20Petreski%2C%20V.%20Mishev%2C%20K.%20Gjorgjevikj%2C%20A.%20%E2%80%98Predicting%20companies%20stock%20price%20direction%20by%20using%20sentiment%20analysis%20of%20news%20articles%2C%E2%80%99%202019-07&author=Dodevska&title=%E2%80%98Predicting%20companies%20stock%20price%20direction%20by%20using%20sentiment%20analysis%20of%20news%20articles%2C%E2%80%99&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Dodevska%2C%20L.%20Petreski%2C%20V.%20Mishev%2C%20K.%20Gjorgjevikj%2C%20A.%20%E2%80%98Predicting%20companies%20stock%20price%20direction%20by%20using%20sentiment%20analysis%20of%20news%20articles%2C%E2%80%99%202019-07) [Scite](/scite_tallies?query=author%3ADodevska%2Ctitle%3A%E2%80%98Predicting%20companies%20stock%20price%20direction%20by%20using%20sentiment%20analysis%20of%20news%20articles%2C%E2%80%99%2Cyear%3A2019)

[^8]: W. Souma, I. Vodenska, and H. Aoyama, ‘‘Enhanced news sentiment analysis using deep learning methods,’’ J. Comput. Social Sci., vol. 2, no. 1, pp. 33–46, Jan. 2019.  [OA](https://engine.scholarcy.com/oa_version?query=Souma%2C%20W.%20Vodenska%2C%20I.%20Aoyama%2C%20H.%20%E2%80%98Enhanced%20news%20sentiment%20analysis%20using%20deep%20learning%20methods%2C%E2%80%99%202019-01&author=Souma&title=%E2%80%98Enhanced%20news%20sentiment%20analysis%20using%20deep%20learning%20methods%2C%E2%80%99&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Souma%2C%20W.%20Vodenska%2C%20I.%20Aoyama%2C%20H.%20%E2%80%98Enhanced%20news%20sentiment%20analysis%20using%20deep%20learning%20methods%2C%E2%80%99%202019-01) [Scite](/scite_tallies?query=author%3ASouma%2Ctitle%3A%E2%80%98Enhanced%20news%20sentiment%20analysis%20using%20deep%20learning%20methods%2C%E2%80%99%2Cyear%3A2019)

[^9]: S. F. Crone and C. Koeppel, ‘‘Predicting exchange rates with sentiment indicators: An empirical evaluation using text mining and multilayer perceptrons,’’ in Proc. IEEE Conf. Comput. Intell. Financial Eng. Econ. (CIFEr), Mar. 2014, pp. 114–121.  [OA](https://scholar.google.co.uk/scholar?q=Crone%2C%20S.F.%20Koeppel%2C%20C.%20%E2%80%98Predicting%20exchange%20rates%20with%20sentiment%20indicators%3A%20An%20empirical%20evaluation%20using%20text%20mining%20and%20multilayer%20perceptrons%2C%E2%80%99%202014-03) [GScholar](https://scholar.google.co.uk/scholar?q=Crone%2C%20S.F.%20Koeppel%2C%20C.%20%E2%80%98Predicting%20exchange%20rates%20with%20sentiment%20indicators%3A%20An%20empirical%20evaluation%20using%20text%20mining%20and%20multilayer%20perceptrons%2C%E2%80%99%202014-03) 

[^10]: C. Curme, H. E. Stanley, and I. Vodenska, ‘‘Coupled network approach to predictability of financial market returns and news sentiments,’’ Int. J. Theor. Appl. Finance, vol. 18, no. 7, Nov. 2015, Art. no. 1550043.  [OA](https://engine.scholarcy.com/oa_version?query=Curme%2C%20C.%20Stanley%2C%20H.E.%20Vodenska%2C%20I.%20%E2%80%98Coupled%20network%20approach%20to%20predictability%20of%20financial%20market%20returns%20and%20news%20sentiments%2C%E2%80%99%202015-11&author=Curme&title=%E2%80%98Coupled%20network%20approach%20to%20predictability%20of%20financial%20market%20returns%20and%20news%20sentiments%2C%E2%80%99&year=2015) [GScholar](https://scholar.google.co.uk/scholar?q=Curme%2C%20C.%20Stanley%2C%20H.E.%20Vodenska%2C%20I.%20%E2%80%98Coupled%20network%20approach%20to%20predictability%20of%20financial%20market%20returns%20and%20news%20sentiments%2C%E2%80%99%202015-11) [Scite](/scite_tallies?query=author%3ACurme%2Ctitle%3A%E2%80%98Coupled%20network%20approach%20to%20predictability%20of%20financial%20market%20returns%20and%20news%20sentiments%2C%E2%80%99%2Cyear%3A2015)

[^11]: K. Mishev, A. Gjorgjevikj, I. Vodenska, L. Chitkushev, W. Souma, and D. Trajanov, ‘‘Forecasting corporate revenue by using deep-learning methodologies,’’ in Proc. Int. Conf. Control, Artif. Intell., Robot. Optim. (ICCAIRO), May 2019, pp. 115–120.  [OA](https://engine.scholarcy.com/oa_version?query=Mishev%2C%20K.%20Gjorgjevikj%2C%20A.%20Vodenska%2C%20I.%20Chitkushev%2C%20L.%20%E2%80%98Forecasting%20corporate%20revenue%20by%20using%20deep-learning%20methodologies%2C%E2%80%99%202019&author=Mishev&title=%E2%80%98Forecasting%20corporate%20revenue%20by%20using%20deep-learning%20methodologies%2C%E2%80%99&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Mishev%2C%20K.%20Gjorgjevikj%2C%20A.%20Vodenska%2C%20I.%20Chitkushev%2C%20L.%20%E2%80%98Forecasting%20corporate%20revenue%20by%20using%20deep-learning%20methodologies%2C%E2%80%99%202019) [Scite](/scite_tallies?query=author%3AMishev%2Ctitle%3A%E2%80%98Forecasting%20corporate%20revenue%20by%20using%20deep-learning%20methodologies%2C%E2%80%99%2Cyear%3A2019)

[^12]: T. Loughran and B. Mcdonald, ‘‘When is a liability not a liability? Textual analysis, dictionaries, and 10-ks,’’ J. Finance, vol. 66, no. 1, pp. 35–65, Feb. 2011.  [OA](https://engine.scholarcy.com/oa_version?query=Loughran%2C%20T.%20Mcdonald%2C%20B.%20%E2%80%98When%20is%20a%20liability%20not%20a%20liability%3F%20Textual%20analysis%2C%20dictionaries%2C%20and%2010-ks%2C%E2%80%99%202011-02&author=Loughran&title=%E2%80%98When%20is%20a%20liability%20not%20a%20liability%3F%20Textual%20analysis%2C%20dictionaries%2C%20and%2010-ks%2C%E2%80%99&year=2011) [GScholar](https://scholar.google.co.uk/scholar?q=Loughran%2C%20T.%20Mcdonald%2C%20B.%20%E2%80%98When%20is%20a%20liability%20not%20a%20liability%3F%20Textual%20analysis%2C%20dictionaries%2C%20and%2010-ks%2C%E2%80%99%202011-02) [Scite](/scite_tallies?query=author%3ALoughran%2Ctitle%3A%E2%80%98When%20is%20a%20liability%20not%20a%20liability%3F%20Textual%20analysis%2C%20dictionaries%2C%20and%2010-ks%2C%E2%80%99%2Cyear%3A2011)

[^13]: M. Ghiassi, J. Skinner, and D. Zimbra, ‘‘Twitter brand sentiment analysis: A hybrid system using n-gram analysis and dynamic artificial neural network,’’ Expert Syst. Appl., vol. 40, no. 16, pp. 6266–6282, Nov. 2013.  [OA](https://engine.scholarcy.com/oa_version?query=Ghiassi%2C%20M.%20Skinner%2C%20J.%20Zimbra%2C%20D.%20%E2%80%98Twitter%20brand%20sentiment%20analysis%3A%20A%20hybrid%20system%20using%20n-gram%20analysis%20and%20dynamic%20artificial%20neural%20network%2C%E2%80%99%202013-11&author=Ghiassi&title=%E2%80%98Twitter%20brand%20sentiment%20analysis%3A%20A%20hybrid%20system%20using%20n-gram%20analysis%20and%20dynamic%20artificial%20neural%20network%2C%E2%80%99&year=2013) [GScholar](https://scholar.google.co.uk/scholar?q=Ghiassi%2C%20M.%20Skinner%2C%20J.%20Zimbra%2C%20D.%20%E2%80%98Twitter%20brand%20sentiment%20analysis%3A%20A%20hybrid%20system%20using%20n-gram%20analysis%20and%20dynamic%20artificial%20neural%20network%2C%E2%80%99%202013-11) [Scite](/scite_tallies?query=author%3AGhiassi%2Ctitle%3A%E2%80%98Twitter%20brand%20sentiment%20analysis%3A%20A%20hybrid%20system%20using%20n-gram%20analysis%20and%20dynamic%20artificial%20neural%20network%2C%E2%80%99%2Cyear%3A2013)

[^14]: N. Li, X. Liang, X. Li, C. Wang, and D. D. Wu, ‘‘Network environment and financial risk using machine learning and sentiment analysis,’’ Hum. Ecological Risk Assessment, Int. J., vol. 15, no. 2, pp. 227–252, Apr. 2009.  [OA](https://engine.scholarcy.com/oa_version?query=Li%2C%20N.%20Liang%2C%20X.%20Li%2C%20X.%20Wang%2C%20C.%20%E2%80%98Network%20environment%20and%20financial%20risk%20using%20machine%20learning%20and%20sentiment%20analysis%2C%E2%80%99%202009-04&author=Li&title=%E2%80%98Network%20environment%20and%20financial%20risk%20using%20machine%20learning%20and%20sentiment%20analysis%2C%E2%80%99&year=2009) [GScholar](https://scholar.google.co.uk/scholar?q=Li%2C%20N.%20Liang%2C%20X.%20Li%2C%20X.%20Wang%2C%20C.%20%E2%80%98Network%20environment%20and%20financial%20risk%20using%20machine%20learning%20and%20sentiment%20analysis%2C%E2%80%99%202009-04) [Scite](/scite_tallies?query=author%3ALi%2Ctitle%3A%E2%80%98Network%20environment%20and%20financial%20risk%20using%20machine%20learning%20and%20sentiment%20analysis%2C%E2%80%99%2Cyear%3A2009)

[^15]: G. Wang, T. Wang, B. Wang, D. Sambasivan, Z. Zhang, H. Zheng, and B. Y. Zhao, ‘‘Crowds on wall street: Extracting value from collaborative investing platforms,’’ in Proc. 18th ACM Conf. Comput. Supported Cooperat. Work Social Comput. CSCW, 2015, pp. 17–30.  [OA](https://scholar.google.co.uk/scholar?q=Wang%2C%20G.%20Wang%2C%20T.%20Wang%2C%20B.%20Sambasivan%2C%20D.%20%E2%80%98Crowds%20on%20wall%20street%3A%20Extracting%20value%20from%20collaborative%20investing%20platforms%2C%E2%80%99%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Wang%2C%20G.%20Wang%2C%20T.%20Wang%2C%20B.%20Sambasivan%2C%20D.%20%E2%80%98Crowds%20on%20wall%20street%3A%20Extracting%20value%20from%20collaborative%20investing%20platforms%2C%E2%80%99%202015) 

[^16]: M. Atzeni, A. Dridi, and D. R. Recupero, ‘‘Fine-grained sentiment analysis on financial microblogs and news headlines,’’ in Semantic Web Challenges. Cham, Switzerland: Springer, 2017, pp. 124–128.  [OA](https://scholar.google.co.uk/scholar?q=Atzeni%2C%20M.%20Dridi%2C%20A.%20Recupero%2C%20D.R.%20%E2%80%98Fine-grained%20sentiment%20analysis%20on%20financial%20microblogs%20and%20news%20headlines%2C%E2%80%99%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Atzeni%2C%20M.%20Dridi%2C%20A.%20Recupero%2C%20D.R.%20%E2%80%98Fine-grained%20sentiment%20analysis%20on%20financial%20microblogs%20and%20news%20headlines%2C%E2%80%99%202017) 

[^17]: S. Agaian and P. Kolm, ‘‘Financial sentiment analysis using machine learning techniques,’’ Int. J. Investment Manage. Financial Innov., vol. 3, pp. 1–9, 2017.  [OA](https://engine.scholarcy.com/oa_version?query=Agaian%2C%20S.%20Kolm%2C%20P.%20%E2%80%98Financial%20sentiment%20analysis%20using%20machine%20learning%20techniques%2C%E2%80%99%202017&author=Agaian&title=%E2%80%98Financial%20sentiment%20analysis%20using%20machine%20learning%20techniques%2C%E2%80%99&year=2017) [GScholar](https://scholar.google.co.uk/scholar?q=Agaian%2C%20S.%20Kolm%2C%20P.%20%E2%80%98Financial%20sentiment%20analysis%20using%20machine%20learning%20techniques%2C%E2%80%99%202017) [Scite](/scite_tallies?query=author%3AAgaian%2Ctitle%3A%E2%80%98Financial%20sentiment%20analysis%20using%20machine%20learning%20techniques%2C%E2%80%99%2Cyear%3A2017)

[^18]: S. Sohangir, N. Petty, and D. Wang, ‘‘Financial sentiment lexicon analysis,’’ in Proc. IEEE 12th Int. Conf. Semantic Comput. (ICSC), Jan. 2018, pp. 286–289.  [OA](https://scholar.google.co.uk/scholar?q=Sohangir%2C%20S.%20Petty%2C%20N.%20Wang%2C%20D.%20%E2%80%98Financial%20sentiment%20lexicon%20analysis%2C%E2%80%99%202018-01) [GScholar](https://scholar.google.co.uk/scholar?q=Sohangir%2C%20S.%20Petty%2C%20N.%20Wang%2C%20D.%20%E2%80%98Financial%20sentiment%20lexicon%20analysis%2C%E2%80%99%202018-01) 

[^19]: S. Sohangir, D. Wang, A. Pomeranets, and T. M. Khoshgoftaar, ‘‘Big data: Deep learning for financial sentiment analysis,’’ J. Big Data, vol. 5, no. 1, p. 3, Dec. 2018.  [OA](https://engine.scholarcy.com/oa_version?query=Sohangir%2C%20S.%20Wang%2C%20D.%20Pomeranets%2C%20A.%20Khoshgoftaar%2C%20T.M.%20%E2%80%98Big%20data%3A%20Deep%20learning%20for%20financial%20sentiment%20analysis%2C%E2%80%99%202018-12&author=Sohangir&title=%E2%80%98Big%20data%3A%20Deep%20learning%20for%20financial%20sentiment%20analysis%2C%E2%80%99&year=2018) [GScholar](https://scholar.google.co.uk/scholar?q=Sohangir%2C%20S.%20Wang%2C%20D.%20Pomeranets%2C%20A.%20Khoshgoftaar%2C%20T.M.%20%E2%80%98Big%20data%3A%20Deep%20learning%20for%20financial%20sentiment%20analysis%2C%E2%80%99%202018-12) [Scite](/scite_tallies?query=author%3ASohangir%2Ctitle%3A%E2%80%98Big%20data%3A%20Deep%20learning%20for%20financial%20sentiment%20analysis%2C%E2%80%99%2Cyear%3A2018)

[^20]: L. Zhang, S. Wang, and B. Liu, ‘‘Deep learning for sentiment analysis: A survey,’’ Wiley Interdiscipl. Rev., Data Mining Knowl. Discovery, vol. 8, no. 4, 2018, Art. no. e1253.  [OA](https://engine.scholarcy.com/oa_version?query=Zhang%2C%20L.%20Wang%2C%20S.%20Liu%2C%20B.%20%E2%80%98Deep%20learning%20for%20sentiment%20analysis%3A%20A%20survey%2C%E2%80%99%202018&author=Zhang&title=%E2%80%98Deep%20learning%20for%20sentiment%20analysis%3A%20A%20survey%2C%E2%80%99&year=2018) [GScholar](https://scholar.google.co.uk/scholar?q=Zhang%2C%20L.%20Wang%2C%20S.%20Liu%2C%20B.%20%E2%80%98Deep%20learning%20for%20sentiment%20analysis%3A%20A%20survey%2C%E2%80%99%202018) [Scite](/scite_tallies?query=author%3AZhang%2Ctitle%3A%E2%80%98Deep%20learning%20for%20sentiment%20analysis%3A%20A%20survey%2C%E2%80%99%2Cyear%3A2018)

[^21]: D. Tang, B. Qin, and T. Liu, ‘‘Document modeling with gated recurrent neural network for sentiment classification,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., 2015, pp. 1422–1432.  [OA](https://scholar.google.co.uk/scholar?q=Tang%2C%20D.%20Qin%2C%20B.%20Liu%2C%20T.%20%E2%80%98Document%20modeling%20with%20gated%20recurrent%20neural%20network%20for%20sentiment%20classification%2C%E2%80%99%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Tang%2C%20D.%20Qin%2C%20B.%20Liu%2C%20T.%20%E2%80%98Document%20modeling%20with%20gated%20recurrent%20neural%20network%20for%20sentiment%20classification%2C%E2%80%99%202015) 

[^22]: K. Sheng Tai, R. Socher, and C. D. Manning, ‘‘Improved semantic representations from tree-structured long short-term memory networks,’’ 2015, arXiv:1503.00075. [Online]. Available: http://arxiv.org/abs/1503.00075  [OA](http://arxiv.org/abs/1503.00075)  

[^23]: Y. Kim, ‘‘Convolutional neural networks for sentence classification,’’ 2014, arXiv:1408.5882. [Online]. Available: http://arxiv.org/abs/1408.5882  [OA](http://arxiv.org/abs/1408.5882)  

[^24]: X. Zhang, J. Zhao, and Y. LeCun, ‘‘Character-level convolutional networks for text classification,’’ in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 649–657.  [OA](https://scholar.google.co.uk/scholar?q=Zhang%2C%20X.%20Zhao%2C%20J.%20LeCun%2C%20Y.%20%E2%80%98Character-level%20convolutional%20networks%20for%20text%20classification%2C%E2%80%99%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Zhang%2C%20X.%20Zhao%2C%20J.%20LeCun%2C%20Y.%20%E2%80%98Character-level%20convolutional%20networks%20for%20text%20classification%2C%E2%80%99%202015) 

[^25]: R. Johnson and T. Zhang, ‘‘Deep pyramid convolutional neural networks for text categorization,’’ in Proc. 55th Annu. Meeting Assoc. Comput. Linguistics (Long Papers), vol. 1, 2017, pp. 562–570.  [OA](https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20%E2%80%98Deep%20pyramid%20convolutional%20neural%20networks%20for%20text%20categorization%2C%E2%80%99%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20%E2%80%98Deep%20pyramid%20convolutional%20neural%20networks%20for%20text%20categorization%2C%E2%80%99%202017) 

[^26]: Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, ‘‘Hierarchical attention networks for document classification,’’ in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol., 2016, pp. 1480–1489.  [OA](https://engine.scholarcy.com/oa_version?query=Yang%2C%20Z.%20Yang%2C%20D.%20Dyer%2C%20C.%20He%2C%20X.%20%E2%80%98Hierarchical%20attention%20networks%20for%20document%20classification%2C%E2%80%99%202016&author=Yang&title=%E2%80%98Hierarchical%20attention%20networks%20for%20document%20classification%2C%E2%80%99&year=2016) [GScholar](https://scholar.google.co.uk/scholar?q=Yang%2C%20Z.%20Yang%2C%20D.%20Dyer%2C%20C.%20He%2C%20X.%20%E2%80%98Hierarchical%20attention%20networks%20for%20document%20classification%2C%E2%80%99%202016) [Scite](/scite_tallies?query=author%3AYang%2Ctitle%3A%E2%80%98Hierarchical%20attention%20networks%20for%20document%20classification%2C%E2%80%99%2Cyear%3A2016)

[^27]: T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‘‘Distributed representations of words and phrases and their compositionality,’’ in Proc. Adv. Neural Inf. Process. Syst., 2013, pp. 3111–3119.  [OA](https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Sutskever%2C%20I.%20Chen%2C%20K.%20Corrado%2C%20G.S.%20%E2%80%98Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%2C%E2%80%99%202013) [GScholar](https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Sutskever%2C%20I.%20Chen%2C%20K.%20Corrado%2C%20G.S.%20%E2%80%98Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%2C%E2%80%99%202013) 

[^28]: J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for word representation,’’ in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP), 2014, pp. 1532–1543.  [OA](https://scholar.google.co.uk/scholar?q=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.%20%E2%80%98Glove%3A%20Global%20vectors%20for%20word%20representation%2C%E2%80%99%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.%20%E2%80%98Glove%3A%20Global%20vectors%20for%20word%20representation%2C%E2%80%99%202014) 

[^29]: P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ‘‘Enriching word vectors with subword information,’’ Trans. Assoc. Comput. Linguistics, vol. 5, pp. 135–146, Dec. 2017.  [OA](https://engine.scholarcy.com/oa_version?query=Bojanowski%2C%20P.%20Grave%2C%20E.%20Joulin%2C%20A.%20Mikolov%2C%20T.%20%E2%80%98Enriching%20word%20vectors%20with%20subword%20information%2C%E2%80%99%202017-12&author=Bojanowski&title=%E2%80%98Enriching%20word%20vectors%20with%20subword%20information%2C%E2%80%99&year=2017) [GScholar](https://scholar.google.co.uk/scholar?q=Bojanowski%2C%20P.%20Grave%2C%20E.%20Joulin%2C%20A.%20Mikolov%2C%20T.%20%E2%80%98Enriching%20word%20vectors%20with%20subword%20information%2C%E2%80%99%202017-12) [Scite](/scite_tallies?query=author%3ABojanowski%2Ctitle%3A%E2%80%98Enriching%20word%20vectors%20with%20subword%20information%2C%E2%80%99%2Cyear%3A2017)

[^30]: Q. Le and T. Mikolov, ‘‘Distributed representations of sentences and documents,’’ in Proc. Int. Conf. Mach. Learn., 2014, pp. 1188–1196.  [OA](https://scholar.google.co.uk/scholar?q=Le%2C%20Q.%20Mikolov%2C%20T.%20%E2%80%98Distributed%20representations%20of%20sentences%20and%20documents%2C%E2%80%99%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Le%2C%20Q.%20Mikolov%2C%20T.%20%E2%80%98Distributed%20representations%20of%20sentences%20and%20documents%2C%E2%80%99%202014) 

[^31]: R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler, ‘‘Skip-thought vectors,’’ in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 3294–3302.  [OA](https://scholar.google.co.uk/scholar?q=Kiros%2C%20R.%20Zhu%2C%20Y.%20Salakhutdinov%2C%20R.R.%20Zemel%2C%20R.%20%E2%80%98Skip-thought%20vectors%2C%E2%80%99%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Kiros%2C%20R.%20Zhu%2C%20Y.%20Salakhutdinov%2C%20R.R.%20Zemel%2C%20R.%20%E2%80%98Skip-thought%20vectors%2C%E2%80%99%202015) 

[^32]: A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes, ‘‘Supervised learning of universal sentence representations from natural language inference data,’’ 2017, arXiv:1705.02364. [Online]. Available: http://arxiv.org/abs/1705.02364  [OA](http://arxiv.org/abs/1705.02364)  

[^33]: M. Artetxe and H. Schwenk, ‘‘Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond,’’ Trans. Assoc. Comput. Linguistics, vol. 7, pp. 597–610, Mar. 2019.  [OA](https://engine.scholarcy.com/oa_version?query=Artetxe%2C%20M.%20Schwenk%2C%20H.%20%E2%80%98Massively%20multilingual%20sentence%20embeddings%20for%20zero-shot%20cross-lingual%20transfer%20and%20beyond%2C%E2%80%99%202019-03&author=Artetxe&title=%E2%80%98Massively%20multilingual%20sentence%20embeddings%20for%20zero-shot%20cross-lingual%20transfer%20and%20beyond%2C%E2%80%99&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Artetxe%2C%20M.%20Schwenk%2C%20H.%20%E2%80%98Massively%20multilingual%20sentence%20embeddings%20for%20zero-shot%20cross-lingual%20transfer%20and%20beyond%2C%E2%80%99%202019-03) [Scite](/scite_tallies?query=author%3AArtetxe%2Ctitle%3A%E2%80%98Massively%20multilingual%20sentence%20embeddings%20for%20zero-shot%20cross-lingual%20transfer%20and%20beyond%2C%E2%80%99%2Cyear%3A2019)

[^34]: X. Man, T. Luo, and J. Lin, ‘‘Financial sentiment analysis(FSA): A survey,’’ in Proc. IEEE Int. Conf. Ind. Cyber Phys. Syst. (ICPS), May 2019, pp. 617–622.  [OA](https://engine.scholarcy.com/oa_version?query=Man%2C%20X.%20Luo%2C%20T.%20Lin%2C%20J.%20%E2%80%98Financial%20sentiment%20analysis%28FSA%29%3A%20A%20survey%2C%E2%80%99%202019&author=Man&title=%E2%80%98Financial%20sentiment%20analysis%28FSA%29%3A%20A%20survey%2C%E2%80%99&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Man%2C%20X.%20Luo%2C%20T.%20Lin%2C%20J.%20%E2%80%98Financial%20sentiment%20analysis%28FSA%29%3A%20A%20survey%2C%E2%80%99%202019) [Scite](/scite_tallies?query=author%3AMan%2Ctitle%3A%E2%80%98Financial%20sentiment%20analysis%28FSA%29%3A%20A%20survey%2C%E2%80%99%2Cyear%3A2019)

[^35]: S. Yang, J. Rosenfeld, and J. Makutonin, ‘‘Financial aspect-based sentiment analysis using deep representations,’’ 2018, arXiv:1808.07931. [Online]. Available: http://arxiv.org/abs/1808.07931  [OA](http://arxiv.org/abs/1808.07931)  

[^36]: C.-H. Du, M.-F. Tsai, and C.-J. Wang, ‘‘Beyond word-level to sentence-level sentiment analysis for financial reports,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May 2019, pp. 1562–1566.  [OA](https://scholar.google.co.uk/scholar?q=Du%2C%20C.-H.%20Tsai%2C%20M.-F.%20Wang%2C%20C.-J.%20%E2%80%98Beyond%20word-level%20to%20sentence-level%20sentiment%20analysis%20for%20financial%20reports%2C%E2%80%99%202019-05) [GScholar](https://scholar.google.co.uk/scholar?q=Du%2C%20C.-H.%20Tsai%2C%20M.-F.%20Wang%2C%20C.-J.%20%E2%80%98Beyond%20word-level%20to%20sentence-level%20sentiment%20analysis%20for%20financial%20reports%2C%E2%80%99%202019-05) 

[^37]: L. Zhao, L. Li, and X. Zheng, ‘‘A BERT based sentiment analysis and key entity detection approach for online financial texts,’’ 2020, arXiv:2001.05326. [Online]. Available: http://arxiv.org/abs/2001.05326  [OA](http://arxiv.org/abs/2001.05326)  

[^38]: J. Howard and S. Ruder, ‘‘Universal language model fine-tuning for text classification,’’ 2018, arXiv:1801.06146. [Online]. Available: http://arxiv.org/abs/1801.06146  [OA](http://arxiv.org/abs/1801.06146)  

[^39]: P. J. Stone, D. C. Dunphy, and M. S. Smith, The General Inquirer: A Computer Approach to Content Analysis. Oxford, U.K.: MIT Press, 1966.  [OA](https://scholar.google.co.uk/scholar?q=Stone%2C%20P.J.%20Dunphy%2C%20D.C.%20Smith%2C%20M.S.%20The%20General%20Inquirer%3A%20A%20Computer%20Approach%20to%20Content%20Analysis%201966) [GScholar](https://scholar.google.co.uk/scholar?q=Stone%2C%20P.J.%20Dunphy%2C%20D.C.%20Smith%2C%20M.S.%20The%20General%20Inquirer%3A%20A%20Computer%20Approach%20to%20Content%20Analysis%201966) 

[^40]: J. L. Rogers, A. Van Buskirk, and S. L. C. Zechman, ‘‘Disclosure tone and shareholder litigation,’’ Accounting Rev., vol. 86, no. 6, pp. 2155–2183, Nov. 2011.  [OA](https://engine.scholarcy.com/oa_version?query=Rogers%2C%20J.L.%20Buskirk%2C%20A.%20Zechman%2C%20S.L.C.%20%E2%80%98Disclosure%20tone%20and%20shareholder%20litigation%2C%E2%80%99%202011-11&author=Rogers&title=%E2%80%98Disclosure%20tone%20and%20shareholder%20litigation%2C%E2%80%99&year=2011) [GScholar](https://scholar.google.co.uk/scholar?q=Rogers%2C%20J.L.%20Buskirk%2C%20A.%20Zechman%2C%20S.L.C.%20%E2%80%98Disclosure%20tone%20and%20shareholder%20litigation%2C%E2%80%99%202011-11) [Scite](/scite_tallies?query=author%3ARogers%2Ctitle%3A%E2%80%98Disclosure%20tone%20and%20shareholder%20litigation%2C%E2%80%99%2Cyear%3A2011)

[^41]: A. K. Davis, W. Ge, D. Matsumoto, and J. L. Zhang, ‘‘The effect of manager-specific optimism on the tone of earnings conference calls,’’ Rev. Accounting Stud., vol. 20, no. 2, pp. 639–673, Jun. 2015.  [OA](https://engine.scholarcy.com/oa_version?query=Davis%2C%20A.K.%20Ge%2C%20W.%20Matsumoto%2C%20D.%20Zhang%2C%20J.L.%20%E2%80%98The%20effect%20of%20manager-specific%20optimism%20on%20the%20tone%20of%20earnings%20conference%20calls%2C%E2%80%99%202015-06&author=Davis&title=%E2%80%98The%20effect%20of%20manager-specific%20optimism%20on%20the%20tone%20of%20earnings%20conference%20calls%2C%E2%80%99&year=2015) [GScholar](https://scholar.google.co.uk/scholar?q=Davis%2C%20A.K.%20Ge%2C%20W.%20Matsumoto%2C%20D.%20Zhang%2C%20J.L.%20%E2%80%98The%20effect%20of%20manager-specific%20optimism%20on%20the%20tone%20of%20earnings%20conference%20calls%2C%E2%80%99%202015-06) [Scite](/scite_tallies?query=author%3ADavis%2Ctitle%3A%E2%80%98The%20effect%20of%20manager-specific%20optimism%20on%20the%20tone%20of%20earnings%20conference%20calls%2C%E2%80%99%2Cyear%3A2015)

[^42]: W. Zhang and S. Skiena, ‘‘Trading strategies to exploit blog and news sentiment,’’ in Proc. 4th Int. AAAI Conf. Weblogs Social Media, May 2010, pp. 1–4.  [OA](https://scholar.google.co.uk/scholar?q=Zhang%2C%20W.%20Skiena%2C%20S.%20%E2%80%98Trading%20strategies%20to%20exploit%20blog%20and%20news%20sentiment%2C%E2%80%99%202010-05) [GScholar](https://scholar.google.co.uk/scholar?q=Zhang%2C%20W.%20Skiena%2C%20S.%20%E2%80%98Trading%20strategies%20to%20exploit%20blog%20and%20news%20sentiment%2C%E2%80%99%202010-05) 

[^43]: T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Efficient estimation of word representations in vector space,’’ 2013, arXiv:1301.3781. [Online]. Available: http://arxiv.org/abs/1301.3781  [OA](http://arxiv.org/abs/1301.3781)  

[^44]: Z. S. Harris, ‘‘Distributional structure,’’ Word, vol. 10, nos. 2–3, pp. 146–162, Aug. 1954.  [OA](https://engine.scholarcy.com/oa_version?query=Harris%2C%20Z.S.%20%E2%80%98Distributional%20structure%2C%E2%80%99%201954-08&author=Harris&title=%E2%80%98Distributional%20structure%2C%E2%80%99&year=1954) [GScholar](https://scholar.google.co.uk/scholar?q=Harris%2C%20Z.S.%20%E2%80%98Distributional%20structure%2C%E2%80%99%201954-08) [Scite](/scite_tallies?query=author%3AHarris%2Ctitle%3A%E2%80%98Distributional%20structure%2C%E2%80%99%2Cyear%3A1954)

[^45]: T. K. Landauer and S. T. Dumais, ‘‘A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.,’’ Psychol. Rev., vol. 104, no. 2, p. 211, 1997.  [OA](https://engine.scholarcy.com/oa_version?query=Landauer%2C%20T.K.%20Dumais%2C%20S.T.%20%E2%80%98A%20solution%20to%20Plato%E2%80%99s%20problem%3A%20The%20latent%20semantic%20analysis%20theory%20of%20acquisition%2C%20induction%2C%20and%20representation%20of%20knowledge.%2C%E2%80%99%201997&author=Landauer&title=%E2%80%98A%20solution%20to%20Plato%E2%80%99s%20problem%3A%20The%20latent%20semantic%20analysis%20theory%20of%20acquisition%2C%20induction%2C%20and%20representation%20of%20knowledge.%2C%E2%80%99&year=1997) [GScholar](https://scholar.google.co.uk/scholar?q=Landauer%2C%20T.K.%20Dumais%2C%20S.T.%20%E2%80%98A%20solution%20to%20Plato%E2%80%99s%20problem%3A%20The%20latent%20semantic%20analysis%20theory%20of%20acquisition%2C%20induction%2C%20and%20representation%20of%20knowledge.%2C%E2%80%99%201997) [Scite](/scite_tallies?query=author%3ALandauer%2Ctitle%3A%E2%80%98A%20solution%20to%20Plato%E2%80%99s%20problem%3A%20The%20latent%20semantic%20analysis%20theory%20of%20acquisition%2C%20induction%2C%20and%20representation%20of%20knowledge.%2C%E2%80%99%2Cyear%3A1997)

[^46]: M. Sahlgren, ‘‘The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces,’’ Ph.D. dissertation, Dept. Comput. Linguistics, Stockholm Univ., Stockholm, Sweden, 2006.  [OA](https://scholar.google.co.uk/scholar?q=Sahlgren%2C%20M.%20%E2%80%98The%20word-space%20model%3A%20Using%20distributional%20analysis%20to%20represent%20syntagmatic%20and%20paradigmatic%20relations%20between%20words%20in%20high-dimensional%20vector%20spaces%2C%E2%80%99%202006) [GScholar](https://scholar.google.co.uk/scholar?q=Sahlgren%2C%20M.%20%E2%80%98The%20word-space%20model%3A%20Using%20distributional%20analysis%20to%20represent%20syntagmatic%20and%20paradigmatic%20relations%20between%20words%20in%20high-dimensional%20vector%20spaces%2C%E2%80%99%202006) 

[^47]: P. D. Turney and P. Pantel, ‘‘From frequency to meaning: Vector space models of semantics,’’ J. Artif. Intell. Res., vol. 37, pp. 141–188, Feb. 2010.  [OA](https://engine.scholarcy.com/oa_version?query=Turney%2C%20P.D.%20Pantel%2C%20P.%20%E2%80%98From%20frequency%20to%20meaning%3A%20Vector%20space%20models%20of%20semantics%2C%E2%80%99%202010-02&author=Turney&title=%E2%80%98From%20frequency%20to%20meaning%3A%20Vector%20space%20models%20of%20semantics%2C%E2%80%99&year=2010) [GScholar](https://scholar.google.co.uk/scholar?q=Turney%2C%20P.D.%20Pantel%2C%20P.%20%E2%80%98From%20frequency%20to%20meaning%3A%20Vector%20space%20models%20of%20semantics%2C%E2%80%99%202010-02) [Scite](/scite_tallies?query=author%3ATurney%2Ctitle%3A%E2%80%98From%20frequency%20to%20meaning%3A%20Vector%20space%20models%20of%20semantics%2C%E2%80%99%2Cyear%3A2010)

[^48]: A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, ‘‘Bag of tricks for efficient text classification,’’ in Proc. 15th Conf. Eur. Chapter Assoc. Comput. Linguistics, Short Papers, vol. 2, 2017, pp. 427–431.  [OA](https://engine.scholarcy.com/oa_version?query=Joulin%2C%20A.%20Grave%2C%20E.%20Bojanowski%2C%20P.%20Mikolov%2C%20T.%20%E2%80%98Bag%20of%20tricks%20for%20efficient%20text%20classification%2C%E2%80%99%202017&author=Joulin&title=%E2%80%98Bag%20of%20tricks%20for%20efficient%20text%20classification%2C%E2%80%99&year=2017) [GScholar](https://scholar.google.co.uk/scholar?q=Joulin%2C%20A.%20Grave%2C%20E.%20Bojanowski%2C%20P.%20Mikolov%2C%20T.%20%E2%80%98Bag%20of%20tricks%20for%20efficient%20text%20classification%2C%E2%80%99%202017) [Scite](/scite_tallies?query=author%3AJoulin%2Ctitle%3A%E2%80%98Bag%20of%20tricks%20for%20efficient%20text%20classification%2C%E2%80%99%2Cyear%3A2017)

[^49]: Y. Sharma, G. Agrawal, P. Jain, and T. Kumar, ‘‘Vector representation of words for sentiment analysis using GloVe,’’ in Proc. Int. Conf. Intell. Commun. Comput. Techn. (ICCT), Dec. 2017, pp. 279–284.  [OA](https://engine.scholarcy.com/oa_version?query=Sharma%2C%20Y.%20Agrawal%2C%20G.%20Jain%2C%20P.%20Kumar%2C%20T.%20%E2%80%98Vector%20representation%20of%20words%20for%20sentiment%20analysis%20using%20GloVe%2C%E2%80%99%202017-12&author=Sharma&title=%E2%80%98Vector%20representation%20of%20words%20for%20sentiment%20analysis%20using%20GloVe%2C%E2%80%99&year=2017) [GScholar](https://scholar.google.co.uk/scholar?q=Sharma%2C%20Y.%20Agrawal%2C%20G.%20Jain%2C%20P.%20Kumar%2C%20T.%20%E2%80%98Vector%20representation%20of%20words%20for%20sentiment%20analysis%20using%20GloVe%2C%E2%80%99%202017-12) [Scite](/scite_tallies?query=author%3ASharma%2Ctitle%3A%E2%80%98Vector%20representation%20of%20words%20for%20sentiment%20analysis%20using%20GloVe%2C%E2%80%99%2Cyear%3A2017)

[^50]: P. Lauren, G. Qu, J. Yang, P. Watta, G.-B. Huang, and A. Lendasse, ‘‘Generating word embeddings from an extreme learning machine for sentiment analysis and sequence labeling tasks,’’ Cognit. Comput., vol. 10, no. 4, pp. 625–638, Aug. 2018.  [OA](https://engine.scholarcy.com/oa_version?query=Lauren%2C%20P.%20Qu%2C%20G.%20Yang%2C%20J.%20Watta%2C%20P.%20%E2%80%98Generating%20word%20embeddings%20from%20an%20extreme%20learning%20machine%20for%20sentiment%20analysis%20and%20sequence%20labeling%20tasks%2C%E2%80%99%202018-08&author=Lauren&title=%E2%80%98Generating%20word%20embeddings%20from%20an%20extreme%20learning%20machine%20for%20sentiment%20analysis%20and%20sequence%20labeling%20tasks%2C%E2%80%99&year=2018) [GScholar](https://scholar.google.co.uk/scholar?q=Lauren%2C%20P.%20Qu%2C%20G.%20Yang%2C%20J.%20Watta%2C%20P.%20%E2%80%98Generating%20word%20embeddings%20from%20an%20extreme%20learning%20machine%20for%20sentiment%20analysis%20and%20sequence%20labeling%20tasks%2C%E2%80%99%202018-08) [Scite](/scite_tallies?query=author%3ALauren%2Ctitle%3A%E2%80%98Generating%20word%20embeddings%20from%20an%20extreme%20learning%20machine%20for%20sentiment%20analysis%20and%20sequence%20labeling%20tasks%2C%E2%80%99%2Cyear%3A2018)

[^51]: S. M. Rezaeinia, R. Rahmani, A. Ghodsi, and H. Veisi, ‘‘Sentiment analysis based on improved pre-trained word embeddings,’’ Expert Syst. Appl., vol. 117, pp. 139–147, Mar. 2019.  [OA](https://engine.scholarcy.com/oa_version?query=Rezaeinia%2C%20S.M.%20Rahmani%2C%20R.%20Ghodsi%2C%20A.%20Veisi%2C%20H.%20%E2%80%98Sentiment%20analysis%20based%20on%20improved%20pre-trained%20word%20embeddings%2C%E2%80%99%202019-03&author=Rezaeinia&title=%E2%80%98Sentiment%20analysis%20based%20on%20improved%20pre-trained%20word%20embeddings%2C%E2%80%99&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Rezaeinia%2C%20S.M.%20Rahmani%2C%20R.%20Ghodsi%2C%20A.%20Veisi%2C%20H.%20%E2%80%98Sentiment%20analysis%20based%20on%20improved%20pre-trained%20word%20embeddings%2C%E2%80%99%202019-03) [Scite](/scite_tallies?query=author%3ARezaeinia%2Ctitle%3A%E2%80%98Sentiment%20analysis%20based%20on%20improved%20pre-trained%20word%20embeddings%2C%E2%80%99%2Cyear%3A2019)

[^52]: T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin, ‘‘Advances in pre-training distributed word representations,’’ 2017, arXiv:1712.09405. [Online]. Available: http://arxiv.org/abs/1712.09405  [OA](http://arxiv.org/abs/1712.09405)  

[^53]: R. Velioglu, T. Yildiz, and S. Yildirim, ‘‘Sentiment analysis using learning approaches over emojis for turkish tweets,’’ in Proc. 3rd Int. Conf. Comput. Sci. Eng. (UBMK), Sep. 2018, pp. 303–307.  [OA](https://engine.scholarcy.com/oa_version?query=Velioglu%2C%20R.%20Yildiz%2C%20T.%20Yildirim%2C%20S.%20%E2%80%98Sentiment%20analysis%20using%20learning%20approaches%20over%20emojis%20for%20turkish%20tweets%2C%E2%80%99%202018-09&author=Velioglu&title=%E2%80%98Sentiment%20analysis%20using%20learning%20approaches%20over%20emojis%20for%20turkish%20tweets%2C%E2%80%99&year=2018) [GScholar](https://scholar.google.co.uk/scholar?q=Velioglu%2C%20R.%20Yildiz%2C%20T.%20Yildirim%2C%20S.%20%E2%80%98Sentiment%20analysis%20using%20learning%20approaches%20over%20emojis%20for%20turkish%20tweets%2C%E2%80%99%202018-09) [Scite](/scite_tallies?query=author%3AVelioglu%2Ctitle%3A%E2%80%98Sentiment%20analysis%20using%20learning%20approaches%20over%20emojis%20for%20turkish%20tweets%2C%E2%80%99%2Cyear%3A2018)

[^54]: A. A. Altowayan and A. Elnagar, ‘‘Improving arabic sentiment analysis with sentiment-specific embeddings,’’ in Proc. IEEE Int. Conf. Big Data (Big Data), Dec. 2017, pp. 4314–4320.  [OA](https://scholar.google.co.uk/scholar?q=Altowayan%2C%20A.A.%20Elnagar%2C%20A.%20%E2%80%98Improving%20arabic%20sentiment%20analysis%20with%20sentiment-specific%20embeddings%2C%E2%80%99%202017-12) [GScholar](https://scholar.google.co.uk/scholar?q=Altowayan%2C%20A.A.%20Elnagar%2C%20A.%20%E2%80%98Improving%20arabic%20sentiment%20analysis%20with%20sentiment-specific%20embeddings%2C%E2%80%99%202017-12) 

[^55]: M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018, arXiv:1802.05365. [Online]. Available: http://arxiv.org/abs/1802.05365  [OA](http://arxiv.org/abs/1802.05365)  

[^56]: J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, ‘‘Empirical evaluation of gated recurrent neural networks on sequence modeling,’’ 2014, arXiv:1412.3555. [Online]. Available: http://arxiv.org/abs/1412.3555  [OA](http://arxiv.org/abs/1412.3555)  

[^57]: L. Logeswaran and H. Lee, ‘‘An efficient framework for learning sentence representations,’’ 2018, arXiv:1803.02893. [Online]. Available: http://arxiv.org/abs/1803.02893  [OA](http://arxiv.org/abs/1803.02893)  

[^58]: A. Akbik, D. Blythe, and R. Vollgraf, ‘‘Contextual string embeddings for sequence labeling,’’ in Proc. 27th Int. Conf. Comput. Linguistics, 2018, pp. 1638–1649.  [OA](https://scholar.google.co.uk/scholar?q=Akbik%2C%20A.%20Blythe%2C%20D.%20Vollgraf%2C%20R.%20%E2%80%98Contextual%20string%20embeddings%20for%20sequence%20labeling%2C%E2%80%99%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Akbik%2C%20A.%20Blythe%2C%20D.%20Vollgraf%2C%20R.%20%E2%80%98Contextual%20string%20embeddings%20for%20sequence%20labeling%2C%E2%80%99%202018) 

[^59]: A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 5998–6008.  [OA](https://scholar.google.co.uk/scholar?q=Vaswani%2C%20A.%20Shazeer%2C%20N.%20Parmar%2C%20N.%20Uszkoreit%2C%20J.%20%E2%80%98Attention%20is%20all%20you%20need%2C%E2%80%99%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Vaswani%2C%20A.%20Shazeer%2C%20N.%20Parmar%2C%20N.%20Uszkoreit%2C%20J.%20%E2%80%98Attention%20is%20all%20you%20need%2C%E2%80%99%202017) 

[^60]: Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler, ‘‘Aligning books and movies: Towards story-like visual explanations by watching movies and reading books,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 19–27.  [OA](https://scholar.google.co.uk/scholar?q=Zhu%2C%20Y.%20Kiros%2C%20R.%20Zemel%2C%20R.%20Salakhutdinov%2C%20R.%20%E2%80%98Aligning%20books%20and%20movies%3A%20Towards%20story-like%20visual%20explanations%20by%20watching%20movies%20and%20reading%20books%2C%E2%80%99%202015-12) [GScholar](https://scholar.google.co.uk/scholar?q=Zhu%2C%20Y.%20Kiros%2C%20R.%20Zemel%2C%20R.%20Salakhutdinov%2C%20R.%20%E2%80%98Aligning%20books%20and%20movies%3A%20Towards%20story-like%20visual%20explanations%20by%20watching%20movies%20and%20reading%20books%2C%E2%80%99%202015-12) 

[^61]: D. Araci, ‘‘FinBERT: Financial sentiment analysis with pre-trained language models,’’ 2019, arXiv:1908.10063. [Online]. Available: http://arxiv.org/abs/1908.10063  [OA](http://arxiv.org/abs/1908.10063)  

[^62]: G. Lample and A. Conneau, ‘‘Cross-lingual language model pretraining,’’ 2019, arXiv:1901.07291. [Online]. Available: http://arxiv.org/abs/1901.07291  [OA](http://arxiv.org/abs/1901.07291)  

[^63]: Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, ‘‘ALBERT: A lite BERT for self-supervised learning of language representations,’’ 2019, arXiv:1909.11942. [Online]. Available: http://arxiv.org/abs/1909.11942  [OA](http://arxiv.org/abs/1909.11942)  

[^64]: M. A. Al-Garadi, Y.-C. Yang, H. Cai, Y. Ruan, K. O’Connor, G. Gonzalez-Hernandez, J. Perrone, and A. Sarker, Text Classification Models for the Automatic Detection of Nonmedical Prescription Medication Use from Social Media. Cold Spring Harbor Laboratory Press, 2020. [Online]. Available: https://www.medrxiv.org/content/early/2020/04/17/2020.04.13.20064089.full.pdf, doi:10.1101/2020.04.13.20064089.  [OA](https://doi.org/10.1101/2020.04.13.20064089)  [Scite](/scite_tallies?query=https://doi.org/10.1101/2020.04.13.20064089)

[^65]: V. Sanh, L. Debut, J. Chaumond, and T. Wolf, ‘‘DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter,’’ 2019, arXiv:1910.01108. [Online]. Available: http://arxiv.org/abs/1910.01108  [OA](http://arxiv.org/abs/1910.01108)  

[^66]: A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, ‘‘Unsupervised cross-lingual representation learning at scale,’’ 2019, arXiv:1911.02116. [Online]. Available: http://arxiv.org/abs/1911.02116  [OA](http://arxiv.org/abs/1911.02116)  

[^67]: M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, ‘‘BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension,’’ 2019, arXiv:1910.13461. [Online]. Available: http://arxiv.org/abs/1910.13461  [OA](http://arxiv.org/abs/1910.13461)  

[^68]: A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, ‘‘Language models are unsupervised multitask learners,’’ OpenAI Blog, vol. 1, no. 8, p. 9, 2019.  [OA](https://engine.scholarcy.com/oa_version?query=Radford%2C%20A.%20Wu%2C%20J.%20Child%2C%20R.%20Luan%2C%20D.%20%E2%80%98Language%20models%20are%20unsupervised%20multitask%20learners%2C%E2%80%99%202019&author=Radford&title=%E2%80%98Language%20models%20are%20unsupervised%20multitask%20learners%2C%E2%80%99&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Radford%2C%20A.%20Wu%2C%20J.%20Child%2C%20R.%20Luan%2C%20D.%20%E2%80%98Language%20models%20are%20unsupervised%20multitask%20learners%2C%E2%80%99%202019) [Scite](/scite_tallies?query=author%3ARadford%2Ctitle%3A%E2%80%98Language%20models%20are%20unsupervised%20multitask%20learners%2C%E2%80%99%2Cyear%3A2019)

[^69]: P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala, ‘‘Good debt or bad debt: Detecting semantic orientations in economic texts,’’ J. Assoc. Inf. Sci. Technol., vol. 65, no. 4, pp. 782–796, Apr. 2014.  [OA](https://engine.scholarcy.com/oa_version?query=Malo%2C%20P.%20Sinha%2C%20A.%20Korhonen%2C%20P.%20Wallenius%2C%20J.%20%E2%80%98Good%20debt%20or%20bad%20debt%3A%20Detecting%20semantic%20orientations%20in%20economic%20texts%2C%E2%80%99%202014-04&author=Malo&title=%E2%80%98Good%20debt%20or%20bad%20debt%3A%20Detecting%20semantic%20orientations%20in%20economic%20texts%2C%E2%80%99&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Malo%2C%20P.%20Sinha%2C%20A.%20Korhonen%2C%20P.%20Wallenius%2C%20J.%20%E2%80%98Good%20debt%20or%20bad%20debt%3A%20Detecting%20semantic%20orientations%20in%20economic%20texts%2C%E2%80%99%202014-04) [Scite](/scite_tallies?query=author%3AMalo%2Ctitle%3A%E2%80%98Good%20debt%20or%20bad%20debt%3A%20Detecting%20semantic%20orientations%20in%20economic%20texts%2C%E2%80%99%2Cyear%3A2014)

[^70]: K. Cortis, A. Freitas, T. Daudert, M. Huerlimann, M. Zarrouk, S. Handschuh, and B. Davis, ‘‘SemEval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news,’’ in Proc. 11th Int. Workshop Semantic Eval. (SemEval-), 2017, pp. 519–535.  [OA](https://scholar.google.co.uk/scholar?q=Cortis%2C%20K.%20Freitas%2C%20A.%20Daudert%2C%20T.%20Huerlimann%2C%20M.%20%E2%80%98SemEval-2017%20task%205%3A%20Fine-grained%20sentiment%20analysis%20on%20financial%20microblogs%20and%20news%2C%E2%80%99%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Cortis%2C%20K.%20Freitas%2C%20A.%20Daudert%2C%20T.%20Huerlimann%2C%20M.%20%E2%80%98SemEval-2017%20task%205%3A%20Fine-grained%20sentiment%20analysis%20on%20financial%20microblogs%20and%20news%2C%E2%80%99%202017) 

[^71]: F. Chollet. (2015). Keras. [Online]. Available: https://keras.io  [OA](https://keras.io)  

[^72]: T. Wolf et al., ‘‘HuggingFace’s transformers: State-of-the-art natural language processing,’’ 2019, arXiv:1910.03771. [Online]. Available: http://arxiv.org/abs/1910.03771  [OA](http://arxiv.org/abs/1910.03771)  

[^73]: J. H. Friedman, ‘‘Greedy function approximation: A gradient boosting machine,’’ Ann. Statist., vol. 29, no. 5, pp. 1189–1232, 2001.  [OA](https://engine.scholarcy.com/oa_version?query=Friedman%2C%20J.H.%20%E2%80%98Greedy%20function%20approximation%3A%20A%20gradient%20boosting%20machine%2C%E2%80%99%202001&author=Friedman&title=%E2%80%98Greedy%20function%20approximation%3A%20A%20gradient%20boosting%20machine%2C%E2%80%99&year=2001) [GScholar](https://scholar.google.co.uk/scholar?q=Friedman%2C%20J.H.%20%E2%80%98Greedy%20function%20approximation%3A%20A%20gradient%20boosting%20machine%2C%E2%80%99%202001) [Scite](/scite_tallies?query=author%3AFriedman%2Ctitle%3A%E2%80%98Greedy%20function%20approximation%3A%20A%20gradient%20boosting%20machine%2C%E2%80%99%2Cyear%3A2001)

[^74]: T. Chen and C. Guestrin, ‘‘XGBoost: A scalable tree boosting system,’’ in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD), San Francisco, CA, USA. New York, NY, USA: ACM, 2016, pp. 785–794. [Online]. Available: http://doi.acm.org/10.1145/2939672.2939785, doi:10.1145/2939672.2939785.  [OA](https://doi.org/10.1145/2939672.2939785)  [Scite](/scite_tallies?query=https://doi.org/10.1145/2939672.2939785)

[^75]: L. Deng and D. Yu, ‘‘Deep learning: Methods and applications,’’ Found. Trends Signal Process., vol. 7, nos. 3–4, pp. 197–387, Jun. 2014.  [OA](https://engine.scholarcy.com/oa_version?query=Deng%2C%20L.%20Yu%2C%20D.%20%E2%80%98Deep%20learning%3A%20Methods%20and%20applications%2C%E2%80%99%202014-06&author=Deng&title=%E2%80%98Deep%20learning%3A%20Methods%20and%20applications%2C%E2%80%99&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Deng%2C%20L.%20Yu%2C%20D.%20%E2%80%98Deep%20learning%3A%20Methods%20and%20applications%2C%E2%80%99%202014-06) [Scite](/scite_tallies?query=author%3ADeng%2Ctitle%3A%E2%80%98Deep%20learning%3A%20Methods%20and%20applications%2C%E2%80%99%2Cyear%3A2014)

[^76]: D. Yu and L. Deng, ‘‘Deep learning and its applications to signal and information processing [exploratory DSP],’’ IEEE Signal Process. Mag., vol. 28, no. 1, pp. 145–154, Jan. 2011.  [OA](https://engine.scholarcy.com/oa_version?query=Yu%2C%20D.%20Deng%2C%20L.%20%E2%80%98Deep%20learning%20and%20its%20applications%20to%20signal%20and%20information%20processing%20%5Bexploratory%20DSP%5D%2C%E2%80%99%202011-01&author=Yu&title=%E2%80%98Deep%20learning%20and%20its%20applications%20to%20signal%20and%20information%20processing%20%5Bexploratory%20DSP%5D%2C%E2%80%99&year=2011) [GScholar](https://scholar.google.co.uk/scholar?q=Yu%2C%20D.%20Deng%2C%20L.%20%E2%80%98Deep%20learning%20and%20its%20applications%20to%20signal%20and%20information%20processing%20%5Bexploratory%20DSP%5D%2C%E2%80%99%202011-01) [Scite](/scite_tallies?query=author%3AYu%2Ctitle%3A%E2%80%98Deep%20learning%20and%20its%20applications%20to%20signal%20and%20information%20processing%20%5Bexploratory%20DSP%5D%2C%E2%80%99%2Cyear%3A2011)

[^77]: A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, ‘‘Deep learning for computer vision: A brief review,’’ Comput. Intell. Neurosci., vol. 2018, pp. 1–13, Feb. 2018.  [OA](https://engine.scholarcy.com/oa_version?query=Voulodimos%2C%20A.%20Doulamis%2C%20N.%20Doulamis%2C%20A.%20Protopapadakis%2C%20E.%20%E2%80%98Deep%20learning%20for%20computer%20vision%3A%20A%20brief%20review%2C%E2%80%99%202018-02&author=Voulodimos&title=%E2%80%98Deep%20learning%20for%20computer%20vision%3A%20A%20brief%20review%2C%E2%80%99&year=2018) [GScholar](https://scholar.google.co.uk/scholar?q=Voulodimos%2C%20A.%20Doulamis%2C%20N.%20Doulamis%2C%20A.%20Protopapadakis%2C%20E.%20%E2%80%98Deep%20learning%20for%20computer%20vision%3A%20A%20brief%20review%2C%E2%80%99%202018-02) [Scite](/scite_tallies?query=author%3AVoulodimos%2Ctitle%3A%E2%80%98Deep%20learning%20for%20computer%20vision%3A%20A%20brief%20review%2C%E2%80%99%2Cyear%3A2018)

[^78]: L. Deng, G. Hinton, and B. Kingsbury, ‘‘New types of deep neural network learning for speech recognition and related applications: An overview,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., May 2013, pp. 8599–8603.  [OA](https://scholar.google.co.uk/scholar?q=Deng%2C%20L.%20Hinton%2C%20G.%20Kingsbury%2C%20B.%20%E2%80%98New%20types%20of%20deep%20neural%20network%20learning%20for%20speech%20recognition%20and%20related%20applications%3A%20An%20overview%2C%E2%80%99%202013-05) [GScholar](https://scholar.google.co.uk/scholar?q=Deng%2C%20L.%20Hinton%2C%20G.%20Kingsbury%2C%20B.%20%E2%80%98New%20types%20of%20deep%20neural%20network%20learning%20for%20speech%20recognition%20and%20related%20applications%3A%20An%20overview%2C%E2%80%99%202013-05) 

[^79]: J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, R. A. Saurous, Y. Agiomvrgiannakis, and Y. Wu, ‘‘Natural TTS synthesis by conditioning wavenet on MEL spectrogram predictions,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Apr. 2018, pp. 4779–4783.  [OA](https://scholar.google.co.uk/scholar?q=Shen%2C%20J.%20Pang%2C%20R.%20Weiss%2C%20R.J.%20Schuster%2C%20M.%20%E2%80%98Natural%20TTS%20synthesis%20by%20conditioning%20wavenet%20on%20MEL%20spectrogram%20predictions%2C%E2%80%99%202018-04) [GScholar](https://scholar.google.co.uk/scholar?q=Shen%2C%20J.%20Pang%2C%20R.%20Weiss%2C%20R.J.%20Schuster%2C%20M.%20%E2%80%98Natural%20TTS%20synthesis%20by%20conditioning%20wavenet%20on%20MEL%20spectrogram%20predictions%2C%E2%80%99%202018-04) 

[^80]: W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang, J. Raiman, and J. Miller, ‘‘Deep voice 3: Scaling text-to-speech with convolutional sequence learning,’’ 2017, arXiv:1710.07654. [Online]. Available: http://arxiv.org/abs/1710.07654  [OA](http://arxiv.org/abs/1710.07654)  

[^81]: G. Liu and J. Guo, ‘‘Bidirectional LSTM with attention mechanism and convolutional layer for text classification,’’ Neurocomputing, vol. 337, pp. 325–338, Apr. 2019.  [OA](https://engine.scholarcy.com/oa_version?query=Liu%2C%20G.%20Guo%2C%20J.%20%E2%80%98Bidirectional%20LSTM%20with%20attention%20mechanism%20and%20convolutional%20layer%20for%20text%20classification%2C%E2%80%99%202019-04&author=Liu&title=%E2%80%98Bidirectional%20LSTM%20with%20attention%20mechanism%20and%20convolutional%20layer%20for%20text%20classification%2C%E2%80%99&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Liu%2C%20G.%20Guo%2C%20J.%20%E2%80%98Bidirectional%20LSTM%20with%20attention%20mechanism%20and%20convolutional%20layer%20for%20text%20classification%2C%E2%80%99%202019-04) [Scite](/scite_tallies?query=author%3ALiu%2Ctitle%3A%E2%80%98Bidirectional%20LSTM%20with%20attention%20mechanism%20and%20convolutional%20layer%20for%20text%20classification%2C%E2%80%99%2Cyear%3A2019)

[^82]: P. Liu, X. Qiu, and X. Huang, ‘‘Recurrent neural network for text classification with multi-task learning,’’ 2016, arXiv:1605.05101. [Online]. Available: http://arxiv.org/abs/1605.05101  [OA](http://arxiv.org/abs/1605.05101)  

[^83]: D. Bahdanau, K. Cho, and Y. Bengio, ‘‘Neural machine translation by jointly learning to align and translate,’’ 2014, arXiv:1409.0473. [Online]. Available: http://arxiv.org/abs/1409.0473  [OA](http://arxiv.org/abs/1409.0473)  

[^84]: K. Mishev et al., ‘‘Performance evaluation of word and sentence embeddings for finance headlines sentiment analysis,’’ in ICT Innovations 2019. Big Data Processing and Mining (Communications in Computer and Information Science), vol. 1110, S. Gievska and G. Madjarov, Eds. Cham, Switzerland: Springer, 2019, pp. 161–172.  [OA](https://scholar.google.co.uk/scholar?q=Mishev%2C%20K.%20%E2%80%98Performance%20evaluation%20of%20word%20and%20sentence%20embeddings%20for%20finance%20headlines%20sentiment%20analysis%2C%E2%80%99%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Mishev%2C%20K.%20%E2%80%98Performance%20evaluation%20of%20word%20and%20sentence%20embeddings%20for%20finance%20headlines%20sentiment%20analysis%2C%E2%80%99%202019) 

[^85]: D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’ 2014, arXiv:1412.6980. [Online]. Available: http://arxiv.org/abs/1412.6980  [OA](http://arxiv.org/abs/1412.6980)  

[^86]: N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, ‘‘Dropout: A simple way to prevent neural networks from overfitting,’’ J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958, 2014.  [OA](https://engine.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20%E2%80%98Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%2C%E2%80%99%202014&author=Srivastava&title=%E2%80%98Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%2C%E2%80%99&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20%E2%80%98Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%2C%E2%80%99%202014) [Scite](/scite_tallies?query=author%3ASrivastava%2Ctitle%3A%E2%80%98Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%2C%E2%80%99%2Cyear%3A2014)

[^87]: T. Rao and S. Srivastava, ‘‘Analyzing stock market movements using twitter sentiment analysis,’’ Tech. Rep., 2012.  [OA](https://engine.scholarcy.com/oa_version?query=Rao%2C%20T.%20Srivastava%2C%20S.%20%E2%80%98Analyzing%20stock%20market%20movements%20using%20twitter%20sentiment%20analysis%2C%E2%80%99%202012&author=Rao&title=%E2%80%98Analyzing%20stock%20market%20movements%20using%20twitter%20sentiment%20analysis%2C%E2%80%99&year=2012) [GScholar](https://scholar.google.co.uk/scholar?q=Rao%2C%20T.%20Srivastava%2C%20S.%20%E2%80%98Analyzing%20stock%20market%20movements%20using%20twitter%20sentiment%20analysis%2C%E2%80%99%202012) [Scite](/scite_tallies?query=author%3ARao%2Ctitle%3A%E2%80%98Analyzing%20stock%20market%20movements%20using%20twitter%20sentiment%20analysis%2C%E2%80%99%2Cyear%3A2012)

[^88]: J. Smailović, M. Grčar, N. Lavrač, and M. Žnidaršič, ‘‘Predictive sentiment analysis of tweets: A stock market application,’’ in Proc. Int. Workshop Hum.-Comput. Interact. Knowl. Discovery Complex, Unstructured, Big Data. Berlin, Germany: Springer, 2013, pp. 77–88.  [OA](https://scholar.google.co.uk/scholar?q=Smailovi%C4%87%2C%20J.%20Gr%C4%8Dar%2C%20M.%20Lavra%C4%8D%2C%20N.%20%C5%BDnidar%C5%A1i%C4%8D%2C%20M.%20%E2%80%98Predictive%20sentiment%20analysis%20of%20tweets%3A%20A%20stock%20market%20application%2C%E2%80%99%202013) [GScholar](https://scholar.google.co.uk/scholar?q=Smailovi%C4%87%2C%20J.%20Gr%C4%8Dar%2C%20M.%20Lavra%C4%8D%2C%20N.%20%C5%BDnidar%C5%A1i%C4%8D%2C%20M.%20%E2%80%98Predictive%20sentiment%20analysis%20of%20tweets%3A%20A%20stock%20market%20application%2C%E2%80%99%202013) 

[^89]: X. Li, H. Xie, L. Chen, J. Wang, and X. Deng, ‘‘News impact on stock price return via sentiment analysis,’’ Knowl.-Based Syst., vol. 69, pp. 14–23, Oct. 2014. KOSTADIN MISHEV received the bachelor’s degree in informatics and computer engineering and the master’s degree in computer networks and e-technologies degree from Saints Cyril and Methodius University, Skopje, in 2013 and 2016, respectively, where he is currently pursuing the Ph.D. degree. He is also a Teaching and a Research Assistant with the Faculty of Computer Science and Engineering, Saints Cyril and Methodius University. His research interests include data science, natural language processing, semantic Web, enterprise application architectures, Web technologies, and computer networks.  [OA](https://engine.scholarcy.com/oa_version?query=Li%2C%20X.%20Xie%2C%20H.%20Chen%2C%20L.%20Wang%2C%20J.%20%E2%80%98News%20impact%20on%20stock%20price%20return%20via%20sentiment%20analysis%2C%E2%80%99%202014-10&author=Li&title=%E2%80%98News%20impact%20on%20stock%20price%20return%20via%20sentiment%20analysis%2C%E2%80%99&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Li%2C%20X.%20Xie%2C%20H.%20Chen%2C%20L.%20Wang%2C%20J.%20%E2%80%98News%20impact%20on%20stock%20price%20return%20via%20sentiment%20analysis%2C%E2%80%99%202014-10) [Scite](/scite_tallies?query=author%3ALi%2Ctitle%3A%E2%80%98News%20impact%20on%20stock%20price%20return%20via%20sentiment%20analysis%2C%E2%80%99%2Cyear%3A2014)

[^90]: ANA GJORGJEVIKJ received the bachelor’s degree in computer science and engineering and the master’s degree in computer networks and e-technologies from Saints Cyril and Methodius University, Skopje, in 2010 and 2014, respectively, where she is currently pursuing the Ph.D. degree in the domain of data science, with particular focus on deep learning and natural language processing. She has been working as a Software Engineer, since 2010. Her research interests include data learning, natural language processing, and knowledge  [OA](https://scholar.google.co.uk/scholar?q=ANA%20GJORGJEVIKJ%20received%20the%20bachelors%20degree%20in%20computer%20science%20and%20engineering%20and%20the%20masters%20degree%20in%20computer%20networks%20and%20etechnologies%20from%20Saints%20Cyril%20and%20Methodius%20University%20Skopje%20in%202010%20and%202014%20respectively%20where%20she%20is%20currently%20pursuing%20the%20PhD%20degree%20in%20the%20domain%20of%20data%20science%20with%20particular%20focus%20on%20deep%20learning%20and%20natural%20language%20processing%20She%20has%20been%20working%20as%20a%20Software%20Engineer%20since%202010%20Her%20research%20interests%20include%20data%20learning%20natural%20language%20processing%20and%20knowledge) [GScholar](https://scholar.google.co.uk/scholar?q=ANA%20GJORGJEVIKJ%20received%20the%20bachelors%20degree%20in%20computer%20science%20and%20engineering%20and%20the%20masters%20degree%20in%20computer%20networks%20and%20etechnologies%20from%20Saints%20Cyril%20and%20Methodius%20University%20Skopje%20in%202010%20and%202014%20respectively%20where%20she%20is%20currently%20pursuing%20the%20PhD%20degree%20in%20the%20domain%20of%20data%20science%20with%20particular%20focus%20on%20deep%20learning%20and%20natural%20language%20processing%20She%20has%20been%20working%20as%20a%20Software%20Engineer%20since%202010%20Her%20research%20interests%20include%20data%20learning%20natural%20language%20processing%20and%20knowledge) 

[^91]: DIMITAR TRAJANOV (Member, IEEE) received the Ph.D. degree. He is currently a Professor and the Head of the Department of Information Systems and Network Technologies, Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, Skopje. He is also the Leader of Regional Social Innovation Hub established, in 2013, as a co-operation between UNDP and the Faculty of Computer Science and Engineering. He is the author of more than 150 journal and conference papers and seven books. He has been involved in more than 60 research and industrial projects, mostly as the Project Leader. His research interests include data science, machine learning, NLP, FinTech, semantic Web, open data, sharing economy, social innovation, e-commerce, entrepreneurship, technology for development, mobile development, and climate change.  [OA](https://scholar.google.co.uk/scholar?q=DIMITAR%20TRAJANOV%20Member%20IEEE%20received%20the%20PhD%20degree%20He%20is%20currently%20a%20Professor%20and%20the%20Head%20of%20the%20Department%20of%20Information%20Systems%20and%20Network%20Technologies%20Faculty%20of%20Computer%20Science%20and%20Engineering%20Ss%20Cyril%20and%20Methodius%20University%20Skopje%20He%20is%20also%20the%20Leader%20of%20Regional%20Social%20Innovation%20Hub%20established%20in%202013%20as%20a%20cooperation%20between%20UNDP%20and%20the%20Faculty%20of%20Computer%20Science%20and%20Engineering%20He%20is%20the%20author%20of%20more%20than%20150%20journal%20and%20conference%20papers%20and%20seven%20books%20He%20has%20been%20involved%20in%20more%20than%2060%20research%20and%20industrial%20projects%20mostly%20as%20the%20Project%20Leader%20His%20research%20interests%20include%20data%20science%20machine%20learning%20NLP%20FinTech%20semantic%20Web%20open%20data%20sharing%20economy%20social%20innovation%20ecommerce%20entrepreneurship%20technology%20for%20development%20mobile%20development%20and%20climate%20change) [GScholar](https://scholar.google.co.uk/scholar?q=DIMITAR%20TRAJANOV%20Member%20IEEE%20received%20the%20PhD%20degree%20He%20is%20currently%20a%20Professor%20and%20the%20Head%20of%20the%20Department%20of%20Information%20Systems%20and%20Network%20Technologies%20Faculty%20of%20Computer%20Science%20and%20Engineering%20Ss%20Cyril%20and%20Methodius%20University%20Skopje%20He%20is%20also%20the%20Leader%20of%20Regional%20Social%20Innovation%20Hub%20established%20in%202013%20as%20a%20cooperation%20between%20UNDP%20and%20the%20Faculty%20of%20Computer%20Science%20and%20Engineering%20He%20is%20the%20author%20of%20more%20than%20150%20journal%20and%20conference%20papers%20and%20seven%20books%20He%20has%20been%20involved%20in%20more%20than%2060%20research%20and%20industrial%20projects%20mostly%20as%20the%20Project%20Leader%20His%20research%20interests%20include%20data%20science%20machine%20learning%20NLP%20FinTech%20semantic%20Web%20open%20data%20sharing%20economy%20social%20innovation%20ecommerce%20entrepreneurship%20technology%20for%20development%20mobile%20development%20and%20climate%20change) 

